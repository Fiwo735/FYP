\chapter{Conclusion}\label{conclusion}
\indo{Conclude after writing all other sections, recapping the motivation, proposed model use cases, meta-programming approaches, how it all ties together and extends hardware-software co-design}
\indo{|}
\indo{|}
\indo{|}

\section{Achievements}
\indo{|}
\indo{|}
\indo{|}
\indo{|}

\section{Discussion}
\indo{|}
\indo{|}
\indo{|}
\indo{|}

\section{Future Work}
\indo{Bullet points or subsections}
\indo{e.g. fuse batch norm to linear, pruning}
\indo{Newer, efficient transformers like e.g. sparse transformer, low-rank transformer}
\indo{this work didnt touch energy consumption}
\indo{post-training novel method could use a faster simulation and some synthesis resource model to allow for a more sophisticated algorithm}
\indo{|}
\indo{|}

\subsection{High-Level-Synthesis Optimization}
\indo{Introduce MLIR and the overall flow of how PyTorch models are mapped, include nice diagrams}
\indo{|}
\indo{|}
\indo{Talk about how ScaleHLS extends MLIR to HLS, again diagrams}
\indo{|}
\indo{|}
\indo{Talk about potential integration/relation between hls4ml (Python -> HLS) and ScaleHLS (PyTorch/HLS -> Optimized HLS) }
\indo{|}
\indo{|}

%//////////////////////////////////////////////////////
\indo{at the end, CHECK: hyphens, 3rd person s in verbs, remaining todos, ugly page breaks, axis and titles in figures, correct some references to evaluation to be more specific (find all of them), change explanation in (brackets) to footnotes (always?), ensure no pre-training quantization, but quantization-aware training, }