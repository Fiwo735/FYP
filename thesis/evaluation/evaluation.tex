\chapter{Evaluation}\label{evaluation}
This chapter starts by evaluating proposed neural network architectures using CPUs and GPUs to find a baseline inference latency, accuracy and AUC values. This information is then compared with the results obtained from simulating and synthesizing the models on reconfigurable hardware. The outcome of the design space exploration includes hardware resource utilization metrics as well as discussion about the Pareto front and applicability in high energy physics environments. Both the pre-training and post-training quantization are evaluated quantitatively in terms of the trade-off between quality of results and bit-width reduction as well as qualitatively for their ease of adaptation to existing designs. Lastly, the contributions to the \hlsml library are descriptively presented, highlighting the areas that this work expands upon.

% This chapter outlines the proposed evaluation plan for the project. The first objective of developing and optimizing a state-of-the-art neural network in hardware can be evaluated quantitatively, while integrating it into the \textit{hls4ml} library and making it easy for new users to use requires a more qualitative approach.

% \section{Quantitative results}
% The following describes the quantities to be measured for each neural network design:

% \begin{outline}
%   \1 Classification accuracy, AUC and confusion matrix on a validation dataset
%   \1 Inference latency and throughput when running on the target platform
%   \1 Hardware resource utilization (exact values for comparison with other platforms and percentage of available resources for understanding limitations):
%     \2 Block RAM (BRAM) and Ultra RAM (URAM)
%     \2 Digital Signal Processing units (DSP)
%     \2 Flip-Flops (FF)
%     \2 Look-Up Tables (LUT)
% \end{outline}

% In the early stages of the project, the above quantities will be measured from the results from the simulation and synthesis reports. At a later stage, the best designs will be run on actual hardware platforms to validate them under real-life use cases. The platform planned for this part is an Intel Stratix V FPGA hosted in a Maxeler MPC-X dataflow node with 8 Maia dataflow engines and 48 GB of DRAM. A consideration is also planned for the specific hardware used in the LHC L1T detectors and its available resources, which although cannot be directly tested on, can guide the state space exploration.

% Apart from clear design improvements, it is predicted that most evaluated designs will offer trade-offs between classification accuracy, AUC, inference throughput and hardware utilization. It is not possible to find a design that is superior in every way, hence a Pareto front and the Roofline model will play the key roles in understanding the overall performance and selecting configuration with specific needs in mind.


% \section{Qualitative Results}
% To assess the success of enhancing the \textit{hls4ml} library, qualitative comparisons will be drawn between it and the already existing neural network components and architectures. Depending on the project's timeline, it is possible that the improvements can get official approval and get merged into the main repository, however if this is not feasible before the final deadline, current users of the library will be surveyed and their opinion will be taken into consideration instead.

\section{Architecture Analysis}


\section{Hardware Implementation}
\indo{Analytical models for latency/resources?}


\section{Quantization Results}


\subsection{Pre-Training Quantization}


\subsection{Post-Training Quantization}


\section{\hlsml Contributions}



\todofig{pre-training quantization compared to varying floating-point widths}
\indo{float16 doesnt learn anything (acc 20\%) as its range is too small and we cannot consider normalizing inputs coz its real time system}
\indo{brevitas only gets 34\% accuracy}
\indo{pytorch quantization is too experimental and doesnt support the model}
\indo{post-training quantization}

\indo{somewhere: fuse batch norm to linear???}
\indo{graph showing existing hls4ml blocks and the one I added}