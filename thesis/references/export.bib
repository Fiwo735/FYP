@inproceedings{RefWorks:RefID:80-wang2019haq:,
	author={Kuan Wang and Zhijian Liu and Yujun Lin and Ji Lin and Song Han},
	year={Jun 2019},
	title={HAQ: Hardware-Aware Automated Quantization With Mixed Precision},
	publisher={IEEE},
	pages={8604-8612},
	abstract={Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. There are plenty of specialized hardware for neural networks, but little research has been done for specialized neural network optimization for a particular hardware architecture. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95× and the energy consumption by 1.9× with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.},
	url={https://ieeexplore.ieee.org/document/8954415},
	doi={10.1109/CVPR.2019.00881}
}
@misc{RefWorks:RefID:79-sharmapytorch,
	author={Abhishek Sharma},
	title={PyTorch JIT and TorchScript},
	volume={2022},
	number={June 14,},
	url={https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff}
}
@misc{RefWorks:RefID:78-zhangfx,
	author={Jerry Zhang},
	title={FX Graph Mode Quantization User Guide},
	volume={2022},
	number={June 13,},
	url={https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html}
}
@misc{RefWorks:RefID:77-krishnamoorthistatic,
	author={Raghuraman Krishnamoorthi and Seth Weidman and Jerry Zhang},
	title={Static Quantization with Eager Mode in PyTorch},
	volume={2022},
	number={Jun 13,},
	url={https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html}
}
@misc{RefWorks:RefID:76-shaumontfixed,
	author={Patrick Shaumont},
	title={Fixed Point Arithmetic in DSP — Real Time Digital Signal Processing},
	volume={2022},
	number={Jun 13,},
	url={https://schaumont.dyn.wpi.edu/ece4703b21/lecture6.html}
}
@inproceedings{RefWorks:RefID:75-kaiming2016deep,
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={Jun 2016},
	title={Deep Residual Learning for Image Recognition},
	publisher={IEEE},
	pages={770-778},
	abstract={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	url={https://ieeexplore.ieee.org/document/7780459},
	doi={10.1109/CVPR.2016.90}
}
@article{RefWorks:RefID:74-szegedy2016inception-v4,
	author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
	year={2016},
	month={Feb 23,},
	title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
	abstract={Very deep convolutional networks have been central to the largest advances in
image recognition performance in recent years. One example is the Inception
architecture that has been shown to achieve very good performance at relatively
low computational cost. Recently, the introduction of residual connections in
conjunction with a more traditional architecture has yielded state-of-the-art
performance in the 2015 ILSVRC challenge; its performance was similar to the
latest generation Inception-v3 network. This raises the question of whether
there are any benefit in combining the Inception architecture with residual
connections. Here we give clear empirical evidence that training with residual
connections accelerates the training of Inception networks significantly. There
is also some evidence of residual Inception networks outperforming similarly
expensive Inception networks without residual connections by a thin margin. We
also present several new streamlined architectures for both residual and
non-residual Inception networks. These variations improve the single-frame
recognition performance on the ILSVRC 2012 classification task significantly.
We further demonstrate how proper activation scaling stabilizes the training of
very wide residual Inception networks. With an ensemble of three residual and
one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the
ImageNet classification (CLS) challenge},
	url={https://arxiv.org/abs/1602.07261}
}
@misc{RefWorks:RefID:73-alammarillustrated,
	author={Jay Alammar},
	title={The Illustrated Transformer},
	volume={2022},
	number={Jun 9,},
	abstract={Discussions:
Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)


Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Russian, Spanish, Vietnamese

Watch: MIT’s Deep Learning State of the Art lecture referencing this post

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:




A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	url={https://jalammar.github.io/illustrated-transformer/}
}
@inproceedings{RefWorks:RefID:72-keren2016convolutional,
	author={Gil Keren and Bjorn Schuller},
	year={Jul 2016},
	title={Convolutional RNN: An enhanced model for extracting features from sequential data},
	publisher={IEEE},
	pages={3412-3419},
	abstract={Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers, we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers.},
	url={https://ieeexplore.ieee.org/document/7727636},
	doi={10.1109/IJCNN.2016.7727636}
}
@article{RefWorks:RefID:71-chorowski2015attention-based,
	author={Jan Chorowski and Dzmitry Bahdanau and Dmitriy Serdyuk and Kyunghyun Cho and Yoshua Bengio},
	year={2015},
	month={Jun 24,},
	title={Attention-Based Models for Speech Recognition},
	abstract={Recurrent sequence generators conditioned on input data through an attention
mechanism have recently shown very good performance on a range of tasks in-
cluding machine translation, handwriting synthesis and image caption gen-
eration. We extend the attention-mechanism with features needed for speech
recognition. We show that while an adaptation of the model used for machine
translation in reaches a competitive 18.7% phoneme error rate (PER) on the
TIMIT phoneme recognition task, it can only be applied to utterances which are
roughly as long as the ones it was trained on. We offer a qualitative
explanation of this failure and propose a novel and generic method of adding
location-awareness to the attention mechanism to alleviate this issue. The new
method yields a model that is robust to long inputs and achieves 18% PER in
single utterances and 20% in 10-times longer (repeated) utterances. Finally, we
propose a change to the at- tention mechanism that prevents it from
concentrating too much on single frames, which further reduces PER to 17.6%
level.},
	url={https://arxiv.org/abs/1506.07503}
}
@misc{RefWorks:RefID:70-keanecern's,
	author={Sean Keane},
	title={CERN's Large Hadron Collider Restarts After Three-Year Upgrade},
	volume={2022},
	number={Jun 7,},
	abstract={The particle collider will reach a record level of energy in the coming months, scientists say.},
	url={https://www.cnet.com/science/cerns-large-hadron-collider-restarts-after-three-year-upgrade/}
}
@techreport{RefWorks:RefID:69-pagano2020gravity,
	author={D. Pagano and M. Caccia and J. Fesel and S. Gerber and C. Malbrunot and S. R. Müller and G. Cerchiari and M. Giammarchi and F. Guatieri and L. Smestad and S. Aghion and E. Widmann and F. Prelz and N. Zurlo and F. Sorrentino and H. Sandaker and J. Robert and A. Evans and S. Haider and P. Yzombard and A. Gligorova and L. Di Noto and P. Lansonneur and O. Khalidova and D. Krasnický and R. Santoro and P. Lebrun and M. Fani and A. Kellerbauer and Z. Mazzotta and H. Holmestad and V. Petracek and S. Mariazzi and L. Penasa and D. Comparat and M. Prevedelli and G. Nebbia and J. Zmeskal and A. Hinterberger and C. Zimmer and I. C. Tietje and R. Caravita and C. Amsler and N. Pacifico and G. Testera and L. Ravelli and M. Doser and R. Ferragut and A. Rotondi and M. Oberthaler and B. Rienaecker and G. Bonomi and A. Demetrio and P. Nedelec and O. M. Røhne and A. Fontana and R. S. Brusa and F. Castelli and J. Marton and V. Matveev and G. Consolati and V. Lagomarsino},
	year={2020},
	title={Gravity and antimatter: the AEgIS experiment at CERN},
	institution={IOP Publishing},
	number={1342},
	pages={12016},
	abstract={From the experimental point of view, very little is known about the gravitational interaction between matter and antimatter. In particular, the Weak Equivalence Principle, which is of paramount importance for the General Relativity, has not yet been directly probed with antimatter. The main goal of the AEgIS experiment at CERN is to perform a direct measurement of the gravitational force on antimatter. The idea is to measure the vertical displacement of a beam of cold antihydrogen atoms, traveling in the gravitational field of the Earth, by the means of a moiré deflectometer. An overview of the physics goals of the experiment, of its apparatus and of the first results is presented.},
	isbn={1742-6588},
	url={http://cds.cern.ch/record/2714100},
	doi={10.1088/1742-6596/1342/1/012016}
}
@techreport{RefWorks:RefID:68-walz2015gbar,
	author={J. Walz and P. Grandemange and B. Vallage and P. Debu and Y. Matsuda and N. Sillitoe and M. Charlton and T. Mortensen and A. Husson and F. Nez and Y. Sacquin and F. Biraben and G. Dufour and Y. Yamazaki and O. Dalkarov and P. A. Hervieux and J. M. Rey and R. Guérout and L. Liszkay and S. Wronka and D. Lunney and S. Wolf and A. Voronin and V. Nesvizhevsky and M. Valdes and D. Banerjee and N. Madsen and N. Kuroda and P. Froelich and K. Khabarova and L. Hilico and P. Comini and C. Regenfus and S. Reynaud and S. Jonsell and A. Rubbia and G. Manfredi and J. M. Heinrich and J. P. Karr and C. I. Szabo-Foster and S. Eriksson and M. Staszczak and H. Torii and P. Indelicato and Y. Nagashima and P. Dupré and P. Cladé and A. Lambrecht and A. Mohri and J. M. Reymond and D. P. Werf and B. Mansoulié and S. Guellati and A. Douillet and P. Crivelli and A. M. M. Leite and P. Pérez and D. Brook-Roberge and F. Schmidt-Kaler and N. Kolachevsky},
	year={2015},
	title={The GBAR antimatter gravity experiment},
	institution={Springer International Publishing},
	number={233},
	pages={21-27},
	abstract={The GBAR project (Gravitational Behaviour of Anti hydrogen at Rest) at CERN, aims to measure the free fall acceleration of ultracold neutral anti hydrogen atoms in the terrestrial gravitational field. The experiment consists preparing anti hydrogen ions (one antiproton and two positrons) and sympathetically cooling them with Be$^{+}$ ions to less than 10 μK. The ultracold ions will then be photo-ionized just above threshold, and the free fall time over a known distance measured. We will describe the project, the accuracy that can be reached by standard techniques, and discuss a possible improvement to reduce the vertical velocity spread.},
	isbn={0304-3843},
	url={http://cds.cern.ch/record/2055685},
	doi={10.1007/s10751-015-1154-8}
}
@article{RefWorks:RefID:67-krasnov2018gravity,
	author={K. Krasnov and R. Percacci},
	year={2018},
	month={Jun 14,},
	title={Gravity and unification: a review},
	journal={Classical and quantum gravity},
	volume={35},
	number={14},
	pages={143001},
	abstract={We review various classical unified theories of gravity and other interactions that have appeared in the literature, paying special attention to scenarios in which spacetime remains four-dimensional, while an 'internal' space is enlarged. The starting point for each such unification scenario is a particular formalism for general relativity. We thus start by reviewing, besides the usual Einstein-Hilbert and Palatini formulations, the Einstein-Cartan, MacDowell-Mansouri and BF (both non-chiral and chiral) formulations. Each of these introduces some version of 'internal' bundle and a dynamical variable that ties the internal and tangent bundles. In each of these formulations there is also an independent connection in the 'internal' bundle. One can then study the effects of 'enlarging the internal space', which typically leads to a theory of gravity and Yang-Mills fields. We review what has been done in the literature on each of these unification schemes, and compare and contrast their achievements to those of the better developed Kaluza-Klein scenario.},
	isbn={0264-9381},
	url={https://iopscience.iop.org/article/10.1088/1361-6382/aac58d},
	doi={10.1088/1361-6382/aac58d}
}
@inproceedings{RefWorks:RefID:66-peng2021accelerating,
	author={Hongwu Peng and Shaoyi Huang and Tong Geng and Ang Li and Weiwen Jiang and Hang Liu and Shusen Wang and Caiwen Ding},
	year={2021},
	title={Accelerating transformer-based deep learning models on fpgas using column balanced block pruning},
	booktitle={2021 22nd International Symposium on Quality Electronic Design (ISQED)},
	publisher={IEEE},
	pages={142-148}
}
@inproceedings{RefWorks:RefID:65-qi2021accommodating,
	author={Panjie Qi and Yuhong Song and Hongwu Peng and Shaoyi Huang and Qingfeng Zhuge and Edwin Hsing-Mean Sha},
	year={2021},
	title={Accommodating transformer onto fpga: Coupling the balanced model compression and fpga-implementation optimization},
	booktitle={Proceedings of the 2021 on Great Lakes Symposium on VLSI},
	pages={163-168}
}
@article{RefWorks:RefID:64-assad2017performance,
	author={M. El Haj Assad and E. Bani-Hani and M. Khalil},
	year={2017},
	title={Performance of geothermal power plants (single, dual, and binary) to compensate for LHC-CERN power consumption: comparative study},
	journal={Geothermal Energy},
	volume={5},
	number={1},
	pages={1-16}
}
@inproceedings{RefWorks:RefID:63-li2020ftrans:,
	author={Bingbing Li and Santosh Pandey and Haowen Fang and Yanjun Lyv and Ji Li and Jieyang Chen and Mimi Xie and Lipeng Wan and Hang Liu and Caiwen Ding},
	year={2020},
	title={Ftrans: energy-efficient acceleration of transformers using fpga},
	booktitle={Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
	pages={175-180}
}
@inproceedings{RefWorks:RefID:62-gillioz2020overview,
	author={Anthony Gillioz and Jacky Casas and Elena Mugellini and Omar Abou Khaled},
	year={2020},
	title={Overview of the Transformer-based Models for NLP Tasks},
	booktitle={2020 15th Conference on Computer Science and Information Systems (FedCSIS)},
	publisher={IEEE},
	pages={179-183}
}
@article{RefWorks:RefID:61-coleman2018importance,
	author={E. Coleman and M. Freytsis and A. Hinzmann and M. Narain and J. Thaler and N. Tran and C. Vernieri},
	year={2018},
	month={Jan 9,},
	title={The importance of calorimetry for highly-boosted jet substructure},
	journal={Journal of instrumentation},
	volume={13},
	number={1},
	pages={T01003},
	abstract={Jet substructure techniques are playing an essential role in exploring the TeV scale at the Large Hadron Collider (LHC), since they facilitate the efficient reconstruction and identification of highly-boosted objects. Both for the LHC and for future colliders, there is a growing interest in using jet substructure methods based only on charged-particle information. The reason is that silicon-based tracking detectors offer excellent granularity and precise vertexing, which can improve the angular resolution on highly-collimated jets and mitigate the impact of pileup. In this paper, we assess how much jet substructure performance degrades by using track-only information, and we demonstrate physics contexts in which calorimetry is most beneficial. Specifically, we consider five different hadronic final states—W bosons, Z bosons, top quarks, light quarks, gluons—and test the pairwise discrimination power with a multi-variate combination of substructure observables. In the idealized case of perfect reconstruction, we quantify the loss in discrimination performance when using just charged particles compared to using all detected particles. We also consider the intermediate case of using charged particles plus photons, which provides valuable information about neutral pions. In the more realistic case of a segmented calorimeter, we assess the potential performance gains from improving calorimeter granularity and resolution, comparing a CMS-like detector to more ambitious future detector concepts. Broadly speaking, we find large performance gains from neutral-particle information and from improved calorimetry in cases where jet mass resolution drives the discrimination power, whereas the gains are more modest if an absolute mass scale calibration is not required.},
	isbn={1748-0221},
	url={https://search.proquest.com/docview/2365693051},
	doi={10.1088/1748-0221/13/01/T01003}
}
@article{RefWorks:RefID:60-blanchard2019accurate,
	author={Pierre Blanchard and Desmond J. Higham and Nicholas J. Higham},
	year={2019},
	title={Accurate computation of the log-sum-exp and softmax functions},
	journal={arXiv preprint arXiv:1909.03469}
}
@article{RefWorks:RefID:59-barr1991einstein,
	author={Alan H. Barr},
	year={1991},
	title={The Einstein summation notation},
	journal={An Introduction to Physically Based Modeling (Course Notes 19), pages E},
	volume={1},
	pages={57}
}
@article{RefWorks:RefID:57-garbin2020dropout,
	author={Christian Garbin and Xingquan Zhu and Oge Marques},
	year={2020},
	month={Jan 22,},
	title={Dropout vs. batch normalization: an empirical study of their impact to deep learning},
	journal={Multimedia tools and applications},
	volume={79},
	number={19-20},
	pages={12777-12815},
	abstract={Overfitting and long training time are two fundamental challenges in multilayered neural network learning and deep learning in particular. Dropout and batch normalization are two well-recognized approaches to tackle these challenges. While both approaches share overlapping design principles, numerous research results have shown that they have unique strengths to improve deep learning. Many tools simplify these two approaches as a simple function call, allowing flexible stacking to form deep learning architectures. Although their usage guidelines are available, unfortunately no well-defined set of rules or comprehensive studies to investigate them concerning data input, network configurations, learning efficiency, and accuracy. It is not clear when users should consider using dropout and/or batch normalization, and how they should be combined (or used alternatively) to achieve optimized deep learning outcomes. In this paper we conduct an empirical study to investigate the effect of dropout and batch normalization on training deep learning models. We use multilayered dense neural networks and convolutional neural networks (CNN) as the deep learning models, and mix dropout and batch normalization to design different architectures and subsequently observe their performance in terms of training and test CPU time, number of parameters in the model (as a proxy for model size), and classification accuracy. The interplay between network structures, dropout, and batch normalization, allow us to conclude when and how dropout and batch normalization should be considered in deep learning. The empirical study quantified the increase in training time when dropout and batch normalization are used, as well as the increase in prediction time (important for constrained environments, such as smartphones and low-powered IoT devices). It showed that a non-adaptive optimizer (e.g. SGD) can outperform adaptive optimizers, but only at the cost of a significant amount of training times to perform hyperparameter tuning, while an adaptive optimizer (e.g. RMSProp) performs well without much tuning. Finally, it showed that dropout and batch normalization should be used in CNNs only with caution and experimentation (when in doubt and short on time to experiment, use only batch normalization).},
	isbn={1380-7501},
	url={https://link.springer.com/article/10.1007/s11042-019-08453-9},
	doi={10.1007/s11042-019-08453-9}
}
@inproceedings{RefWorks:RefID:56-cabello2015implementation,
	author={Frank Cabello and Julio Leon and Yuzo Iano and Rangel Arthur},
	year={Sep 2015},
	title={Implementation of a fixed-point 2D Gaussian Filter for Image Processing based on FPGA},
	publisher={Division of Signal Processing and Electronic Systems, Poznan University of Technology (DSPES PUT)},
	pages={28-33},
	abstract={One of the very useful techniques in Image Processing is the 2D Gaussian Filter, especially when smoothing images. However, the implementation of a 2D Gaussian Filter requires heavy computational resources, and when it comes down to real-time applications, efficiency in the implementation is vital. Floating-point math represents an obstacle for this, as its implementation requires a large amount of computational power in order to achieve real-time image processing. On the other hand, a fixed-point approach is much more suitable; implementation of a 2D Gaussian Filter in FPGA using fixed-point arithmetic provides efficiency in the processing and reduction in computational costs. The purpose of this study is to present the FPGA resource usage for different sizes of Gaussian Kernel; to provide a comparison between fixed-point and floating point implementations; and to define the amount of bits are necessary to use in order to have a Root Mean Square Error (RMSE) below 5%.},
	isbn={2326-0262},
	url={https://ieeexplore.ieee.org/document/7365108},
	doi={10.1109/SPA.2015.7365108}
}
@inproceedings{RefWorks:RefID:55-solovyev2019fixed-point,
	author={Roman Solovyev and Alexander Kustov and Dmitry Telpukhov and Vladimir Rukhlov and Alexandr Kalinin},
	year={Jan 2019},
	title={Fixed-Point Convolutional Neural Network for Real-Time Video Processing in FPGA},
	publisher={IEEE},
	pages={1605-1611},
	abstract={Modern mobile neural networks with a reduced number of weights and parameters do a good job with image classification tasks, but even they may be too complex to be implemented in an FPGA for video processing tasks. The article proposes neural network architecture for the practical task of recognizing images from a camera, which has several advantages in terms of speed. This is achieved by reducing the number of weights, moving from a floating-point to a fixed-point arithmetic, and due to a number of hardware-level optimizations associated with storing weights in blocks, a shift register, and an adjustable number of convolutional blocks that work in parallel. The article also proposed methods for adapting the existing data set for solving a different task. As the experiments showed, the proposed neural network copes well with real-time video processing even on the cheap FPGAs.},
	url={https://ieeexplore.ieee.org/document/8656778},
	doi={10.1109/EIConRus.2019.8656778}
}
@misc{RefWorks:RefID:54-solovyev2018fpga,
	author = 	 {Roman A. Solovyev and Alexandr A. Kalinin and Alexander G. Kustov and Dmitry V. Telpukhov and Vladimir S. Ruhlov},
	year = 	 {2018},
	month = 	 {Aug 29,},
	title = 	 {FPGA Implementation of Convolutional Neural Networks with Fixed-Point Calculations},
	abstract = 	 {Neural network-based methods for image processing are becoming widely used in practical applications. Modern neural networks are computationally expensive and require specialized hardware, such as graphics processing units. Since such hardware is not always available in real life applications, there is a compelling need for the design of neural networks for mobile devices. Mobile neural networks typically have reduced number of parameters and require a relatively small number of arithmetic operations. However, they usually still are executed at the software level and use floating-point calculations. The use of mobile networks without further optimization may not provide sufficient performance when high processing speed is required, for example, in real-time video processing (30 frames per second). In this study, we suggest optimizations to speed up computations in order to efficiently use already trained neural networks on a mobile device. Specifically, we propose an approach for speeding up neural networks by moving computation from software to hardware and by using fixed-point calculations instead of floating-point. We propose a number of methods for neural network architecture design to improve the performance with fixed-point calculations. We also show an example of how existing datasets can be modified and adapted for the recognition task in hand. Finally, we present the design and the implementation of a floating-point gate array-based device to solve the practical problem of real-time handwritten digit classification from mobile camera video feed.},
	url = 	 {https://explore.openaire.eu/search/publication?articleId=od________18::a7b50ad6649ba7942600e9d20ecf2b80},
	doi={10.1109/EIConRus.2019.8656778}
}
@techreport{RefWorks:RefID:53-kreinar2018fast,
	author={Edward Kreinar and Jennifer Ngadiuba and Zhenbin Wu and Philip Harris and Maurizio Pierini and Ryan Rivera and Song Han and Javier Duarte and Benjamin Kreis and Nhan Tran and Sergo Jindariani},
	year={2018},
	month={Apr 16,},
	title={Fast inference of deep neural networks in FPGAs for particle physics},
	institution={IOP Publishing},
	number={13},
	pages={P07027},
	abstract={Recent results at the Large Hadron Collider (LHC) have pointed to enhanced physics capabilities through the improvement of the real-time event processing techniques. Machine learning methods are ubiquitous and have proven to be very powerful in LHC physics, and particle physics as a whole. However, exploration of the use of such techniques in low-latency, low-power FPGA (Field Programmable Gate Array) hardware has only just begun. FPGA-based trigger and data acquisition systems have extremely low, sub-microsecond latency requirements that are unique to particle physics. We present a case study for neural network inference in FPGAs focusing on a classifier for jet substructure which would enable, among many other physics scenarios, searches for new dark sector particles and novel measurements of the Higgs boson. While we focus on a specific example, the lessons are far-reaching. A companion compiler package for this work is developed based on High-Level Synthesis (HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS increases accessibility across a broad user community and allows for a drastic decrease in firmware development time. We map out FPGA resource usage and latency versus neural network hyperparameters to identify the problems in particle physics that would benefit from performing neural network inference with FPGAs. For our example jet substructure model, we fit well within the available resources of modern FPGAs with a latency on the scale of 100 ns.
Recent results at the Large Hadron Collider (LHC) have pointed to enhanced physics capabilities through the improvement of the real-time event processing techniques. Machine learning methods are ubiquitous and have proven to be very powerful in LHC physics, and particle physics as a whole. However, exploration of the use of such techniques in low-latency, low-power FPGA hardware has only just begun. FPGA-based trigger and data acquisition (DAQ) systems have extremely low, sub-microsecond latency requirements that are unique to particle physics. We present a case study for neural network inference in FPGAs focusing on a classifier for jet substructure which would enable, among many other physics scenarios, searches for new dark sector particles and novel measurements of the Higgs boson. While we focus on a specific example, the lessons are far-reaching. We develop a package based on High-Level Synthesis (HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS increases accessibility across a broad user community and allows for a drastic decrease in firmware development time. We map out FPGA resource usage and latency versus neural network hyperparameters to identify the problems in particle physics that would benefit from performing neural network inference with FPGAs. For our example jet substructure model, we fit well within the available resources of modern FPGAs with a latency on the scale of 100 ns.},
	isbn={1748-0221},
	url={http://cds.cern.ch/record/2316331},
	doi={10.1088/1748-0221/13/07/P07027}
}
@article{RefWorks:RefID:52-elfwing2018sigmoid-weighted,
	author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
	year={2018},
	month={Nov},
	title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
	journal={Neural networks},
	volume={107},
	pages={3-11},
	abstract={In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro’s TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10 × 10 board, using TD(λ) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(λ) agent with SiLU and dSiLU hidden units.},
	isbn={0893-6080},
	url={https://dx.doi.org/10.1016/j.neunet.2017.12.012},
	doi={10.1016/j.neunet.2017.12.012},
	pmid={29395652}
}
@article{RefWorks:RefID:51-ba2016layer,
	author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
	year={2016},
	month={Jul 21,},
	title={Layer Normalization},
	abstract={Training state-of-the-art, deep neural networks is computationally expensive.
One way to reduce the training time is to normalize the activities of the
neurons. A recently introduced technique called batch normalization uses the
distribution of the summed input to a neuron over a mini-batch of training
cases to compute a mean and variance which are then used to normalize the
summed input to that neuron on each training case. This significantly reduces
the training time in feed-forward neural networks. However, the effect of batch
normalization is dependent on the mini-batch size and it is not obvious how to
apply it to recurrent neural networks. In this paper, we transpose batch
normalization into layer normalization by computing the mean and variance used
for normalization from all of the summed inputs to the neurons in a layer on a
single training case. Like batch normalization, we also give each neuron its
own adaptive bias and gain which are applied after the normalization but before
the non-linearity. Unlike batch normalization, layer normalization performs
exactly the same computation at training and test times. It is also
straightforward to apply to recurrent neural networks by computing the
normalization statistics separately at each time step. Layer normalization is
very effective at stabilizing the hidden state dynamics in recurrent networks.
Empirically, we show that layer normalization can substantially reduce the
training time compared with previously published techniques.},
	url={https://arxiv.org/abs/1607.06450}
}
@misc{RefWorks:RefID:49-tappertriggering,
	author={Alex Tapper},
	title={Triggering at collider experiments},
	volume={2022},
	number={Jan 27,},
	url={http://www.hep.ph.imperial.ac.uk/~tapper/lecture/CMSIndia-2020.pdf}
}
@misc{RefWorks:RefID:48-trigger,
	title={Trigger, DAQ and FPGAs},
	volume={2022},
	number={Jan 27,},
	url={http://www.hep.ph.imperial.ac.uk/~tapper/lecture/trigger.pdf}
}
@misc{RefWorks:RefID:47-greeene2013higgs,
	author={Brian Greeene},
	year={2013},
	month={July},
	title={How the Higgs Boson Was Found},
	volume={2022},
	number={Jan 27,},
	url={https://www.smithsonianmag.com/science-nature/how-the-higgs-boson-was-found-4723520/}
}
@inproceedings{RefWorks:RefID:46-fan2018real-time,
	author={Hongxiang Fan and Shuanglong Liu and Martin Ferianc and Ho-Cheung Ng and Zhiqiang Que and Shen Liu and Xinyu Niu and Wayne Luk},
	year={Dec 2018},
	title={A Real-Time Object Detection Accelerator with Compressed SSDLite on FPGA},
	publisher={IEEE},
	pages={14-21},
	abstract={Convolutional neural network (CNN)-based object detection has been widely employed in various applications such as autonomous driving and intelligent video surveillance. However, the computational complexity of conventional convolution hinders its application in embedded systems. Recently, a mobile-friendly CNN model SSDLite-MobileNetV2 (SSDLiteM2) has been proposed for object detection. This model consists of a novel layer called bottleneck residual block (BRB). Although SSDLiteM2 contains far fewer parameters and computations than conventional CNN models, its performance on embedded devices still cannot meet the requirements of real-time processing. This paper proposes a novel FPGA-based architecture for SSDLiteM2 in combination with hardware optimizations including fused BRB, processing element (PE) sharing and load-balanced channel pruning. Moreover, a novel quantization scheme called partial quantization has been developed, which partially quantizes SSDLiteM2 to 8 bits with only 1.8% accuracy loss. Experiments show that the proposed design on a Xilinx ZC706 device can achieve up to 65 frames per second with 20.3 mean average precision on the COCO dataset.},
	url={https://ieeexplore.ieee.org/document/8742299},
	doi={10.1109/FPT.2018.00014}
}
@article{RefWorks:RefID:45-liang2021pruning,
	author={Tailin Liang and John Glossner and Lei Wang and Shaobo Shi and Xiaotong Zhang},
	year={2021},
	month={Oct 21,},
	title={Pruning and quantization for deep neural network acceleration: A survey},
	journal={Neurocomputing (Amsterdam)},
	volume={461},
	pages={370-403},
	abstract={Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.},
	isbn={0925-2312},
	url={https://dx.doi.org/10.1016/j.neucom.2021.07.045},
	doi={10.1016/j.neucom.2021.07.045}
}
@article{RefWorks:RefID:44-vaswani2017attention,
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2017},
	month={Jun 12,},
	title={Attention Is All You Need},
	abstract={The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.},
	url={https://arxiv.org/abs/1706.03762}
}
@techreport{RefWorks:RefID:42-kreinar2020distance-weighted,
	author={Edward Kreinar and Zhenbin Wu and Gianluca Cerminara and Kinga Wozniak and Gerrit Van Onsem and Marcel Rieger and Giuseppe Di Guglielmo and Jan Kieseler and Shah Rukh Qasim and Sioni Summers and Sergo Jindariani and Jennifer Ngadiuba and Mia Liu and Philip Harris and Maurizio Pierini and Vladimir Loncar and Kevin Pedro and Yutaro Iiyama and Javier Duarte and Dylan Rankin and Nhan Tran and Abhijay Gupta},
	year={2020},
	title={Distance-Weighted Graph Neural Networks on FPGAs for Real-Time Particle Reconstruction in High Energy Physics},
	institution={Frontiers Media S.A},
	number={3},
	pages={598927},
	abstract={Graph neural networks have been shown to achieve excellent performance for several crucial tasks in particle physics, such as charged particle tracking, jet tagging, and clustering. An important domain for the application of these networks is the FGPA-based first layer of real-time data filtering at the CERN Large Hadron Collider, which has strict latency and resource constraints. We discuss how to design distance-weighted graph networks that can be executed with a latency of less than 1$\mu\mathrm{s}$ on an FPGA. To do so, we consider a representative task associated to particle reconstruction and identification in a next-generation calorimeter operating at a particle collider. We use a graph network architecture developed for such purposes, and apply additional simplifications to match the computing constraints of Level-1 trigger systems, including weight quantization. Using the $\mathtt{hls4ml}$ library, we convert the compressed models into firmware to be implemented on an FPGA. Performance of the synthesized models is presented both in terms of inference accuracy and resource usage.},
	isbn={2624-909X},
	url={http://cds.cern.ch/record/2728798},
	doi={10.3389/fdata.2020.598927}
}
@article{RefWorks:RefID:41-elabd2021graph,
	author={Abdelrahman Elabd and Vesal Razavimaleki and Shi-Yu Huang and Javier Duarte and Markus Atkinson and Gage DeZoort and Peter Elmer and Jin-Xuan Hu and Shih-Chieh Hsu and Bo-Cheng Lai and Mark Neubauer and Isobel Ojalvo and Savannah Thais},
	year={2021},
	month={Dec 3,},
	title={Graph Neural Networks for Charged Particle Tracking on FPGAs},
	abstract={The determination of charged particle trajectories in collisions at the CERN
Large Hadron Collider (LHC) is an important but challenging problem, especially
in the high interaction density conditions expected during the future
high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a
type of geometric deep learning algorithm that has successfully been applied to
this task by embedding tracker data as a graph -- nodes represent hits, while
edges represent possible track segments -- and classifying the edges as true or
fake track segments. However, their study in hardware- or software-based
trigger applications has been limited due to their large computational cost. In
this paper, we introduce an automated translation workflow, integrated into a
broader tool called $\texttt{hls4ml}$, for converting GNNs into firmware for
field-programmable gate arrays (FPGAs). We use this translation tool to
implement GNNs for charged particle tracking, trained using the TrackML
challenge dataset, on FPGAs with designs targeting different graph sizes, task
complexites, and latency/throughput requirements. This work could enable the
inclusion of charged particle tracking GNNs at the trigger level for HL-LHC
experiments.},
	url={https://arxiv.org/abs/2112.02048}
}
@article{RefWorks:RefID:40-ren2020unveiling,
	author={Jie Ren and Lei Wu and Jin Min Yang},
	year={2020},
	month={Mar 10,},
	title={Unveiling CP property of top-Higgs coupling with graph neural networks at the LHC},
	journal={Physics letters. B},
	volume={802},
	pages={135198},
	abstract={The top-Higgs coupling plays an important role in particle physics and cosmology. The precision measurements of this coupling can provide an insight to new physics beyond the Standard Model. In this paper, we propose to use Message Passing Neural Network (MPNN) to reveal the CP nature of top-Higgs interaction through semi-leptonic channel pp→t(→bℓ−νℓ)t¯(→b¯jj)h(→bb¯). Using the test statistics constructed from the event classification probabilities given by the MPNN, we find that the pure CP-even and CP-odd components can be well distinguished at the LHC, with at most 300 fb−1 experimental data.},
	isbn={0370-2693},
	url={https://dx.doi.org/10.1016/j.physletb.2020.135198},
	doi={10.1016/j.physletb.2020.135198}
}
@techreport{RefWorks:RefID:39-skoczeń2016lstm,
	author={Andrzej Skoczeń and Maciej Wielgosz and Matej Mertik},
	year={2016},
	month={Nov 18,},
	title={Using LSTM recurrent neural networks for monitoring the LHC superconducting magnets},
	institution={Elsevier B.V},
	number={867},
	pages={40-50},
	abstract={The superconducting LHC magnets are coupled with an electronic monitoring system which records and analyses voltage time series reflecting their performance. A currently used system is based on a range of preprogrammed triggers which launches protection procedures when a misbehavior of the magnets is detected. All the procedures used in the protection equipment were designed and implemented according to known working scenarios of the system and are updated and monitored by human operators. This paper proposes a novel approach to monitoring and fault protection of the Large Hadron Collider (LHC) superconducting magnets which employs state-of-the-art Deep Learning algorithms. Consequently, the authors of the paper decided to examine the performance of LSTM recurrent neural networks for modeling of voltage time series of the magnets. In order to address this challenging task different network architectures and hyper-parameters were used to achieve the best possible performance of the solution. The regression results were measured in terms of RMSE for different number of future steps and history length taken into account for the prediction. The best result of RMSE=0.00104 was obtained for a network of 128 LSTM cells within the internal layer and 16 steps history buffer.
The superconducting LHC magnets are coupled with an electronic monitoring system which records and analyzes voltage time series reflecting their performance. A currently used system is based on a range of preprogrammed triggers which launches protection procedures when a misbehavior of the magnets is detected. All the procedures used in the protection equipment were designed and implemented according to known working scenarios of the system and are updated and monitored by human operators.},
	isbn={0168-9002},
	url={http://cds.cern.ch/record/2234465},
	doi={10.1016/j.nima.2017.06.020}
}
@inproceedings{RefWorks:RefID:38-valentino2012classification,
	author={G. Valentino and R. W. Assmann and R. Bruce and N. Sammut},
	year={Jan 2012},
	title={Classification of LHC beam loss spikes using Support Vector Machines},
	publisher={IEEE},
	pages={355-358},
	abstract={The CERN Large Hadron Collider's (LHC) collimation system is the most complex beam cleaning system ever designed. It requires frequent setups to determine the beam centres and beam sizes at the 86 collimator positions. A collimator jaw is aligned to the beam halo when a clear beam loss spike is detected on a Beam Loss Monitor (BLM) downstream of the collimator. This paper presents a technique for identifying such clear loss spikes with the aid of Support Vector Machines. The training data was gathered from setups held during the first three months of the 2011 LHC run, and the model was tested with data from a machine development period.},
	url={https://ieeexplore.ieee.org/document/6208988},
	doi={10.1109/SAMI.2012.6208988}
}
@techreport{RefWorks:RefID:36-kreinar2018fast,
	author={Edward Kreinar and Jennifer Ngadiuba and Zhenbin Wu and Philip Harris and Maurizio Pierini and Ryan Rivera and Song Han and Javier Duarte and Benjamin Kreis and Nhan Tran and Sergo Jindariani},
	year={2018},
	month={Apr 16,},
	title={Fast inference of deep neural networks in FPGAs for particle physics},
	institution={Institute of Physics (IOP)},
	number={13},
	pages={P07027},
	abstract={Recent results at the Large Hadron Collider (LHC) have pointed to enhanced physics capabilities through the improvement of the real-time event processing techniques. Machine learning methods are ubiquitous and have proven to be very powerful in LHC physics, and particle physics as a whole. However, exploration of the use of such techniques in low-latency, low-power FPGA (Field Programmable Gate Array) hardware has only just begun. FPGA-based trigger and data acquisition systems have extremely low, sub-microsecond latency requirements that are unique to particle physics. We present a case study for neural network inference in FPGAs focusing on a classifier for jet substructure which would enable, among many other physics scenarios, searches for new dark sector particles and novel measurements of the Higgs boson. While we focus on a specific example, the lessons are far-reaching. A companion compiler package for this work is developed based on High-Level Synthesis (HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS increases accessibility across a broad user community and allows for a drastic decrease in firmware development time. We map out FPGA resource usage and latency versus neural network hyperparameters to identify the problems in particle physics that would benefit from performing neural network inference with FPGAs. For our example jet substructure model, we fit well within the available resources of modern FPGAs with a latency on the scale of 100 ns.
Recent results at the Large Hadron Collider (LHC) have pointed to enhanced physics capabilities through the improvement of the real-time event processing techniques. Machine learning methods are ubiquitous and have proven to be very powerful in LHC physics, and particle physics as a whole. However, exploration of the use of such techniques in low-latency, low-power FPGA hardware has only just begun. FPGA-based trigger and data acquisition (DAQ) systems have extremely low, sub-microsecond latency requirements that are unique to particle physics. We present a case study for neural network inference in FPGAs focusing on a classifier for jet substructure which would enable, among many other physics scenarios, searches for new dark sector particles and novel measurements of the Higgs boson. While we focus on a specific example, the lessons are far-reaching. We develop a package based on High-Level Synthesis (HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS increases accessibility across a broad user community and allows for a drastic decrease in firmware development time. We map out FPGA resource usage and latency versus neural network hyperparameters to identify the problems in particle physics that would benefit from performing neural network inference with FPGAs. For our example jet substructure model, we fit well within the available resources of modern FPGAs with a latency on the scale of 100 ns.},
	isbn={1748-0221},
	url={http://cds.cern.ch/record/2316331},
	doi={10.1088/1748-0221/13/07/P07027}
}
@article{RefWorks:RefID:35-cacciari2008anti-kt,
	author={Matteo Cacciari and Gavin P. Salam and Gregory Soyez},
	year={2008},
	month={Apr 1,},
	title={The anti-kt jet clustering algorithm},
	journal={The journal of high energy physics},
	volume={2008},
	pages={063},
	isbn={1126-6708},
	url={http://iopscience.iop.org/1126-6708/2008/04/063},
	doi={10.1088/1126-6708/2008/04/063}
}
@misc{RefWorks:RefID:34-pierinihls4ml,
	author={Maurizio Pierini and Javier Mauricio Duarte and Nhan Tran and Marat Freytsis},
	title={HLS4ML LHC Jet dataset (150 particles)},
	volume={2022},
	number={Jan 27,},
	url={https://doi.org/10.5281/zenodo.3602260}
}
@misc{RefWorks:RefID:33-pierinihls4ml,
	author={Maurizio Pierini and Javier Mauricio Duarte and Nhan Tran and Marat Freytsis},
	title={HLS4ML LHC Jet dataset (100 particles)},
	volume={2022},
	number={Jan 27,},
	url={https://doi.org/10.5281/zenodo.3602254}
}
@misc{RefWorks:RefID:32-pierinihls4ml,
	author={Maurizio Pierini and Javier Mauricio Duarte and Nhan Tran and Marat Freytsis},
	title={HLS4ML LHC Jet dataset (50 particles)},
	volume={2022},
	number={Jan 27,},
	url={https://doi.org/10.5281/zenodo.3601443}
}
@misc{RefWorks:RefID:31-pierinihls4ml,
	author={Maurizio Pierini and Javier Mauricio Duarte and Nhan Tran and Marat Freytsis},
	title={HLS4ML LHC Jet dataset (30 particles)},
	volume={2022},
	number={Jan 27,},
	url={https://doi.org/10.5281/zenodo.3601436}
}
@article{RefWorks:RefID:30-lahti2019yet?,
	author={Sakari Lahti and Panu Sjovall and Jarno Vanne and Timo D. Hamalainen},
	year={2019},
	month={May},
	title={Are We There Yet? A Study on the State of High-Level Synthesis},
	journal={IEEE transactions on computer-aided design of integrated circuits and systems},
	volume={38},
	number={5},
	pages={898-911},
	abstract={To increase productivity in designing digital hardware components, high-level synthesis (HLS) is seen as the next step in raising the design abstraction level. However, the quality of results (QoRs) of HLS tools has tended to be behind those of manual register-transfer level (RTL) flows. In this paper, we survey the scientific literature published since 2010 about the QoR and productivity differences between the HLS and RTL design flows. Altogether, our survey spans 46 papers and 118 associated applications. Our results show that on average, the QoR of RTL flow is still better than that of the state-of-the-art HLS tools. However, the average development time with HLS tools is only a third of that of the RTL flow, and a designer obtains over four times as high productivity with HLS. Based on our findings, we also present a model case study to sum up the best practices in comparative studies between HLS and RTL. The outcome of our case study is also in line with the survey results, as using an HLS tool is seen to increase the productivity by a factor of six. In addition, to help close the QoR gap, we present a survey of literature focused on improving HLS. Our results let us conclude that HLS is currently a viable option for fast prototyping and for designs with short time to market.},
	isbn={0278-0070},
	url={https://ieeexplore.ieee.org/document/8356004},
	doi={10.1109/TCAD.2018.2834439}
}
@article{RefWorks:RefID:28-li2018gpu-outperforming,
	author={Yixing Li and Zichuan Liu and Kai Xu and Hao Yu and Fengbo Ren},
	year={2018},
	month={Jul 27,},
	title={A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks},
	journal={ACM journal on emerging technologies in computing systems},
	volume={14},
	number={2},
	pages={1-16},
	abstract={FPGA-based hardware accelerators for convolutional neural networks (CNNs) have received attention due to their higher energy efficiency than GPUs. However, it is challenging for FPGA-based solutions to achieve a higher throughput than GPU counterparts. In this article, we demonstrate that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is trained with binary constraints on weights and activations. Specifically, we propose an optimized fully mapped FPGA accelerator architecture tailored for bitwise convolution and normalization that features massive spatial parallelism with deep pipelines stages. A key advantage of the FPGA accelerator is that its performance is insensitive to data batch size, while the performance of GPU acceleration varies largely depending on the batch size of the data. Experiment results show that the proposed accelerator architecture for binary CNNs running on a Virtex-7 FPGA is 8.3× faster and 75× more energy-efficient than a Titan X GPU for processing online individual requests in small batch sizes. For processing static data in large batch sizes, the proposed solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5× higher energy efficiency.},
	isbn={1550-4832},
	url={http://dl.acm.org/citation.cfm?id=3154839},
	doi={10.1145/3154839}
}
@inproceedings{RefWorks:RefID:27-nurvitadhi2017fpgas,
	author={Eriko Nurvitadhi and Ganesh Venkatesh and Jaewoong Sim and Debbie Marr and Randy Huang and Jason Ong Gee Hock and Yeong Tat Liew and Krishnan Srivatsan and Duncan Moss and Suchit Subhaschandra and Guy Boudoukh},
	year={Feb 22, 2017},
	title={Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks?},
	series={FPGA '17},
	publisher={ACM},
	pages={5-14},
	abstract={Current-generation Deep Neural Networks (DNNs), such as AlexNet and VGG, rely heavily on dense floating-point matrix multiplication (GEMM), which maps well to GPUs (regular parallelism, high TFLOP/s). Because of this, GPUs are widely used for accelerating DNNs. Current FPGAs offer superior energy efficiency (Ops/Watt), but they do not offer the performance of today's GPUs on DNNs. In this paper, we look at upcoming FPGA technology advances, the rapid pace of innovation in DNN algorithms, and consider whether future high-performance FPGAs will outperform GPUs for next-generation DNNs. The upcoming Intel® 14-nm Stratix? 10 FPGAs will have thousands of hard floating-point units (DSPs) and on-chip RAMs (M20K memory blocks). They will also have high bandwidth memories (HBMs) and improved frequency (HyperFlex? core architecture). This combination of features brings FPGA raw floating point performance within striking distance of GPUs. Meanwhile, DNNs are quickly evolving. For example, recent innovations that exploit sparsity (e.g., pruning) and compact data types (e.g., 1-2 bit) result in major leaps in algorithmic efficiency. However, these innovations introduce irregular parallelism on custom data types, which are difficult for GPUs to handle but would be a great fit for FPGA's extreme customizability.
This paper evaluates a selection of emerging DNN algorithms on two generations of Intel FPGAs (Arria'10, Stratix'10) against the latest highest performance Titan X Pascal GPU. We created a customizable DNN accelerator template for FPGAs and used it in our evaluations. First, we study various GEMM operations for next-generation DNNs. Our results show that Stratix 10 FPGA is 10%, 50%, and 5.4x better in performance (TOP/sec) than Titan X Pascal GPU on GEMM operations for pruned, Int6, and binarized DNNs, respectively. Then, we present a detailed case study on accelerating Ternary ResNet which relies on sparse GEMM on 2-bit weights (i.e., weights constrained to 0,+1,-1) and full-precision neurons. The Ternary ResNet accuracy is within ~1% of the full-precision ResNet which won the 2015 ImageNet competition. On Ternary-ResNet, the Stratix 10 FPGA can deliver 60% better performance over Titan X Pascal GPU, while being 2.3x better in performance/watt. Our results indicate that FPGAs may become the platform of choice for accelerating next-generation DNNs.},
	url={http://dl.acm.org/citation.cfm?id=3021740},
	doi={10.1145/3020078.3021740}
}
@inproceedings{RefWorks:RefID:26-nurvitadhi2016accelerating,
	author={Eriko Nurvitadhi and David Sheffield and Jaewoong Sim and Asit Mishra and Ganesh Venkatesh and Debbie Marr},
	year={Dec 2016},
	title={Accelerating Binarized Neural Networks: Comparison of FPGA, CPU, GPU, and ASIC},
	publisher={IEEE},
	pages={77-84},
	abstract={Deep neural networks (DNNs) are widely used in data analytics, since they deliver state-of-the-art accuracies. Binarized neural networks (BNNs) are recently proposed optimized variant of DNNs. BNNs constraint network weight and/or neuron value to either +1 or -1, which is representable in 1 bit. This leads to dramatic algorithm efficiency improvement, due to reduction in the memory and computational demands. This paper evaluates the opportunity to further improve the execution efficiency of BNNs through hardware acceleration. We first proposed a BNN hardware accelerator design. Then, we implemented the proposed accelerator on Aria 10 FPGA as well as 14-nm ASIC, and compared them against optimized software on Xeon server CPU, Nvidia Titan X server GPU, and Nvidia TX1 mobile GPU. Our evaluation shows that FPGA provides superior efficiency over CPU and GPU. Even though CPU and GPU offer high peak theoretical performance, they are not as efficiently utilized since BNNs rely on binarized bit-level operations that are better suited for custom hardware. Finally, even though ASIC is still more efficient, FPGA can provide orders of magnitudes in efficiency improvements over software, without having to lock into a fixed ASIC solution.},
	url={https://ieeexplore.ieee.org/document/7929192},
	doi={10.1109/FPT.2016.7929192}
}
@article{RefWorks:RefID:25-boutros2018improve,
	author={Andrew Boutros and Sadegh Yazdanshenas and Vaughn Betz},
	year={2018},
	month={Dec 22,},
	title={You Cannot Improve What You Do not Measure},
	journal={ACM transactions on reconfigurable technology and systems},
	volume={11},
	number={3},
	pages={1-23},
	abstract={Recently, deep learning (DL) has become best-in-class for numerous applications but at a high computational cost that necessitates high-performance energy-efficient acceleration. The reconfigurability of FPGAs is appealing due to the rapid change in DL models but also causes lower performance and area-efficiency compared to ASICs. In this article, we implement three state-of-the-art computing architectures (CAs) for convolutional neural network (CNN) inference on FPGAs and ASICs. By comparing the FPGA and ASIC implementations, we highlight the area and performance costs of programmability to pinpoint the inefficiencies in current FPGA architectures. We perform our experiments using three variations of these CAs for AlexNet, VGG-16 and ResNet-50 to allow extensive comparisons. We find that the performance gap varies significantly from 2.8× to 6.3×, while the area gap is consistent across CAs with an 8.7 average FPGA-to-ASIC area ratio. Among different blocks of the CAs, the convolution engine, constituting up to 60% of the total area, has a high area ratio ranging from 13 to 31. Motivated by our FPGA vs. ASIC comparisons, we suggest FPGA architectural changes such as increasing DSP block count, enhancing low-precision support in DSP blocks and rethinking the on-chip memories to reduce the programmability gap for DL applications.},
	isbn={1936-7406},
	url={http://dl.acm.org/citation.cfm?id=3242898},
	doi={10.1145/3242898}
}
@article{RefWorks:RefID:24-ramanaiah2011asic,
	author={K. Venkata Ramanaiah and Cyril Prasanna Raj},
	year={2011},
	title={ASIC Implementation of Neural Network Based Image Compression},
	journal={International Journal of Computer Theory and Engineering},
	pages={494-498},
	isbn={1793-8201},
	doi={10.7763/IJCTE.2011.V3.356}
}
@article{RefWorks:RefID:23-knag2015sparse,
	author={Phil Knag and Jung Kuk Kim and Thomas Chen and Zhengya Zhang},
	year={2015},
	month={Apr},
	title={A Sparse Coding Neural Network ASIC With On-Chip Learning for Feature Extraction and Encoding},
	journal={IEEE journal of solid-state circuits},
	volume={50},
	number={4},
	pages={1070-1079},
	abstract={Hardware-based computer vision accelerators will be an essential part of future mobile devices to meet the low power and real-time processing requirement. To realize a high energy efficiency and high throughput, the accelerator architecture can be massively parallelized and tailored to vision processing, which is an advantage over software-based solutions and general-purpose hardware. In this work, we present an ASIC that is designed to learn and extract features from images and videos. The ASIC contains 256 leaky integrate-and-fire neurons connected in a scalable two-layer network of 8 × 8 grids linked in a 4-stage ring. Sparse neuron activation and the relatively small grid keep the spike collision probability low to save access arbitration. The weight memory is divided into core memory and auxiliary memory, such that the auxiliary memory is only powered on for learning to save inference power. High-throughput inference is accomplished by the parallel operation of neurons. Efficient learning is implemented by passing parameter update messages, which is further simplified by an approximation technique. A 3.06 mm 2 65 nm CMOS ASIC test chip is designed to achieve a maximum inference throughput of 1.24 Gpixel/s at 1.0 V and 310 MHz, and on-chip learning can be completed in seconds. To improve the power consumption and energy efficiency, core memory supply voltage can be reduced to 440 mV to take advantage of the error resilience of the algorithm, reducing the inference power to 6.67 mW for a 140 Mpixel/s throughput at 35 MHz.},
	isbn={0018-9200},
	url={https://ieeexplore.ieee.org/document/7015626},
	doi={10.1109/JSSC.2014.2386892}
}
@misc{RefWorks:RefID:22-graphcoregraphcore,
	author={Graphcore},
	title={Graphcore Intelligence Processing Unit},
	volume={2022},
	number={Jan 26,},
	url={https://www.graphcore.ai/products/ipu}
}
@article{RefWorks:RefID:21-zhang2019recent,
	author={Qianru Zhang and Meng Zhang and Tinghuan Chen and Zhifei Sun and Yuzhe Ma and Bei Yu},
	year={2019},
	month={Jan 5,},
	title={Recent advances in convolutional neural network acceleration},
	journal={Neurocomputing (Amsterdam)},
	volume={323},
	pages={37-51},
	abstract={In recent years, convolutional neural networks (CNNs) have shown great performance in various fields such as image classification, pattern recognition, and multi-media compression. Two of the feature properties, local connectivity and weight sharing, can reduce the number of parameters and increase processing speed during training and inference. However, as the dimension of data becomes higher and the CNN architecture becomes more complicated, the end-to-end approach or the combined manner of CNN is computationally intensive, which becomes limitation to CNN’s further implementation. Therefore, it is necessary and urgent to implement CNN in a faster way. In this paper, we first summarize the acceleration methods that contribute to but not limited to CNN by reviewing a broad variety of research papers. We propose a taxonomy in terms of three levels, i.e. structure level, algorithm level, and implementation level, for acceleration methods. We also analyze the acceleration methods in terms of CNN architecture compression, algorithm optimization, and hardware-based improvement. At last, we give a discussion on different perspectives of these acceleration and optimization methods within each level. The discussion shows that the methods in each level still have large exploration space. By incorporating such a wide range of disciplines, we expect to provide a comprehensive reference for researchers who are interested in CNN acceleration.},
	isbn={0925-2312},
	url={https://dx.doi.org/10.1016/j.neucom.2018.09.038},
	doi={10.1016/j.neucom.2018.09.038}
}
@article{RefWorks:RefID:20-chen2020gpu-accelerated,
	author={Gang Chen and Haitao Meng and Yucheng Liang and Kai Huang},
	year={2020},
	month={Dec 1,},
	title={GPU-Accelerated Real-Time Stereo Estimation With Binary Neural Network},
	journal={IEEE transactions on parallel and distributed systems},
	volume={31},
	number={12},
	pages={2896-2907},
	abstract={Depth estimation from stereo images is essential to many applications such as robotics and autonomous vehicles, most of which ask for the real-time response, high energy and storage efficiency. Recent work has shown deep neural networks (DNN) perform extremely well for stereo estimation. However, these state-of-the-art DNN based algorithms are challenging to be deployed into real-world applications due to the high computational complexities of DNNs. Most of them are too slow for real-time inference and require several seconds of GPU computation to process image frames. In this article, we address the problem of fast stereo estimation and propose an efficient and light-weighted stereo matching system, called StereoBit, to produce a disparity map in a real-time manner while achieving close to state-of-the-art accuracy. To achieve this goal, we propose a binary neural network to generate weighted Hamming distance for an efficient similarity join in stereo estimation. In addition, we propose a novel approximation approach to derive StereoBit network directly from the well-trained network with the cosine similarity. Our approximation strategies enable a significant speedup while maintaining almost the same accuracy compared to the network with the cosine similarity. Furthermore, we present an optimization framework for fully exploiting the computing power of StereoBit. The framework provides a significant speedup of stereo estimation routines, and at the same time, reduces the memory usage for storing parameters. The effectiveness of StereoBit is evaluated by comprehensive experiments. StereoBit can achieve 60 frames per second on an NVIDIA TITAN Xp GPU on KITTI 2012 benchmark while achieving 3-pixel non-occluded stereo error 3.56 percent.},
	isbn={1045-9219},
	url={https://ieeexplore.ieee.org/document/9130887},
	doi={10.1109/TPDS.2020.3006238}
}
@inproceedings{RefWorks:RefID:19-iyer2018gpu,
	author={Sainathan Ganesh Iyer and Anurag Dipakumar Pawar},
	year={Aug 2018},
	title={GPU and CPU Accelerated Mining of Cryptocurrencies and their Financial Analysis},
	publisher={IEEE},
	pages={599-604},
	abstract={When Bitcoin's price skyrocketed then cryptocurrencies dragged the world's attention. Since then "Cryptocurrency Mining" has became a buzz word for computer geeks as well as for those who wanted to add a new source of income. "Mining" is a process of solving computational problems using hardware which results in obtaining rewards in the form digital currency. Large number of people who could afford a personal computer, started getting attracted towards mining, which resulted in formation of "Mining Pools". Mining Pools are an explicit example of teamwork where miners share their efforts to mine cryptocurrencies. This paper throws light on some of the key concepts of mining such as hash rates, pool analytics and various mining software. A complete financial report is enclosed in this paper which discusses the profitability or non-profitability of various cryptocurrencies and hardware by considering various attributes like pool fee, electricity costs, and revenues excluding the cost of hardware. This paper is an outcome of practical mining of several cryptocurrencies on GPUs and CPUs (non-custom hardware) which results in contextually slower mining as compared to high-end miners. Proposed data will guide novice miners to start mining.},
	url={https://ieeexplore.ieee.org/document/8653733},
	doi={10.1109/I-SMAC.2018.8653733}
}
@inproceedings{RefWorks:RefID:17-dagli1989applications,
	author={Dagli and Lammers},
	year={1989},
	title={Possible applications of neural networks in manufacturing},
	publisher={IEEE TAB Neural Network Committee},
	pages={605 vol.2},
	abstract={Summary form only given. An examination is made of the potential of neural networks and the impact of parallel processing in the design and operations of manufacturing systems. After an initial discussion on possible areas of application, an approach that integrates artificial intelligence, operations research, and neural networks for the solution of a scheduling problem is examined.< >},
	url={https://ieeexplore.ieee.org/document/118423},
	doi={10.1109/IJCNN.1989.118423}
}
@article{RefWorks:RefID:16-wu1995neural,
	author={Cathy Wu and Michael Berry and Sailaja Shivakumar and Jerry McLarty},
	year={1995},
	month={Oct 1,},
	title={Neural Networks for Full-Scale Protein Sequence Classification: Sequence Encoding with Singular Value Decomposition},
	journal={Machine learning},
	volume={21},
	number={1},
	pages={177},
	abstract={Byline: Cathy Wu (1), Michael Berry (2), Sailaja Shivakumar (3), Jerry McLarty (3) Keywords: neural networks; database search; protein classification; sequence analysis; superfamily; singular value decomposition (SVD) A neural network classification method has been developed as an alternative approach to the search/organization problem of protein sequence databases. The neural networks used are three-layered, feed-forward, back-propagation networks. The protein sequences are encoded into neural input vectors by a hashing method that counts occurrences of n-gram words. A new SVD (singular value decomposition) method, which compresses the long and sparse n-gram input vectors and captures semantics of n-gram words, has improved the generalization capability of the network. A full-scale protein classification system has been implemented on a Cray supercomputer to classify unknown sequences into 3311 PIR (Protein Identification Resource) superfamilies/families at a speed of less than 0.05 CPU second per sequence. The sensitivity is close to 90% overall, and approaches 100% for large superfamilies. The system could be used to reduce the database search time and is being used to help organize the PIR protein sequence database. Author Affiliation: Article History: Registration Date: 26/08/2004},
	isbn={0885-6125},
	doi={10.1023/A:1022677900508}
}
@inproceedings{RefWorks:RefID:15-nurvitadhi2016accelerating,
	author={Eriko Nurvitadhi and Jaewoong Sim and David Sheffield and Asit Mishra and Srivatsan Krishnan and Debbie Marr},
	year={Aug 2016},
	title={Accelerating recurrent neural networks in analytics servers: Comparison of FPGA, CPU, GPU, and ASIC},
	publisher={EPFL},
	pages={1-4},
	abstract={Recurrent neural networks (RNNs) provide state-of-the-art accuracy for performing analytics on datasets with sequence (e.g., language model). This paper studied a state-of-the-art RNN variant, Gated Recurrent Unit (GRU). We first proposed memoization optimization to avoid 3 out of the 6 dense matrix vector multiplications (SGEMVs) that are the majority of the computation in GRU. Then, we study the opportunities to accelerate the remaining SGEMVs using FPGAs, in comparison to 14-nm ASIC, GPU, and multi-core CPU. Results show that FPGA provides superior performance/Watt over CPU and GPU because FPGA's on-chip BRAMs, hard DSPs, and reconfigurable fabric allow for efficiently extracting fine-grained parallelisms from small/medium size matrices used by GRU. Moreover, newer FPGAs with more DSPs, on-chip BRAMs, and higher frequency have the potential to narrow the FPGA-ASIC efficiency gap.},
	url={https://ieeexplore.ieee.org/document/7577314},
	doi={10.1109/FPL.2016.7577314}
}
@inproceedings{RefWorks:RefID:14-najafi2017hardware,
	author={Mohammadreza Najafi and Kaiwen Zhang and Mohammad Sadoghi and Hans-Arno Jacobsen},
	year={Jun 2017},
	title={Hardware Acceleration Landscape for Distributed Real-Time Analytics: Virtues and Limitations},
	publisher={IEEE},
	pages={1938-1948},
	abstract={We are witnessing a technological revolution with a broad impact ranging from daily life (e.g., personalized medicine and education) to industry (e.g., data-driven healthcare, commerce, agriculture, and mining). At the core of this transformation lies "data". This transformation is facilitated by embedded devices, collectively known as Internet of Things (IoT), which produce real-time feeds of sensor data which are collected and processed to produce a dynamic physical model used for optimized real-time decision making. At the infrastructure level, there is a need to develop a scalable architecture for processing massive volumes of present and historical data at an unprecedented velocity to support the IoT paradigm. To cope with such extreme scale, we argue for the need to revisit the hardware and software co-design landscape in light of two key technological advancements. First is the virtualization of computation and storage over highly distributed data centers spanning across continents. Second is the emergence of a variety of specialized hardware accelerators that complement traditional general-purpose processors. Further efforts are required to unify these two trends in order to harness the power of big data. In this paper, we present a formulation and characterization of the hardware acceleration landscape geared towards real-time analytics in the cloud. Our goal is to assist both researchers and practitioners navigating the newly revived field of software and hardware co-design for building next generation distributed systems. We further present a case study to explore software and hardware interplay for designing distributed real-time stream processing.},
	isbn={1063-6927},
	url={https://ieeexplore.ieee.org/document/7980135},
	doi={10.1109/ICDCS.2017.194}
}
@article{RefWorks:RefID:11-elabd2021graph,
	author={Abdelrahman Elabd and Vesal Razavimaleki and Shi-Yu Huang and Javier Duarte and Markus Atkinson and Gage DeZoort and Peter Elmer and Jin-Xuan Hu and Shih-Chieh Hsu and Bo-Cheng Lai and Mark Neubauer and Isobel Ojalvo and Savannah Thais},
	year={2021},
	month={Dec 3,},
	title={Graph Neural Networks for Charged Particle Tracking on FPGAs},
	abstract={The determination of charged particle trajectories in collisions at the CERN
Large Hadron Collider (LHC) is an important but challenging problem, especially
in the high interaction density conditions expected during the future
high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a
type of geometric deep learning algorithm that has successfully been applied to
this task by embedding tracker data as a graph -- nodes represent hits, while
edges represent possible track segments -- and classifying the edges as true or
fake track segments. However, their study in hardware- or software-based
trigger applications has been limited due to their large computational cost. In
this paper, we introduce an automated translation workflow, integrated into a
broader tool called $\texttt{hls4ml}$, for converting GNNs into firmware for
field-programmable gate arrays (FPGAs). We use this translation tool to
implement GNNs for charged particle tracking, trained using the TrackML
challenge dataset, on FPGAs with designs targeting different graph sizes, task
complexites, and latency/throughput requirements. This work could enable the
inclusion of charged particle tracking GNNs at the trigger level for HL-LHC
experiments.},
	url={https://arxiv.org/abs/2112.02048}
}
@article{RefWorks:RefID:9-newman2019jedi-net:,
	author={Harvey B. Newman and Avikar Periwal and Maria Spiropulu and Javier M. Duarte and Maurizio Pierini and Eric A. Moreno and Aidana Serikova and Olmo Cerri and Jean-Roch Vlimant and Thong Q. Nguyen},
	year={2019},
	month={Aug 14,},
	title={JEDI-net: a jet identification algorithm based on interaction networks},
	journal={The European physical journal. C, Particles and fields},
	volume={80},
	number={1},
	pages={1-15},
	abstract={We investigate the performance of a jet identification algorithm based on interaction networks (JEDI-net) to identify all-hadronic decays of high-momentum heavy particles produced at the LHC and distinguish them from ordinary jets originating from the hadronization of quarks and gluons. The jet dynamics are described as a set of one-to-one interactions between the jet constituents. Based on a representation learned from these interactions, the jet is associated to one of the considered categories. Unlike other architectures, the JEDI-net models achieve their performance without special handling of the sparse input jet representation, extensive pre-processing, particle ordering, or specific assumptions regarding the underlying detector geometry. The presented models give better results with less model parameters, offering interesting prospects for LHC applications.},
	isbn={1434-6044},
	url={http://cds.cern.ch/record/2688535},
	doi={10.1140/epjc/s10052-020-7608-4}
}
@article{RefWorks:RefID:8-fahim2021hls4ml:,
	author={Farah Fahim and Benjamin Hawks and Christian Herwig and James Hirschauer and Sergo Jindariani and Nhan Tran and Luca P. Carloni and Giuseppe Di Guglielmo and Philip Harris and Jeffrey Krupa and Dylan Rankin and Manuel Blanco Valentin and Josiah Hester and Yingyi Luo and John Mamish and Seda Orgrenci-Memik and Thea Aarrestad and Hamza Javed and Vladimir Loncar and Maurizio Pierini and Adrian Alan Pol and Sioni Summers and Javier Duarte and Scott Hauck and Shih-Chieh Hsu and Jennifer Ngadiuba and Mia Liu and Duc Hoang and Edward Kreinar and Zhenbin Wu},
	year={2021},
	month={Mar 9,},
	title={hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices},
	abstract={Accessible machine learning algorithms, software, and diagnostic tools for
energy-efficient devices and systems are extremely valuable across a broad
range of application domains. In scientific domains, real-time near-sensor
processing can drastically improve experimental design and accelerate
scientific discoveries. To support domain scientists, we have developed hls4ml,
an open-source software-hardware codesign workflow to interpret and translate
machine learning algorithms for implementation with both FPGA and ASIC
technologies. We expand on previous hls4ml work by extending capabilities and
techniques towards low-power implementations and increased usability: new
Python APIs, quantization-aware pruning, end-to-end FPGA workflows, long
pipeline kernels for low power, and new device backends include an ASIC
workflow. Taken together, these and continued efforts in hls4ml will arm a new
generation of domain scientists with accessible, efficient, and powerful tools
for machine-learning-accelerated discovery.},
	url={https://arxiv.org/abs/2103.05579}
}
@article{RefWorks:RefID:7-moore2019reports,
	author={Liam Moore and Karl Nordström and Sreedevi Varma and Malcolm Fairbairn},
	year={2019},
	month={Sep 24,},
	title={Reports of my demise are greatly exaggerated: $N$-subjettiness taggers take on jet images},
	journal={SciPost physics},
	volume={7},
	number={3},
	pages={036},
	abstract={We compare the performance of a convolutional neural network (CNN) trained on jet images with dense neural networks (DNNs) trained on nn-subjettiness variables to study the distinguishing power of these two separate techniques applied to top quark decays. We find that they perform almost identically and are highly correlated once jet mass information is included, which suggests they are accessing the same underlying information which can be intuitively understood as being contained in 4-, 5-, 6-, and 8-body kinematic phase spaces depending on the sample. This suggests both of these methods are highly useful for heavy object tagging and provides a tentative answer to the question of what the image network is actually learning.},
	isbn={2542-4653},
	url={https://hal.archives-ouvertes.fr/hal-01851157},
	doi={10.21468/SciPostPhys.7.3.036}
}
@article{RefWorks:RefID:6-de2016jet-images,
	author={Luke de Oliveira and Michael Kagan and Lester Mackey and Benjamin Nachman and Ariel Schwartzman},
	year={2016},
	month={Jul 13,},
	title={Jet-images - deep learning edition},
	journal={The journal of high energy physics},
	volume={2016},
	number={7},
	pages={1-32},
	abstract={A
bstract
Building on the notion of a particle physics detector as a camera and the collimated streams of high energy particles, or jets, it measures as an image, we investigate the potential of machine learning techniques based on deep learning architectures to identify highly boosted
W
bosons. Modern deep learning algorithms trained on
jet images
can out-perform standard physically-motivated feature driven approaches to jet tagging. We develop techniques for visualizing how these features are learned by the network and what additional information is used to improve performance. This interplay between physicallymotivated feature driven tools and supervised learning algorithms is general and can be used to significantly increase the sensitivity to discover new particles and new forces, and gain a deeper understanding of the physics within jets.},
	isbn={1029-8479},
	url={https://link.springer.com/article/10.1007/JHEP07(2016)069},
	doi={10.1007/JHEP07(2016)069}
}
@article{RefWorks:RefID:5-cogan2015jet-images:,
	author={Josh Cogan and Michael Kagan and Emanuel Strauss and Ariel Schwarztman},
	year={2015},
	month={Feb 18,},
	title={Jet-images: computer vision inspired techniques for jet tagging},
	journal={The journal of high energy physics},
	volume={2015},
	number={2},
	pages={1-16},
	abstract={A
bstract
We introduce a novel approach to jet tagging and classification through the use of techniques inspired by computer vision. Drawing parallels to the problem of facial recognition in images, we define a
jet-image
using calorimeter towers as the elements of the image and establish jet-image preprocessing methods. For the jet-image processing step, we develop a discriminant for classifying the jet-images derived using Fisher discriminant analysis. The effectiveness of the technique is shown within the context of identifying boosted hadronic
W
boson decays with respect to a background of quark- and gluoninitiated jets. Using Monte Carlo simulation, we demonstrate that the performance of this technique introduces additional discriminating power over other substructure approaches, and gives significant insight into the internal structure of jets.},
	isbn={1029-8479},
	url={https://link.springer.com/article/10.1007/JHEP02(2015)118},
	doi={10.1007/JHEP02(2015)118}
}
@misc{RefWorks:RefID:4-cernjets,
	author={CERN},
	title={Jets at CMS and the determination of their energy scale | CMS Experiment},
	volume={2022},
	number={Jan 24,}
}
@techreport{RefWorks:RefID:3-yuan2021constituentnet:,
	author={Xinyang Yuan},
	year={2021},
	month={-09-22},
	title={ConstituentNet: Learn to Solve Jet Tagging through Attention}
}
@article{RefWorks:RefID:2-capeans2017strategies,
	author={M. Capeans and R. Guida and B. Mandelli},
	year={2017},
	title={Strategies for reducing the environmental impact of gaseous detector operation at the CERN LHC experiments},
	journal={Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
	volume={845},
	pages={253-256},
	note={ID: 271580},
	abstract={A wide range of gas mixtures is used for the operation of different gaseous detectors at the Large Hadron Collider (LHC) experiments. Nowadays some of these gases, as C2H2F4, CF4 and SF6, are indicated as greenhouse gases (GHG) and dominate the overall GHG emission from particle detectors at the LHC experiments. The release of GHG is an important subject for the design of future particle detectors as well as for the operation of the current experiments. Different strategies have been adopted at CERN for reducing the GHG emissions. The standard approach is the recirculation of the gas mixture with complex gas systems where system stability and the possible accumulation of impurities need to be attentively evaluated for the good operation and safety of the detectors. A second approach is based on the recuperation of the gas mixture exiting the detectors and the separation of its gas components for re-use. At long-term, the use of less invasive gases is being investigated, especially for the Resistive Plate Chamber (RPC) systems. Operation of RPC with environmentally friendly gas mixtures is demonstrated for streamer mode while avalanche mode operation needs more complex gas mixtures.},
	isbn={0168-9002},
	url={https://www.sciencedirect.com/science/article/pii/S0168900216302807},
	doi={https://doi.org/10.1016/j.nima.2016.04.067}
}
@misc{RefWorks:RefID:1-cernfacts,
	author={CERN},
	title={Facts and figures about the LHC | CERN},
	volume={2022},
	number={Jan 23,},
	url={https://home.cern/resources/faqs/facts-and-figures-about-lhc}
}
