\chapter{Introduction}

\section{Overview}
Particle physics is one of the key branches of modern physics, with the Standard Model theory at its core. It tackles the
underlying questions about the nature of the universe by describing the fundamental forces and elementary particles. In order
to verify the correctness of the theories, countless experiments have to be designed and carefully executed, with
the main driving force of myriads of engineers, physicists and researchers at Large Hadron Collider (LHC) operated by
the European Organization for Nuclear Research (CERN).

LHC is the world's highest-energy particle collider that is capable of producing and detecting the heaviest types of particles
that emerge from collisions such as a proton-proton collisions. The detection is a challenging process as some particles like
quarks and gluons cannot exist on their own and they nearly instantly combine which results in a collimated spray of composite
particles (hadrons) that is typically referred to as a \textbf{jet}\cite{RefWorks:RefID:4-cernjets}. The initial particles created upon collision
and their behaviors are of main interest of the physicists, which leads to \textbf{jet tagging} - the challenge of associating particle jets
with their origin.


\section{Motivation}
There are many detector types used for the analysis the particle collisions, each based on a different physical methodology, which
result in availability of both higher and lower level features. The former have been successfully used in the past using more physically
motivated algorithms, e.g. using computer vision\cite{RefWorks:RefID:5-cogan2015jet-images:}. However, more recently, various deep learning
approaches have proven to outperform their predecessors\cite{RefWorks:RefID:6-de2016jet-images}. It has also been found that all the
detected features carry the same underlying information, with convolutional neural networks (CNN) trained on higher-level data achieving
nearly identical accuracy as dense neural networks (DNN) trained on the data from the other end of the spectrum\cite{RefWorks:RefID:7-moore2019reports}.

The throughput of information collected by the LHC detectors outclasses the inference capabilities of the state-of-the-art solutions
deployed using the typical software-centered approach\cite{?}.

Physics experiments are crucial
Big data is crucial for Physics
LHC is currently bottleneck at real time speed for detectors
Transformer Neural Networks are great

Powerful hardware is king
FPGA are great for NN
Metaprogramming allows for optimizations and customisability

\section{Objectives and Challenges}
Current architecture at LHC is too slow -> make it quick enough for real time processing
HLS is difficult, so coding hardware in Python is desired -> make it easy for engineers and physicists to design systems
FPGA are very hard-coded -> make the code deployable on any platform with optimal settings automatically

\section{Contributions}
hls4ml
