\chapter{Introduction}\label{introduction}

\section{Motivation}\label{motivation}
LHC is the world's highest-energy particle collider that is capable of producing and detecting the heaviest types of particles that emerge from events such as proton-proton collisions. The detection is a challenging process as some particles, like quarks and gluons, cannot exist on their own, and they nearly instantly combine which results in collimated sprays of composite particles (hadrons) that are referred to as jets \cite{4-cernjets}. The initial particles created upon collision and their behaviors are of main interest of the physicists, which leads to jet tagging - the challenge of associating particle jets with their origin.

There are many detector types used for the analysis of the particle collisions, each based on a different physical phenomenon, which results in availability of both higher and lower level features from each event. The former have been successfully used in the past using more physically motivated machine learning (ML) algorithms, e.g. using computer vision \cite{5-cogan2015jet-images:}. However, more recently, various deep learning approaches have proven to outperform their predecessors \cite{6-de2016jet-images}. It has also been found that all the detected features carry the same underlying information, with convolutional neural networks trained on higher-level data achieving nearly identical accuracy as dense neural networks trained on the data from the other end of the spectrum \cite{7-moore2019reports}.

The information throughput of Petabytes per second collected by the LHC detectors outclasses the real-time inference capabilities of the typical state-of-the-art solutions. The real-time decision-making is often of utmost interest, hence this paper is motivated by this challenge which includes exploring various types of neural network architecture as well as the necessary infrastructure and deployment processes. Recently, \textit{\textbf{hls4ml}} codesign workflow have been successfully adopted in particle physics experiments \cite{8-fahim2021hls4ml:}, which allows ML researchers and physicists to easily deploy their solutions trained using common ML frameworks on reconfigurable or application specific hardware, vastly improving the detection algorithms' inference capabilities. However, \textit{hls4ml} lacks support for a number of neural network architectures that have been proven to outperform the previous state-of-the-art, including graph neural networks \cite{9-newman2019jedi-net:, 11-elabd2021graph} and transformer neural networks \cite{3-yuan2021constituentnet:}.


\section{Objectives and Challenges}
The purpose of this project is to develop novel, hardware-aware neural network architectures as well as to establish efficient ways mapping them onto FPGAs. Another objective is to use metaprogramming strategies to integrate them into the \textit{hls4ml} library or standalone tools, with various optimizations approaches that offer trade-offs between latency, throughput and hardware resources usage. Hence, there is an emphasis on creating parametrizable and reusable designs that can support creation of ultra-low latency systems, effectively transforming proof-of-concept implementations into optimized hardware accelerators.

The two main challenges of the project involve:
\begin{itemize}
  \item Developing deep, complex and fast neural network models which require much lower abstraction levels than a typical ML framework. It is also crucial to stay aware of the underlying hardware architecture to exploit its strengths while keeping compile-time and run-time configuration easily accessible.
  \item Bridging the abstraction gap for the translation between high-level representation of neural networks and their optimized mapping to hardware. The design space exploration is a long and difficult process, which needs careful examination and analytical performance models to find the optimal solutions.
\end{itemize}


\section{Contributions and Publication}
The project aims to benefit the open-source community of ML researches that are in need of faster and more parametrizable neural network inference. The main audience for that operation are particle physicists, nonetheless, the hope is for the work to positively contribute many ML fields by both offering a reliable tool for acceleration of existing designs and providing a useful resource for learning about the nature of reconfigurable hardware and its optimization potential.

The bulk of the work and analysis conducted in this project was summarized in the paper "Accelerating Transformer Neural Networks on FPGAs for High Energy Physics Experiments" and submitted in the long paper category to the \textit{18\textsuperscript{th} International Symposium on Applied Reconfigurable Computing}. A journal article derived from this project is being prepared for publication.

\section{Report Outline}
This report begins by discussing the necessary particle physics background to understand the scope of the work, followed by the related work in the field of machine learning, with an emphasis on the state-of-the-art architectures, and a deeper dive into the reconfigurable hardware technology in \cref{background}. Then in \cref{models}, the two proposed novel neural network architectures are described in details, including the necessary training and processing steps. After that, \cref{quantization} covers both the existing and custom ways of conducting design space exploration and how they were applies in this project. \Cref{evaluation} discusses the evaluation metrics and collected experimental results, which is concluded by \cref{conclusion}, which also proposes future work derived from this analysis.
