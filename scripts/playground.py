import numpy as np
import csv
import torch
import torch.nn as nn

torch.set_printoptions(profile='full', sci_mode=False, threshold=2097152)

# x_data = '29.938385009765625 178.55862426757812 -67.51207733154297 193.2311248779297 0.17523084580898285 181.0510711669922 0.1761978417634964 -0.3647479712963104 0.008448980748653412 -0.0 1.4046745300292969 0.000455104949651286 0.0 0.008461219258606434 -0.34938931465148926 6.123234262925839e-17'
# x = np.array([float(el) for el in x_data.split(' ')])

# def read_data():
#   t = []
#   with open('extracted_weights_biases/inp_layer_weight.txt', 'r') as infile:
#     for el in infile:
#       t = np.array([float(el) for el in el.split(',')])
#   return t

def transform_like_HLS(t):
  return t.reshape(16, 128).flatten(order='F')

def revert_HLS_read(t, dim0=16, dim1=128):
  return t.reshape(dim0, dim1, order='F').flatten(order='C')

def output_format_to_numpy(x, cols_in_line=9, end_cols_in_line=2, total_elements=256, e_notation=False):
  x = x.replace('\n', ' ').replace('  ', ' ').split(' ')
  if e_notation:
    x = [float(el) for el in x if 'e' in el]
  else:
    x = [float(el) for el in x if len(el) == 6 or (len(el) == 7 and el[0] == '-')]
  x = np.array(x)

  base_elements = total_elements - end_cols_in_line*2
  x1 = np.array([el for i, el in enumerate(x) if ((i < base_elements and i % (cols_in_line*2) < cols_in_line) or (i >= base_elements and i % (end_cols_in_line*2) < end_cols_in_line))])
  x2 = np.array([el for i, el in enumerate(x) if ((i < base_elements and i % (cols_in_line*2) >= cols_in_line) or (i >= base_elements and i % (end_cols_in_line*2) >= end_cols_in_line))])
  result = torch.unsqueeze(torch.cat((torch.unsqueeze(torch.from_numpy(x1), dim=0), torch.unsqueeze(torch.from_numpy(x2), dim=0)), dim=0), dim=0)
  return result

def case_0():
  two_cols_data = '''Columns 1 to 6  4.0129e-02  8.1093e-01 -6.7605e-02  4.5763e-02 -9.1097e-01 -7.3065e-01
 -6.1937e+00  1.5361e+01 -7.8609e+01  3.5738e+01 -3.8578e+01  6.5512e-01

Columns 7 to 12  1.9955e+00  1.7008e+00  1.1903e+00  6.8385e-01 -7.6752e-01 -1.4813e+00
  5.2728e+01 -3.1096e+01  3.4690e+01 -2.2029e+00  6.3353e+01  3.5349e+01

Columns 13 to 18  3.4362e-01  2.3606e+00 -3.4037e+00  1.5055e+00 -1.8098e-02  4.5189e-02
 -8.1889e+01 -7.3146e+01  6.8400e+01 -4.5082e+01 -8.7851e+00  3.4903e+01

Columns 19 to 24 -1.3255e+00  1.0194e+00  5.2586e-01  7.5091e-01  1.4665e-01  1.4115e-01
  1.0487e+02 -4.3092e+01 -1.9874e+01 -5.7966e+01 -6.3025e+00  5.6187e+01

Columns 25 to 30 -9.4811e-01 -6.8913e-01 -1.5163e-01  1.0495e+00 -1.2274e+00 -4.3410e-01
 -1.8383e+00 -6.8293e+01 -6.3265e+01  5.1773e+01 -2.2836e+01 -8.4276e+01

Columns 31 to 36 -2.9523e-01 -1.1433e+00  3.9863e-01  3.7319e-02  1.0493e+00 -3.2728e-01
 -7.4508e+01  2.0039e+01  8.1425e-01 -5.9820e+01  2.5564e+00 -1.1088e+02

Columns 37 to 42 -9.6229e-01 -7.8601e-01  5.6638e-01  2.0820e+00  9.5599e-01 -4.6012e-01
 -3.2059e+01 -3.1237e+01 -1.2671e+02 -1.3587e+01  4.3975e+01 -8.1219e+00

Columns 43 to 48 -1.1359e+00  7.5818e-01 -6.8697e-01  8.8815e-02  9.6351e-01  6.5712e-01
  2.0680e+01 -5.2564e+01  2.7641e+01  2.8673e+01  3.8287e+01  3.7847e+01

Columns 49 to 54 -1.9308e+00 -4.7343e-02 -7.8666e-01 -3.1326e-01 -1.8380e+00 -4.1530e-01
 -2.0089e+01  1.9987e+01  1.5085e+01  2.5509e+01 -2.3733e+00  4.0189e+01

Columns 55 to 60  1.1444e+00 -4.6327e-01 -8.1061e-01 -3.2287e-01  1.6043e-01  1.6304e-01
  5.1659e+01 -4.6149e+01 -8.6524e+01 -1.9592e+01 -1.1651e+02 -2.5306e+01

Columns 61 to 66  2.2874e+00  2.0731e-03  6.9055e-01  1.5403e+00  3.8258e-01 -1.3344e+00
 -5.1343e+01  5.7282e+01  6.2609e+01 -5.0239e+01  6.0019e+01 -6.7135e+01

Columns 67 to 72  2.3314e-02 -4.2542e-01 -8.5733e-01 -6.2700e-01 -9.6613e-01 -3.8246e-01
  4.6309e+01 -3.5972e+01 -3.8091e+01  9.2045e+01 -4.1175e+01 -2.3876e+00

Columns 73 to 78 -3.2845e-01  1.2401e+00 -8.3011e-01  1.0515e+00  1.8618e-01 -2.3656e-02
  7.3446e+01  1.7168e+01  4.3220e+01 -1.9498e+01  3.8385e+00  1.2803e+02

Columns 79 to 84 -1.2989e+00  6.7152e-01 -5.1173e-01 -1.0610e+00  3.9402e-01 -5.9869e-01
  8.8824e-01 -8.5643e+01 -2.1312e+01  7.6767e+00  2.0002e+01  1.8667e+00

Columns 85 to 90 -8.2251e-01  2.9724e-01  1.2275e+00  1.0696e+00  1.1230e-01  2.0362e+00
 -8.1969e+01 -1.0878e+01 -1.1542e+01  4.4753e+01  1.9577e+01  6.3550e+01

Columns 91 to 96  2.6253e-01 -3.6470e-01 -7.0298e-01 -1.8693e+00  7.7021e-01 -1.7509e+00
  3.0295e+01  7.9039e+00  6.8047e+01  4.0320e+01  6.8742e+01 -1.1869e+02

Columns 97 to 102 -7.5656e-01 -1.4887e+00 -7.0923e-01  1.5125e+00  1.2890e+00  1.0281e+00
  6.9538e+01  1.1321e+01  1.8625e+00  4.6260e+01  5.0166e+01 -8.2065e+01

Columns 103 to 108 -1.4097e+00  4.1892e-01  1.1135e+00 -1.2268e+00  3.7976e-01  9.0445e-01
  3.1439e+01 -3.1508e+01 -1.1561e+02 -1.5516e+01  4.3495e+01  8.7215e+01

Columns 109 to 114 -1.0730e+00 -2.1239e-01  4.4776e-01  8.0886e-02  2.7727e-01  1.4085e-01
 -1.7620e+01 -1.8318e+00 -2.0270e+01 -3.4970e+01  5.5462e+01  6.5110e+01

Columns 115 to 120  9.0327e-01 -3.3493e-01  1.7014e+00  1.8344e-01  6.8673e-03  1.0173e+00
  8.8996e+01  2.1583e+01 -9.4023e+00  5.4407e+01  2.2995e+01  6.3767e+01

Columns 121 to 126  6.3203e-01 -4.9540e-01 -1.5465e-01  4.6508e-01  3.3193e-01 -8.8931e-01
 -2.9605e+01 -2.3669e+01  4.6626e+01 -3.1904e+01  5.3989e+01 -4.0746e+01

Columns 127 to 128 -8.1273e-01  5.3334e-01
 -5.9265e+01  2.7296e+01'''

  col_data = output_format_to_numpy(two_cols_data, cols_in_line=6, end_cols_in_line=2, e_notation=True)

  print(f'{col_data=}')

  col_mean = torch.mean(col_data, dim=2)
  print("col_mean:")
  print(col_mean)

  col_var = torch.var(col_data, dim=2, unbiased=False)
  print("col_var:")
  print(col_var)

  # col_var_sum = None
  # col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
  # col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
  # col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
  # print("col_var_sum")
  # print(col_var_sum)

  # col_var_avg = torch.div(col_var_sum, 3)
  # print("col_var_avg")
  # print(col_var_avg)

  eps = 1e-5
  W0 = 0.99982661008 # 0.99982661008
  b0 = 0.00038191396 # 0.00038191396

  def layer_norm(x, mean, var, eps, weight, bias):
    numerator = x - mean
    denominator = np.sqrt(var + eps)
    return numerator * weight / denominator + bias

  col1_data_normalized = np.array([layer_norm(x=x, mean=col_mean[0][0], var=col_var[0][0], eps=eps, weight=W0, bias=b0) for x in col1_data])
  print(col1_data_normalized)
  print(col1_data_normalized.shape)

  col2_data_normalized = np.array([layer_norm(x=x, mean=col_mean[0][1], var=col_var[0][1], eps=eps, weight=W0, bias=b0) for x in col2_data])
  print(col2_data_normalized)
  print(col2_data_normalized.shape)

  results = ''' Columns 1 to 9  0.0269  0.8112 -0.0830  0.0319 -0.9405 -0.7571  2.0154  1.7169  1.1973
  -0.1204  0.2913 -1.5156  0.6827 -0.7446  0.0081  1.0114 -0.6022  0.6674

  Columns 10 to 18  0.6811 -0.7951 -1.5215  0.3356  2.3869 -3.4773  1.5171 -0.0327  0.0313
  -0.0456  1.2158  0.6755 -1.5757 -1.4110  1.3134 -0.8719 -0.1692  0.6682

  Columns 19 to 27 -1.3624  1.0228  0.5204  0.7497  0.1355  0.1293 -0.9782 -0.7146 -0.1684
    2.0125 -0.8306 -0.3866 -1.1160 -0.1219  1.0768 -0.0392 -1.3145 -1.2181

  Columns 28 to 36  1.0538 -1.2626 -0.4555 -0.3145 -1.1772  0.3913  0.0238  1.0535 -0.3474
    0.9965 -0.4400 -1.6229 -1.4356  0.3822  0.0124 -1.1517  0.0475 -2.1390

  Columns 37 to 45 -0.9927 -0.8135  0.5622  2.1029  0.9584 -0.4817 -1.1695  0.7574 -0.7139
  -0.6206 -0.6035 -2.4428 -0.2628  0.8427 -0.1568  0.3955 -1.0165  0.5264

  Columns 46 to 54  0.0763  0.9659  0.6542 -1.9788 -0.0624 -0.8145 -0.3329 -1.8825 -0.4371
    0.5488  0.7359  0.7247 -0.3920  0.3825  0.2859  0.4862 -0.0456  0.7695

  Columns 55 to 63  1.1498 -0.4855 -0.8378 -0.3423  0.1488  0.1513  2.3109 -0.0120  0.6882
    0.9894 -0.8917 -1.6650 -0.3808 -2.2436 -0.4913 -0.9882  1.1002  1.2014

  Columns 64 to 72  1.5514  0.3752 -1.3709  0.0099 -0.4466 -0.8856 -0.6522 -0.9969 -0.4033
  -0.9672  1.1544 -1.2918  0.8899 -0.6923 -0.7348  1.7687 -0.7946 -0.0490

  Columns 73 to 81 -0.3482  1.2473 -0.8587  1.0551  0.1753 -0.0386 -1.3348  0.6690 -0.5346
    1.4133  0.3290  0.8271 -0.3804  0.0729  2.4583  0.0147 -1.6503 -0.4108

  Columns 82 to 90 -1.0934  0.3866 -0.6237 -0.8511  0.2884  1.2341  1.0730  0.1002  2.0573
    0.1461  0.3839  0.0311 -1.5783 -0.2133 -0.2225  0.8575  0.3761  1.2221

  Columns 91 to 99  0.2528 -0.3850 -0.7296 -1.9156  0.7693 -1.7941 -0.7833 -1.5295 -0.7355
    0.5777  0.1513  1.3076  0.7750  1.3183 -2.2829  1.3326  0.2136  0.0310

  Columns 100 to 108  1.5252  1.2976  1.0307 -1.4479  0.4124  1.1194 -1.2616  0.3726  0.9053
    0.8897  0.9644 -1.5828  0.6027 -0.6073 -2.2261 -0.2994  0.8365  1.6730

  Columns 109 to 117 -1.1063 -0.2299  0.4412  0.0682  0.2676  0.1295  0.9051 -0.3545  1.7165
  -0.3402 -0.0359 -0.3911 -0.6765  1.0624  1.2499  1.7122  0.4116 -0.1820

  Columns 118 to 126  0.1731 -0.0076  1.0214  0.6284 -0.5181 -0.1720  0.4591  0.3228 -0.9185
    1.0429  0.4402  1.2273 -0.5705 -0.4559  0.8919 -0.6148  1.0335 -0.7867

  Columns 127 to 128 -0.8407  0.5285
  -1.1420  0.5243'''

  results = results.replace('\n', ' ').replace('  ', ' ').split(' ')
  results = [float(el) for el in results if len(el) == 6 or (len(el) == 7 and el[0] == '-')]
  results = np.array(results)

  col1_res = np.array([el for i, el in enumerate(results) if ((i < 252 and i % 18 < 9) or (i >= 252 and i % 4 < 2))])
  col2_res = np.array([el for i, el in enumerate(results) if ((i < 252 and i % 18 >= 9) or (i >= 252 and i % 4 >= 2))])
  print("results:\n")
  print(col1_res)
  print(col2_res)
  print(col1_res.shape)
  print(col2_res.shape)

  print("diff:\n")
  col1_diff = np.absolute(col1_data_normalized - col1_res)
  print(col1_diff)
  col2_diff = np.absolute(col2_data_normalized - col2_res)
  print(col2_diff)
  print(f"Biggest error in col1 is {np.amax(col1_diff)}")
  print(f"Biggest error in col2 is {np.amax(col2_diff)}")


  A = torch.tensor([1, 2, 3])
  print(A[0])
  print(A[1:])

  X0 = 3.9991e-02
  X1 = -6.1523

  b0 = -0.01386488933016245
  b1 = -0.0007655795714506348

  W0 = 1.0169684936528849
  W1 = 0.08921465251486772

  print(f"{(X0*W0+b0)=}")
  print(f"{(X1*W1+b1)=}")

class MyLinear(nn.Module):
  def __init__(self, in_dim):
    super(MyLinear, self).__init__()
    self.linear = nn.Linear(in_dim, 3*in_dim, bias=False)

  def forward(self, x):
    return self.linear(x.double())

def case_1():
  input_norm_output = ''' Columns 1 to 9  0.0269  0.8110 -0.0829  0.0321 -0.9407 -0.7572  2.0156  1.7165  1.1969
 -0.1213  0.2932 -1.5148  0.6848 -0.7445  0.0103  1.0117 -0.6006  0.6650

Columns 10 to 18  0.6813 -0.7950 -1.5212  0.3354  2.3870 -3.4773  1.5172 -0.0328  0.0315
 -0.0448  1.2155  0.6774 -1.5769 -1.4096  1.3134 -0.8701 -0.1716  0.6687

Columns 19 to 27 -1.3625  1.0226  0.5205  0.7497  0.1352  0.1294 -0.9786 -0.7149 -0.1685
  2.0147 -0.8308 -0.3849 -1.1173 -0.1234  1.0780 -0.0376 -1.3157 -1.2194

Columns 28 to 36  1.0534 -1.2627 -0.4558 -0.3144 -1.1771  0.3914  0.0238  1.0532 -0.3472
  0.9934 -0.4416 -1.6235 -1.4358  0.3832  0.0134 -1.1529  0.0468 -2.1359

Columns 37 to 45 -0.9927 -0.8136  0.5622  2.1031  0.9582 -0.4819 -1.1692  0.7571 -0.7135
 -0.6189 -0.6033 -2.4403 -0.2636  0.8434 -0.1584  0.3952 -1.0139  0.5290

Columns 46 to 54  0.0761  0.9658  0.6541 -1.9788 -0.0626 -0.8144 -0.3328 -1.8830 -0.4367
  0.5489  0.7338  0.7254 -0.3893  0.3817  0.2874  0.4883 -0.0479  0.7705

Columns 55 to 63  1.1497 -0.4855 -0.8382 -0.3426  0.1489  0.1514  2.3116 -0.0119  0.6882
  0.9911 -0.8904 -1.6660 -0.3793 -2.2435 -0.4897 -0.9898  1.0996  1.2017

Columns 64 to 72  1.5517  0.3750 -1.3711  0.0098 -0.4465 -0.8861 -0.6520 -0.9968 -0.4033
 -0.9688  1.1522 -1.2934  0.8882 -0.6939 -0.7350  1.7685 -0.7944 -0.0486

Columns 73 to 81 -0.3482  1.2472 -0.8586  1.0552  0.1753 -0.0385 -1.3348  0.6688 -0.5345
  1.4106  0.3275  0.8286 -0.3779  0.0715  2.4600  0.0148 -1.6493 -0.4123

Columns 82 to 90 -1.0934  0.3866 -0.6236 -0.8512  0.2883  1.2344  1.0735  0.1001  2.0571
  0.1448  0.3823  0.0332 -1.5796 -0.2114 -0.2247  0.8581  0.3741  1.2200

Columns 91 to 99  0.2527 -0.3850 -0.7297 -1.9156  0.7694 -1.7946 -0.7835 -1.5291 -0.7354
  0.5801  0.1498  1.3064  0.7733  1.3199 -2.2847  1.3346  0.2151  0.0333

Columns 100 to 108  1.5247  1.2973  1.0311 -1.4480  0.4122  1.1190 -1.2616  0.3723  0.9053
  0.8875  0.9628 -1.5806  0.6019 -0.6084 -2.2267 -0.3007  0.8344  1.6745

Columns 109 to 117 -1.1061 -0.2299  0.4412  0.0682  0.2675  0.1293  0.9048 -0.3546  1.7164
 -0.3416 -0.0375 -0.3924 -0.6750  1.0639  1.2497  1.7097  0.4127 -0.1836

Columns 118 to 126  0.1727 -0.0074  1.0210  0.6285 -0.5183 -0.1717  0.4591  0.3230 -0.9187
  1.0445  0.4395  1.2245 -0.5724 -0.4579  0.8942 -0.6161  1.0353 -0.7862

Columns 127 to 128 -0.8409  0.5283
 -1.1424  0.5225'''

  input_norm_output = output_format_to_numpy(input_norm_output, cols_in_line=9, end_cols_in_line=2)

  print(f"{input_norm_output.shape=}")
  print(input_norm_output)

  hls_input = '0.0269086 -0.118814 0.811038 0.294311 -0.0831297 -1.51152 0.0323419 0.68669 -0.941092 -0.741211 -0.757548 0.0115624 2.0151 1.01299 1.71621 -0.598496 1.19619 0.667438 0.681356 -0.0429487 -0.795151 1.21724 -1.5202 0.67852 0.335759 -1.57406 2.38676 -1.40752 -3.477 1.31437 1.51783 -0.867604 -0.0323599 -0.168306 0.031863 0.67087 -1.36191 2.0143 1.02314 -0.828293 0.520689 -0.382319 0.749343 -1.11421 0.134622 -0.120575 0.129044 1.07912 -0.979151 -0.0362937 -0.715236 -1.31286 -0.168044 -1.21587 1.05322 0.996247 -1.26183 -0.438499 -0.45586 -1.62081 -0.31454 -1.43233 -1.17661 0.384195 0.391431 0.0150427 0.0240982 -1.1491 1.05301 0.0496797 -0.347022 -2.13216 -0.993042 -0.617535 -0.813817 -0.601399 0.561986 -2.43682 2.10302 -0.260688 0.958246 0.844396 -0.482265 -0.156033 -1.17003 0.397064 0.756928 -1.0113 -0.712592 0.530414 0.076093 0.551064 0.965819 0.736508 0.654377 0.726731 -1.97731 -0.387319 -0.0623615 0.384214 -0.814551 0.28912 -0.333022 0.489469 -1.88423 -0.0451786 -0.436484 0.77173 1.15011 0.991698 -0.485388 -0.887677 -0.838797 -1.66231 -0.342433 -0.377171 0.148756 -2.24024 0.151342 -0.487263 2.3128 -0.986615 -0.012348 1.10122 0.6876 1.20261 1.55237 -0.965043 0.033364 -0.192759 -0.120115 0.199762 0.000779615 -0.158341 -0.039034 0.122834 -0.0775555 0.131805 -0.0573609 -0.294633 -0.0870022 0.13104 -0.035301 0.0214841 -0.0305592 -0.227438 0.109561 -0.0474663 -0.075392 -0.13453 0.0930227 0.0519626 0.0158479 -0.0187631 -0.0032622 -0.395971 -0.116724 -0.0166748 0.0590363 0.260917 -0.0465029 0.0711151 -0.0956452 -0.0109163 0.0342386 -0.0614958 -0.0543914 -0.0150816 -0.0744552 0.244669 0.0254268 0.0457671 0.108041 0.0431511 0.094012 -0.134045 0.00849369 -0.0548889 0.180314 -0.185107 0.0226559 -0.0918293 -0.0338965 -0.00779573 -0.0634669 -0.209088 -0.167985 -0.129041 0.0674409 -0.195141 -0.157114 0.387549 -0.0685798 -0.217299 -0.133527 -0.021343 -0.0642605 -0.00911182 0.13389 -0.131588 0.114145 -0.156813 0.0904209 0.261517 -0.126631 -0.0986003 0.03649 0.0934614 0.0983884 0.362082 -0.110539 0.0321699 0.0325461 -0.129954 0.0793815 -0.278186 -0.0966719 0.0520174 -0.0201377 -0.00506127 0.0385967 0.0686211 0.00625272 0.104839 0.0236516 -0.162944 0.0114029 -0.201089 0.0796175 -0.293633 -0.0312517 -0.0688784 0.151052 0.0126527 0.0155205 -0.160365 -0.000574701 -0.0641685 0.0896675 -0.190804 0.0550329 0.0970125 -0.0451128 0.0606544 -0.0151648 -0.125356 0.0401825 0.101634 0.0281035 -0.173165 -0.0806829 0.12045 -0.073967 0.173474 0.0464663 -0.0679771'
  hls_input = torch.unsqueeze(torch.from_numpy(np.array([float(el) for el in hls_input.split(' ')]).reshape(2, 128, order='F')), dim=0)

  print(f'{hls_input.shape=}')
  print(f'{hls_input=}')

  # input_norm_raw = '''0.0269086 -0.118814 0.811038 0.294311 -0.0831297 -1.51152 0.0323419 0.68669 -0.941092 -0.741211 -0.757548 0.0115624 2.0151 1.01299 1.71621 -0.598496 1.19619 0.667438 0.681356 -0.0429487 -0.795151 1.21724 -1.5202 0.67852 0.335759 -1.57406 2.38676 -1.40752 -3.477 1.31437 1.51783 -0.867604 -0.0323599 -0.168306 0.031863 0.67087 -1.36191 2.0143 1.02314 -0.828293 0.520689 -0.382319 0.749343 -1.11421 0.134622 -0.120575 0.129044 1.07912 -0.979151 -0.0362937 -0.715236 -1.31286 -0.168044 -1.21587 1.05322 0.996247 -1.26183 -0.438499 -0.45586 -1.62081 -0.31454 -1.43233 -1.17661 0.384195 0.391431 0.0150427 0.0240982 -1.1491 1.05301 0.0496797 -0.347022 -2.13216 -0.993042 -0.617535 -0.813817 -0.601399 0.561986 -2.43682 2.10302 -0.260688 0.958246 0.844396 -0.482265 -0.156033 -1.17003 0.397064 0.756928 -1.0113 -0.712592 0.530414 0.076093 0.551064 0.965819 0.736508 0.654377 0.726731 -1.97731 -0.387319 -0.0623615 0.384214 -0.814551 0.28912 -0.333022 0.489469 -1.88423 -0.0451786 -0.436484 0.77173 1.15011 0.991698 -0.485388 -0.887677 -0.838797 -1.66231 -0.342433 -0.377171 0.148756 -2.24024 0.151342 -0.487263 2.3128 -0.986615 -0.012348 1.10122 0.6876 1.20261 1.55237 -0.965043 0.033364 -0.192759 -0.120115 0.199762 0.000779615 -0.158341 -0.039034 0.122834 -0.0775555 0.131805 -0.0573609 -0.294633 -0.0870022 0.13104 -0.035301 0.0214841 -0.0305592 -0.227438 0.109561 -0.0474663 -0.075392 -0.13453 0.0930227 0.0519626 0.0158479 -0.0187631 -0.0032622 -0.395971 -0.116724 -0.0166748 0.0590363 0.260917 -0.0465029 0.0711151 -0.0956452 -0.0109163 0.0342386 -0.0614958 -0.0543914 -0.0150816 -0.0744552 0.244669 0.0254268 0.0457671 0.108041 0.0431511 0.094012 -0.134045 0.00849369 -0.0548889 0.180314 -0.185107 0.0226559 -0.0918293 -0.0338965 -0.00779573 -0.0634669 -0.209088 -0.167985 -0.129041 0.0674409 -0.195141 -0.157114 0.387549 -0.0685798 -0.217299 -0.133527 -0.021343 -0.0642605 -0.00911182 0.13389 -0.131588 0.114145 -0.156813 0.0904209 0.261517 -0.126631 -0.0986003 0.03649 0.0934614 0.0983884 0.362082 -0.110539 0.0321699 0.0325461 -0.129954 0.0793815 -0.278186 -0.0966719 0.0520174 -0.0201377 -0.00506127 0.0385967 0.0686211 0.00625272 0.104839 0.0236516 -0.162944 0.0114029 -0.201089 0.0796175 -0.293633 -0.0312517 -0.0688784 0.151052 0.0126527 0.0155205 -0.160365 -0.000574701 -0.0641685 0.0896675 -0.190804 0.0550329 0.0970125 -0.0451128 0.0606544 -0.0151648 -0.125356 0.0401825 0.101634 0.0281035 -0.173165 -0.0806829 0.12045 -0.073967 0.173474 0.0464663 -0.0679771'''

  # input_norm = np.array([float(el) for el in input_norm_raw.split(' ')]).reshape(2, 128, order='F')
  # # print(input_norm)
  # print(f"{input_norm.shape=}")
  # print(input_norm)

  weights_raw = np.genfromtxt('extracted_weights_biases/transformers_0_self_attention_qkv_weight.txt', delimiter=',')
  print(f"{weights_raw=}")
  weights = weights_raw.reshape(128, 384, order='C')
  weights = weights.flatten(order='F').reshape(384, 128, order='C')
  # weights *= 100

  print(f"{weights.shape=}")
  print(f"{weights=}")

#   result = np.matmul(input_norm, weights.T)
#   print(f"{result.shape=}")
#   print(f"{result[0][0]=}")
#   print(f"{result[1][0]=}")
#   print(f"{result[0][1]=}")
#   # print(f"{result[0]=}")
#   # print(f"{result[1]=}")

#   print("-"*10 + '\n'*3)

#   my_linear = MyLinear(in_dim=128)
#   my_linear = my_linear.double()

#   with torch.no_grad():
#     my_linear.linear.weight = nn.Parameter(torch.from_numpy(weights).double())
#     print(my_linear.linear.weight.size())
#     print(my_linear.linear.weight[0][0])
#     print(my_linear.linear.weight[0][1])
#     print(my_linear.linear.weight[1][0])
#     torch_input = torch.from_numpy(input_norm).double()
#     torch_result = my_linear(torch_input)
#     print(f"{torch_result.size()=}")
#     print(f"{torch_result[0][0]=}")
#     print(f"{torch_result[0][1]=}")
#     print(f"{torch_result[1][0]=}")

# # ------------------------------
  # print("-"*10 + '\n'*3)

  print("-"*10 + '\n')

  print('from Pytorch')
  result = np.matmul(input_norm_output, weights.T)
  print(f"{result.shape=}")
  print(f"{result[0][0][0]=}")
  print(f"{result[0][0][1]=}")
  print(f"{result[0][1][0]=}")
  # print(f"{result[0]=}")
  # print(f"{result[1]=}")

  print("-"*10 + '\n')

  print('from HLS')
  result = np.matmul(hls_input, weights.T)
  print(f"{result.shape=}")
  print(f"{result[0][0][0]=}")
  print(f"{result[0][0][1]=}")
  print(f"{result[0][1][0]=}")
  # print(f"{result[0]=}")
  # print(f"{result[1]=}")

  print("-"*10 + '\n')

  my_linear = MyLinear(in_dim=128)
  my_linear = my_linear.double()

  with torch.no_grad():
    my_linear.linear.weight = nn.Parameter(torch.from_numpy(weights).double())
    print(my_linear.linear.weight.size())
    print(my_linear.linear.weight[0][0])
    print(my_linear.linear.weight[0][1])
    print(my_linear.linear.weight[1][0])
    torch_input = input_norm_output.double()
    torch_result = my_linear(torch_input)
    print(f"{torch_result.size()=}")
    print(f"{torch_result[0][0][0]=}")
    print(f"{torch_result[0][0][1]=}")
    print(f"{torch_result[0][1][0]=}")

  
# ----------transformers.0.self_attention.qkv.weight----------
# (384, 128)
# [[-0.00898834  0.00902199 -0.01533934 ... -0.005541   -0.05873524
#   -0.00996126]
#  [-0.05303931  0.08799385  0.08227132 ... -0.04167942 -0.04918507
#   -0.00189221]
#  [ 0.01941064  0.05720583 -0.00805616 ... -0.07202554  0.00615531
#   -0.05741082]
#  ...
#  [ 0.06110692  0.00345555 -0.05413802 ... -0.03739041 -0.04096637
#    0.03696138]
#  [ 0.04641997  0.05074244  0.0003428  ...  0.08522545 -0.08433212
#    0.01248936]
#  [-0.08507212  0.06609162  0.02735423 ... -0.05896661  0.04827594
#    0.05005991]]
# -0.008988336
# 0.009021994
# -0.053039312

  
  # for o1 in ['C', 'F']:
  #   for o2 in ['C', 'F']:
  #     for dim0, dim1 in [(128, 384)]:
  #       print(f"weights.flatten(order={o1}).reshape({dim0}, {dim1}, order={o2})")
  #       weights = weights.flatten(order=o1).reshape(dim0, dim1, order=o2)

  #       print(f"{weights.shape=}")

  #       result = np.matmul(input_norm, weights)
  #       print(f"{result[0][0]=}")
  #       print(f"{result[1][0]=}")
  #       print(f"{result[0][1]=}")
  #       print(f"{result.shape=}")

  #       print('-'*20)

def case_2():
  Q0 = ''' Columns 1 to 9  0.7816  0.2182 -0.5967 -0.6641 -0.5740  0.5754  0.9927  0.1897 -0.0010
  0.1142  0.1890  0.4796  0.6132 -0.1758 -0.5132 -0.4686  0.5684 -0.7580

Columns 10 to 18  0.3508 -0.6623 -0.4952 -0.5038 -0.2951  0.7907  0.3334  0.6005  0.3095
  0.1059 -0.9421 -0.0358  0.5901 -0.7009  1.0662 -0.7423  0.5748 -1.3157

Columns 19 to 27  0.4978 -0.1412 -0.3135 -0.0893  0.0319  0.8357 -0.0375  0.7775 -0.9866
 -0.0193  0.2563 -0.3428 -0.5987  0.4950  0.3273  0.7174  0.1965  0.0034

Columns 28 to 36 -0.3525 -0.6089  1.1613  0.3135 -0.3337  0.3960  0.1398  0.5729  0.8596
 -0.0084  0.1167  0.8329 -1.5989  0.4726 -0.2916  0.5019  0.1763 -0.1552

Columns 37 to 45 -0.4243  0.7490  0.1169  0.4393  0.6917  0.5287  0.4618  0.7318 -0.9401
 -0.2427  0.8754  0.2789 -0.6365 -0.5680 -0.0184 -0.8926 -0.1592 -0.2299

Columns 46 to 54 -0.8897  0.1149  1.6177 -0.8365  1.3816 -0.3123 -0.7084 -0.5757  0.7002
 -0.3911 -0.3718  0.0437  0.1062 -1.1578 -0.6145 -0.7539 -0.6955 -1.0695

Columns 55 to 63  0.5533  0.0223  0.5999 -0.2611 -1.0698  0.1765  0.6165  0.3568  0.2168
 -0.8161 -1.0975 -0.8156  0.0807  1.4137 -0.2564 -0.2317  0.9167 -0.7836

Columns 64 to 64 -1.4957
 -0.0880'''

  Q0 = output_format_to_numpy(Q0, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  Q1 = '''Columns 1 to 9 -0.1059 -0.6212 -0.1832 -0.3533  0.8036  0.4080 -0.2310  0.1815 -0.1116
 -0.1508  0.5860 -0.6561  0.5026  0.0625 -0.4479 -1.1988  0.9187  0.3439

Columns 10 to 18  0.8945  0.1846  0.0126  0.5418  0.1616  0.0073  0.6619  0.1191  0.5974
 -0.6358  0.2502  0.2566 -0.5851 -0.7367 -0.1551 -0.8047  0.1899  0.3374

Columns 19 to 27 -0.0953  0.4197 -0.3458  0.0953 -0.2849  0.9333  0.0961  0.5790 -0.4502
 -0.2615 -0.4720  0.1783 -0.3847 -0.1889  0.8073  0.2382  0.4819 -0.2146

Columns 28 to 36  0.2253 -1.0316  0.1609 -0.2694 -0.5618  0.7192  1.1204 -0.8624  0.4389
 -0.4205  0.4281 -0.3806 -0.5208 -0.5082  0.5803 -0.1014 -0.1436 -0.4402

Columns 37 to 45  0.3857  0.0403  0.2308 -0.1865  1.3191 -0.4918 -0.4346 -0.0051 -0.4336
 -0.4383 -0.1340  0.1147  0.5723 -0.1317  0.4509  1.1373 -0.4683 -0.3365

Columns 46 to 54  0.0671 -0.4638  0.4019  1.2216  0.7999 -0.0292  0.8957  0.0788 -0.0755
  0.0597  0.4118 -0.3185  0.3395 -0.2471 -0.0383 -0.7783  0.6862  0.5101

Columns 55 to 63  0.6125  0.8265  0.4655 -0.8866 -1.2173  1.0072 -0.3879 -0.3823 -1.0458
 -0.2952 -0.3399 -0.4470 -0.5657  0.1882  0.0244  0.1526 -0.4349  0.3572

Columns 64 to 64 -0.0963
 -1.1778'''

  Q1 = output_format_to_numpy(Q1, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  Q = torch.cat((Q0.unsqueeze(1), Q1.unsqueeze(1)), dim=1)

  print(f"{Q.shape=}")
  print(Q)

  K0 = ''' Columns 1 to 9 -0.2070 -0.0802 -0.3264 -0.3735 -0.2153  0.7609 -0.4315 -0.0513  0.1610
  0.2679  0.4093  0.7580 -0.6820  0.0685  0.6804  0.4106  0.7682  0.9975

Columns 10 to 18 -0.1361  0.4660 -0.1081  0.0976  0.0715 -0.5853  0.2375  0.6171  0.7801
 -0.2893  0.6736 -0.8219 -0.1779 -0.7313 -0.6961 -0.1527 -0.0408  0.4760

Columns 19 to 27 -1.0615  1.0182  0.6093  0.5407 -0.4834  0.3660  0.5515 -1.1986 -0.3335
  0.2274 -0.2816  1.3787  0.7663 -0.6612  0.3471 -0.0510  0.4430  0.6930

Columns 28 to 36  0.2092  0.2017 -0.1859 -0.9623 -0.0461 -0.0748  0.2908  0.7627 -0.0957
 -0.6927  0.1138 -0.1637  0.1040 -0.8021  0.9052 -0.6556 -0.0745  1.1103

Columns 37 to 45  0.8808 -0.0712 -0.2723 -0.2144 -0.0443 -0.2620  0.1154  0.8655  0.0455
  0.0539 -0.9029 -0.0944 -0.5852 -1.0303  0.3013 -0.6300  0.9261 -0.3297

Columns 46 to 54 -0.7644 -0.1720 -0.1010  0.1692 -0.6255  0.8575 -0.0902 -0.7931 -0.6005
  0.2609 -0.3254  0.4619  1.1797  0.5314  0.2027 -0.5998 -0.5394 -0.1401

Columns 55 to 63  0.0394 -0.4366 -0.2055 -0.6681 -0.9517  0.4048  0.8629 -0.4931  0.9441
 -0.9623 -0.2975  0.0857  0.4390  0.2773  0.1420 -0.5309 -1.2483  0.7230

Columns 64 to 64 -1.1136
 -0.4965'''

  K0 = output_format_to_numpy(K0, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  K1 = '''Columns 1 to 9 -0.2287 -0.4777 -0.7038 -0.1130 -0.6067 -0.4735 -0.2960 -0.3712 -0.5108
 -0.0660 -0.6325  0.9033 -0.6616 -0.1606  0.7991  0.2767 -0.1298  0.4075

Columns 10 to 18 -0.3133  0.2550  0.0498 -0.3837 -0.3095 -0.5382 -0.3890  0.1138 -0.5153
 -0.2420  0.2164  1.1631  0.4055  0.1439  1.0441  1.1151 -0.8698  0.3938

Columns 19 to 27  0.2207  0.0994  0.1544  1.3248 -1.1930 -0.4838  1.0247  0.2549 -0.9181
 -0.5574  0.3987 -0.6866  0.2494 -0.1082  0.0444  0.4146 -0.4557  0.2621

Columns 28 to 36  0.7460  0.8365  0.4663 -0.0951 -0.2285 -0.5172  0.4428 -0.1643  0.8138
  0.3171 -0.1332  0.4793 -0.1823 -0.3557  0.7987  0.2203 -0.5441 -0.6431

Columns 37 to 45  1.1462  0.4526 -0.1573  0.0828 -0.5600  0.3344 -0.5365 -0.5617  0.2033
  0.1431  0.4914  0.4806 -0.4866  0.6111  0.4437 -0.2562 -0.6519 -0.1506

Columns 46 to 54 -0.1362  0.1817 -0.0833  0.7547  0.1360 -0.4704 -0.1767  0.0668  0.0495
 -0.7539 -0.6415  0.4411  0.2198  0.3992  0.1330  0.2499  0.5968  0.3790

Columns 55 to 63 -0.2015  0.0215  0.1744  0.4138  0.0813  0.4185 -0.2212  0.3575 -0.0956
  0.4375 -0.7087  0.3642  0.2765 -0.2911 -0.1942  0.2377  0.1791 -0.3763

Columns 64 to 64 -0.5006
 -0.6957'''

  K1 = output_format_to_numpy(K1, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  K = torch.cat((K0.unsqueeze(1), K1.unsqueeze(1)), dim=1)

  print(f"{K.shape=}")
  print(K)

  Qt0 = torch.transpose(Q, dim0=1, dim1=2)
  print(f"{Qt0.shape=}")
  print(Qt0)

  Kt0 = torch.transpose(K, dim0=1, dim1=2)
  print(f"{Kt0.shape=}")
  print(Kt0)

  Kt = torch.transpose(Kt0, dim0=2, dim1=3)
  print(f"{Kt.shape=}")
  print(Kt)

  energy = torch.matmul(Qt0, Kt)
  print(f"{energy.shape=}")
  print(energy)

  Qt0_0 = Qt0[:,0:1,:,:]
  print(f'{Qt0_0.shape=}')
  print(f'{Qt0_0}')

  Kt_0 = Kt[:,0:1,:,:]
  print(f'{Kt_0.shape=}')
  print(f'{Kt_0}')

  energy0 = torch.matmul(Qt0_0, Kt_0)
  print(f"{energy0.shape=}")
  print(energy0)

  energy1 = torch.matmul(Qt0[:,1:2,:,:], Kt[:,1:2,:,:])
  print(f"{energy1.shape=}")
  print(energy1)

  energy0_scaled = energy0 * 0.0883883476483
  print(f"{energy0_scaled.shape=}")
  print(energy0_scaled)

  energy1_scaled = energy1 * 0.0883883476483
  print(f"{energy1_scaled.shape=}")
  print(energy1_scaled)

  # def my_softmax(a):
  #   import math
  #   denom = 0.
  #   for el in a:
  #     denom += math.exp(el)
    
  #   res = [0] * len(a)
  #   for i, el in enumerate(a):
  #     res[i] = (1/denom) * math.exp(el)

  #   return res

  # a = [0.1139, -0.0598]
  # print(my_softmax(a))


  def my_matmul(a, b, r1, c1, r2, c2):
    res = [''] * (r1*c2)
    for i in range(c1):
      for j in range(c2):
        for k in range(r1):
          res[j*r1 + k] += a[i*r1 + k] + b[j*r2 + i] # + for str, * for others

    return res

  a = ['a', 'c', 'e', 'b', 'd', 'f']
  b = ['g', 'j', 'h', 'k', 'i', 'l']

  print(my_matmul(a, b, 3, 2, 2, 3))

def case_3():
  attention = torch.Tensor(
    [[
      [
        [0.5433, 0.4567],
        [0.5383, 0.4617]
      ],
      [
        [0.4330, 0.5670],
        [0.6094, 0.3906]
      ]
    ]]
  )
  print(f'{attention.shape=}')
  print(f'{attention=}')

  V0_raw = '''Columns 1 to 9 -0.0650 -0.5841 -0.1509 -0.2460 -0.2642 -0.8658  1.0201  0.3724  0.4119
  0.0941 -0.4614  0.3039  0.3784 -0.4162  0.0336  0.1146  0.3917  0.1663

Columns 10 to 18  0.2832 -0.7095  0.0459 -0.3490  0.3871  0.1638  0.3574  0.8309  0.0659
 -0.1820 -0.3054  0.2883  0.1130  0.4493 -0.1468  0.0985  0.0086 -0.3111

Columns 19 to 27 -0.0561 -0.0403  0.0645  0.7857 -0.2670  0.0510  0.2601 -0.2996 -0.5562
  0.0065 -0.6758 -0.5414 -0.2815  0.6808  0.5117 -0.2878 -0.6120 -0.2988

Columns 28 to 36  0.4390  0.0089 -0.2570 -0.0238  1.1973  0.1499 -0.1992  0.5500 -1.9580
  0.8792  0.3696 -0.1427  0.3740 -1.2102 -0.1181  0.5225 -0.5571  0.1194

Columns 37 to 45 -0.3174  0.3788 -0.4581  0.2740 -0.3381  0.5322 -0.6621 -0.6024  0.5271
  0.1596  0.1505  1.4997 -0.7408 -0.0636  0.0254  0.2522  0.5241 -0.7936

Columns 46 to 54  0.3351  0.5222  0.7184  0.3974  1.2780 -0.1299  0.3111 -0.2581 -0.3389
  1.0380 -0.6128  0.3465 -0.1650 -0.5822 -0.3650 -0.6806  0.4508  0.5543

Columns 55 to 63 -0.2982 -0.6822  0.6322 -0.5404 -0.3652  0.2605 -2.0046  0.7174 -0.2708
  0.1613 -0.2951  1.2724 -1.1358  0.4088 -0.2603  0.3140  0.6327 -0.7492

Columns 64 to 64 -0.2524
  0.4847
  '''

  V0 = output_format_to_numpy(V0_raw, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  # print(f'{V0=}')

  V1_raw = '''Columns 1 to 9  0.1560  0.0693  0.0125 -0.3305  0.7646  0.0466  0.8342 -0.7864  0.0428
  0.2903 -0.2793 -0.3975 -0.5091 -0.2505 -0.2594  0.6958 -0.2727  0.3977

Columns 10 to 18 -1.0113 -0.3754 -0.5327 -0.0994  0.9969 -0.4325 -0.6694 -0.1338  0.0131
  0.1172 -0.0116  0.0399  0.3364 -0.2216  0.4053  0.2306 -0.1834 -0.7994

Columns 19 to 27 -0.5812 -0.0225 -1.1963  0.1932  0.4850  0.1237 -0.9797 -0.6129  0.5215
  0.2654  0.3180  0.6228  0.5943 -0.3610  0.2584  0.4601 -0.4857 -0.3587

Columns 28 to 36 -1.9314 -0.8045  0.2324  1.1373  1.1950 -0.7049  0.1844 -0.2838 -0.9160
 -0.9528 -0.1678  0.6803 -0.7199 -0.9337  0.5682 -0.1224  0.7069  0.2910

Columns 37 to 45  0.8066 -0.1878 -0.1893 -0.3751  0.3851 -0.7158  0.4574  0.1460  0.6049
 -0.7566 -0.0869  0.1320 -0.1763  0.1449  0.2444 -0.2579 -0.8452  0.1696

Columns 46 to 54  0.7020 -0.2302 -0.2844  0.5544  0.7920 -0.8926  0.4333  0.8628 -0.4675
  0.2153 -0.3004  0.2897  1.1046  0.0004  0.3160  1.4463 -0.8350 -0.1920

Columns 55 to 63  0.4495  0.2607  0.2537 -0.1025  1.2429 -0.6866  0.3932 -0.5698 -0.6173
 -0.3577  0.2079  0.2383 -0.2284  0.3849 -0.7646 -0.4749 -0.1506 -0.3787

Columns 64 to 64  0.8253
 -0.4429
  '''

  V1 = output_format_to_numpy(V1_raw, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  # print(f'{V1=}')

  V = torch.cat((V0.unsqueeze(1), V1.unsqueeze(1)), dim=1)

  print(f'{V.shape=}')
  print(f'{V=}')

  # ABCD
  # ABDC ACBD ACDB ADBC ADCB
  attention_T = attention # torch.transpose(attention, dim0=1, dim1=2)
  # attention_T = torch.transpose(attention_T, dim0=2, dim1=3)
  V_T = torch.transpose(V, dim0=1, dim1=2)
  # V_T = torch.transpose(V_T, dim0=2, dim1=3)

  out_T = torch.matmul(attention_T.double(), V_T.double())
  # out_T = torch.matmul(V_T.double(), attention_T.double())
  out = torch.transpose(out_T, dim0=1, dim1=2)

  print(f'{out.shape=}')
  print(f'{out=}')

  def my_reshape(ar, particles=2, sc_att=3):
    size = particles * sc_att
    out = [[''] * size] * 2
    for jj in range(2):
      for kk in range(particles):
        for ii in range(sc_att):
          print(f'adding {ar[jj][ii * particles + kk]}')
          out[jj][ii + kk * sc_att] = ar[jj][ii * particles + kk]
          print(f'{out[jj][ii + kk * sc_att]=}')

    return out

  ar = [
    ['a', 'd', 'b', 'e', 'c', 'f'],
    ['g', 'j', 'h', 'k', 'i', 'l']
  ]

  print(my_reshape(ar))

import numpy as np
from math import log

def isPower2(x):
  return (x & (x-1) == 0) and x != 0

def generateTable(size: int, func, max_val: int) -> np.array:
  assert isPower2(size)

  table = np.empty(shape=(size))
  for i, x in enumerate(np.linspace(0, max_val, size, endpoint=False)):
    try:
      table[i] = func(x)
    except:
      table[i] = func(x + 0.001)
    print(f'{i=}, {x=}')
  return table

def case_4():
  table_size = 1024
  table = generateTable(size=table_size, func=log, max_val=32)

  name = 'log_table'
  guard = name.upper() + '_H_'
  var_type = 'general_table_t'
  
  table_values = ', '.join(['\n' * (n % 7 == 6) + f'{el:.16f}' for n, el in enumerate(table)])
  
  content = []
  content.append(f'#ifndef {guard}')
  content.append(f'#define {guard}')
  content.append('')
  # content.append('#ifndef __SYNTHESIS__')
  # content.append(f'{var_type} {name}[{table_size}];')
  # content.append('#else')
  content.append(f'{var_type} {name}[{table_size}] = {{{table_values}}};')
  # content.append('')
  # content.append('#endif')
  content.append('')
  content.append('#endif')

  content = '\n'.join(content)
  
  with open('log_table.h', 'w') as f:
    f.write(content)

def case_5():
  
  n_particles = 2
  n_size = 12


  for i in range(n_particles):
    for l in range(n_size//(2*3)):
      for j in range(2):

        in_index_Q = j * (n_size/2) + 0 * (n_size/(2*3)) + l
        in_index_K = j * (n_size/2) + 1 * (n_size/(2*3)) + l
        in_index_V = j * (n_size/2) + 2 * (n_size/(2*3)) + l


        out_index = l * 2 + j

        print(f'{i=}   ||   {j=}, {l=}   ||   {in_index_Q=}, {in_index_K=}, {in_index_V=}   |   {out_index=}')

    print('-'*50)

if __name__ == '__main__':
  case_5()