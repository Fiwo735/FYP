import numpy as np
import csv
import torch
import torch.nn as nn

torch.set_printoptions(profile='full', sci_mode=False, threshold=2097152, linewidth=1000)

# x_data = '29.938385009765625 178.55862426757812 -67.51207733154297 193.2311248779297 0.17523084580898285 181.0510711669922 0.1761978417634964 -0.3647479712963104 0.008448980748653412 -0.0 1.4046745300292969 0.000455104949651286 0.0 0.008461219258606434 -0.34938931465148926 6.123234262925839e-17'
# x = np.array([float(el) for el in x_data.split(' ')])

# def read_data():
#   t = []
#   with open('extracted_weights_biases/inp_layer_weight.txt', 'r') as infile:
#     for el in infile:
#       t = np.array([float(el) for el in el.split(',')])
#   return t

def transform_like_HLS(t):
  return t.reshape(16, 128).flatten(order='F')

def revert_HLS_read(t, dim0=16, dim1=128):
  return t.reshape(dim0, dim1, order='F').flatten(order='C')

def output_format_to_numpy(x, cols_in_line=9, end_cols_in_line=2, total_elements=256, e_notation=False):
  x = x.replace('\n', ' ').replace('  ', ' ').split(' ')
  if e_notation:
    x = [float(el) for el in x if 'e' in el]
  else:
    x = [float(el) for el in x if len(el) == 6 or (len(el) == 7 and el[0] == '-')]
  x = np.array(x)

  base_elements = total_elements - end_cols_in_line*2
  x1 = np.array([el for i, el in enumerate(x) if ((i < base_elements and i % (cols_in_line*2) < cols_in_line) or (i >= base_elements and i % (end_cols_in_line*2) < end_cols_in_line))])
  x2 = np.array([el for i, el in enumerate(x) if ((i < base_elements and i % (cols_in_line*2) >= cols_in_line) or (i >= base_elements and i % (end_cols_in_line*2) >= end_cols_in_line))])
  result = torch.unsqueeze(torch.cat((torch.unsqueeze(torch.from_numpy(x1), dim=0), torch.unsqueeze(torch.from_numpy(x2), dim=0)), dim=0), dim=0)
  return result

def case_0():
  two_cols_data = '''Columns 1 to 6  4.0129e-02  8.1093e-01 -6.7605e-02  4.5763e-02 -9.1097e-01 -7.3065e-01
 -6.1937e+00  1.5361e+01 -7.8609e+01  3.5738e+01 -3.8578e+01  6.5512e-01

Columns 7 to 12  1.9955e+00  1.7008e+00  1.1903e+00  6.8385e-01 -7.6752e-01 -1.4813e+00
  5.2728e+01 -3.1096e+01  3.4690e+01 -2.2029e+00  6.3353e+01  3.5349e+01

Columns 13 to 18  3.4362e-01  2.3606e+00 -3.4037e+00  1.5055e+00 -1.8098e-02  4.5189e-02
 -8.1889e+01 -7.3146e+01  6.8400e+01 -4.5082e+01 -8.7851e+00  3.4903e+01

Columns 19 to 24 -1.3255e+00  1.0194e+00  5.2586e-01  7.5091e-01  1.4665e-01  1.4115e-01
  1.0487e+02 -4.3092e+01 -1.9874e+01 -5.7966e+01 -6.3025e+00  5.6187e+01

Columns 25 to 30 -9.4811e-01 -6.8913e-01 -1.5163e-01  1.0495e+00 -1.2274e+00 -4.3410e-01
 -1.8383e+00 -6.8293e+01 -6.3265e+01  5.1773e+01 -2.2836e+01 -8.4276e+01

Columns 31 to 36 -2.9523e-01 -1.1433e+00  3.9863e-01  3.7319e-02  1.0493e+00 -3.2728e-01
 -7.4508e+01  2.0039e+01  8.1425e-01 -5.9820e+01  2.5564e+00 -1.1088e+02

Columns 37 to 42 -9.6229e-01 -7.8601e-01  5.6638e-01  2.0820e+00  9.5599e-01 -4.6012e-01
 -3.2059e+01 -3.1237e+01 -1.2671e+02 -1.3587e+01  4.3975e+01 -8.1219e+00

Columns 43 to 48 -1.1359e+00  7.5818e-01 -6.8697e-01  8.8815e-02  9.6351e-01  6.5712e-01
  2.0680e+01 -5.2564e+01  2.7641e+01  2.8673e+01  3.8287e+01  3.7847e+01

Columns 49 to 54 -1.9308e+00 -4.7343e-02 -7.8666e-01 -3.1326e-01 -1.8380e+00 -4.1530e-01
 -2.0089e+01  1.9987e+01  1.5085e+01  2.5509e+01 -2.3733e+00  4.0189e+01

Columns 55 to 60  1.1444e+00 -4.6327e-01 -8.1061e-01 -3.2287e-01  1.6043e-01  1.6304e-01
  5.1659e+01 -4.6149e+01 -8.6524e+01 -1.9592e+01 -1.1651e+02 -2.5306e+01

Columns 61 to 66  2.2874e+00  2.0731e-03  6.9055e-01  1.5403e+00  3.8258e-01 -1.3344e+00
 -5.1343e+01  5.7282e+01  6.2609e+01 -5.0239e+01  6.0019e+01 -6.7135e+01

Columns 67 to 72  2.3314e-02 -4.2542e-01 -8.5733e-01 -6.2700e-01 -9.6613e-01 -3.8246e-01
  4.6309e+01 -3.5972e+01 -3.8091e+01  9.2045e+01 -4.1175e+01 -2.3876e+00

Columns 73 to 78 -3.2845e-01  1.2401e+00 -8.3011e-01  1.0515e+00  1.8618e-01 -2.3656e-02
  7.3446e+01  1.7168e+01  4.3220e+01 -1.9498e+01  3.8385e+00  1.2803e+02

Columns 79 to 84 -1.2989e+00  6.7152e-01 -5.1173e-01 -1.0610e+00  3.9402e-01 -5.9869e-01
  8.8824e-01 -8.5643e+01 -2.1312e+01  7.6767e+00  2.0002e+01  1.8667e+00

Columns 85 to 90 -8.2251e-01  2.9724e-01  1.2275e+00  1.0696e+00  1.1230e-01  2.0362e+00
 -8.1969e+01 -1.0878e+01 -1.1542e+01  4.4753e+01  1.9577e+01  6.3550e+01

Columns 91 to 96  2.6253e-01 -3.6470e-01 -7.0298e-01 -1.8693e+00  7.7021e-01 -1.7509e+00
  3.0295e+01  7.9039e+00  6.8047e+01  4.0320e+01  6.8742e+01 -1.1869e+02

Columns 97 to 102 -7.5656e-01 -1.4887e+00 -7.0923e-01  1.5125e+00  1.2890e+00  1.0281e+00
  6.9538e+01  1.1321e+01  1.8625e+00  4.6260e+01  5.0166e+01 -8.2065e+01

Columns 103 to 108 -1.4097e+00  4.1892e-01  1.1135e+00 -1.2268e+00  3.7976e-01  9.0445e-01
  3.1439e+01 -3.1508e+01 -1.1561e+02 -1.5516e+01  4.3495e+01  8.7215e+01

Columns 109 to 114 -1.0730e+00 -2.1239e-01  4.4776e-01  8.0886e-02  2.7727e-01  1.4085e-01
 -1.7620e+01 -1.8318e+00 -2.0270e+01 -3.4970e+01  5.5462e+01  6.5110e+01

Columns 115 to 120  9.0327e-01 -3.3493e-01  1.7014e+00  1.8344e-01  6.8673e-03  1.0173e+00
  8.8996e+01  2.1583e+01 -9.4023e+00  5.4407e+01  2.2995e+01  6.3767e+01

Columns 121 to 126  6.3203e-01 -4.9540e-01 -1.5465e-01  4.6508e-01  3.3193e-01 -8.8931e-01
 -2.9605e+01 -2.3669e+01  4.6626e+01 -3.1904e+01  5.3989e+01 -4.0746e+01

Columns 127 to 128 -8.1273e-01  5.3334e-01
 -5.9265e+01  2.7296e+01'''

  col_data = output_format_to_numpy(two_cols_data, cols_in_line=6, end_cols_in_line=2, e_notation=True)

  print(f'{col_data=}')

  col_mean = torch.mean(col_data, dim=2)
  print("col_mean:")
  print(col_mean)

  col_var = torch.var(col_data, dim=2, unbiased=False)
  print("col_var:")
  print(col_var)

  # col_var_sum = None
  # col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
  # col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
  # col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
  # print("col_var_sum")
  # print(col_var_sum)

  # col_var_avg = torch.div(col_var_sum, 3)
  # print("col_var_avg")
  # print(col_var_avg)

  eps = 1e-5
  W0 = 0.99982661008 # 0.99982661008
  b0 = 0.00038191396 # 0.00038191396

  def layer_norm(x, mean, var, eps, weight, bias):
    numerator = x - mean
    denominator = np.sqrt(var + eps)
    return numerator * weight / denominator + bias

  col1_data_normalized = np.array([layer_norm(x=x, mean=col_mean[0][0], var=col_var[0][0], eps=eps, weight=W0, bias=b0) for x in col1_data])
  print(col1_data_normalized)
  print(col1_data_normalized.shape)

  col2_data_normalized = np.array([layer_norm(x=x, mean=col_mean[0][1], var=col_var[0][1], eps=eps, weight=W0, bias=b0) for x in col2_data])
  print(col2_data_normalized)
  print(col2_data_normalized.shape)

  results = ''' Columns 1 to 9  0.0269  0.8112 -0.0830  0.0319 -0.9405 -0.7571  2.0154  1.7169  1.1973
  -0.1204  0.2913 -1.5156  0.6827 -0.7446  0.0081  1.0114 -0.6022  0.6674

  Columns 10 to 18  0.6811 -0.7951 -1.5215  0.3356  2.3869 -3.4773  1.5171 -0.0327  0.0313
  -0.0456  1.2158  0.6755 -1.5757 -1.4110  1.3134 -0.8719 -0.1692  0.6682

  Columns 19 to 27 -1.3624  1.0228  0.5204  0.7497  0.1355  0.1293 -0.9782 -0.7146 -0.1684
    2.0125 -0.8306 -0.3866 -1.1160 -0.1219  1.0768 -0.0392 -1.3145 -1.2181

  Columns 28 to 36  1.0538 -1.2626 -0.4555 -0.3145 -1.1772  0.3913  0.0238  1.0535 -0.3474
    0.9965 -0.4400 -1.6229 -1.4356  0.3822  0.0124 -1.1517  0.0475 -2.1390

  Columns 37 to 45 -0.9927 -0.8135  0.5622  2.1029  0.9584 -0.4817 -1.1695  0.7574 -0.7139
  -0.6206 -0.6035 -2.4428 -0.2628  0.8427 -0.1568  0.3955 -1.0165  0.5264

  Columns 46 to 54  0.0763  0.9659  0.6542 -1.9788 -0.0624 -0.8145 -0.3329 -1.8825 -0.4371
    0.5488  0.7359  0.7247 -0.3920  0.3825  0.2859  0.4862 -0.0456  0.7695

  Columns 55 to 63  1.1498 -0.4855 -0.8378 -0.3423  0.1488  0.1513  2.3109 -0.0120  0.6882
    0.9894 -0.8917 -1.6650 -0.3808 -2.2436 -0.4913 -0.9882  1.1002  1.2014

  Columns 64 to 72  1.5514  0.3752 -1.3709  0.0099 -0.4466 -0.8856 -0.6522 -0.9969 -0.4033
  -0.9672  1.1544 -1.2918  0.8899 -0.6923 -0.7348  1.7687 -0.7946 -0.0490

  Columns 73 to 81 -0.3482  1.2473 -0.8587  1.0551  0.1753 -0.0386 -1.3348  0.6690 -0.5346
    1.4133  0.3290  0.8271 -0.3804  0.0729  2.4583  0.0147 -1.6503 -0.4108

  Columns 82 to 90 -1.0934  0.3866 -0.6237 -0.8511  0.2884  1.2341  1.0730  0.1002  2.0573
    0.1461  0.3839  0.0311 -1.5783 -0.2133 -0.2225  0.8575  0.3761  1.2221

  Columns 91 to 99  0.2528 -0.3850 -0.7296 -1.9156  0.7693 -1.7941 -0.7833 -1.5295 -0.7355
    0.5777  0.1513  1.3076  0.7750  1.3183 -2.2829  1.3326  0.2136  0.0310

  Columns 100 to 108  1.5252  1.2976  1.0307 -1.4479  0.4124  1.1194 -1.2616  0.3726  0.9053
    0.8897  0.9644 -1.5828  0.6027 -0.6073 -2.2261 -0.2994  0.8365  1.6730

  Columns 109 to 117 -1.1063 -0.2299  0.4412  0.0682  0.2676  0.1295  0.9051 -0.3545  1.7165
  -0.3402 -0.0359 -0.3911 -0.6765  1.0624  1.2499  1.7122  0.4116 -0.1820

  Columns 118 to 126  0.1731 -0.0076  1.0214  0.6284 -0.5181 -0.1720  0.4591  0.3228 -0.9185
    1.0429  0.4402  1.2273 -0.5705 -0.4559  0.8919 -0.6148  1.0335 -0.7867

  Columns 127 to 128 -0.8407  0.5285
  -1.1420  0.5243'''

  results = results.replace('\n', ' ').replace('  ', ' ').split(' ')
  results = [float(el) for el in results if len(el) == 6 or (len(el) == 7 and el[0] == '-')]
  results = np.array(results)

  col1_res = np.array([el for i, el in enumerate(results) if ((i < 252 and i % 18 < 9) or (i >= 252 and i % 4 < 2))])
  col2_res = np.array([el for i, el in enumerate(results) if ((i < 252 and i % 18 >= 9) or (i >= 252 and i % 4 >= 2))])
  print("results:\n")
  print(col1_res)
  print(col2_res)
  print(col1_res.shape)
  print(col2_res.shape)

  print("diff:\n")
  col1_diff = np.absolute(col1_data_normalized - col1_res)
  print(col1_diff)
  col2_diff = np.absolute(col2_data_normalized - col2_res)
  print(col2_diff)
  print(f"Biggest error in col1 is {np.amax(col1_diff)}")
  print(f"Biggest error in col2 is {np.amax(col2_diff)}")


  A = torch.tensor([1, 2, 3])
  print(A[0])
  print(A[1:])

  X0 = 3.9991e-02
  X1 = -6.1523

  b0 = -0.01386488933016245
  b1 = -0.0007655795714506348

  W0 = 1.0169684936528849
  W1 = 0.08921465251486772

  print(f"{(X0*W0+b0)=}")
  print(f"{(X1*W1+b1)=}")

class MyLinear(nn.Module):
  def __init__(self, in_dim):
    super(MyLinear, self).__init__()
    self.linear = nn.Linear(in_dim, 3*in_dim, bias=False)

  def forward(self, x):
    return self.linear(x.double())

def case_1():
  input_norm_output = ''' Columns 1 to 9  0.0269  0.8110 -0.0829  0.0321 -0.9407 -0.7572  2.0156  1.7165  1.1969
 -0.1213  0.2932 -1.5148  0.6848 -0.7445  0.0103  1.0117 -0.6006  0.6650

Columns 10 to 18  0.6813 -0.7950 -1.5212  0.3354  2.3870 -3.4773  1.5172 -0.0328  0.0315
 -0.0448  1.2155  0.6774 -1.5769 -1.4096  1.3134 -0.8701 -0.1716  0.6687

Columns 19 to 27 -1.3625  1.0226  0.5205  0.7497  0.1352  0.1294 -0.9786 -0.7149 -0.1685
  2.0147 -0.8308 -0.3849 -1.1173 -0.1234  1.0780 -0.0376 -1.3157 -1.2194

Columns 28 to 36  1.0534 -1.2627 -0.4558 -0.3144 -1.1771  0.3914  0.0238  1.0532 -0.3472
  0.9934 -0.4416 -1.6235 -1.4358  0.3832  0.0134 -1.1529  0.0468 -2.1359

Columns 37 to 45 -0.9927 -0.8136  0.5622  2.1031  0.9582 -0.4819 -1.1692  0.7571 -0.7135
 -0.6189 -0.6033 -2.4403 -0.2636  0.8434 -0.1584  0.3952 -1.0139  0.5290

Columns 46 to 54  0.0761  0.9658  0.6541 -1.9788 -0.0626 -0.8144 -0.3328 -1.8830 -0.4367
  0.5489  0.7338  0.7254 -0.3893  0.3817  0.2874  0.4883 -0.0479  0.7705

Columns 55 to 63  1.1497 -0.4855 -0.8382 -0.3426  0.1489  0.1514  2.3116 -0.0119  0.6882
  0.9911 -0.8904 -1.6660 -0.3793 -2.2435 -0.4897 -0.9898  1.0996  1.2017

Columns 64 to 72  1.5517  0.3750 -1.3711  0.0098 -0.4465 -0.8861 -0.6520 -0.9968 -0.4033
 -0.9688  1.1522 -1.2934  0.8882 -0.6939 -0.7350  1.7685 -0.7944 -0.0486

Columns 73 to 81 -0.3482  1.2472 -0.8586  1.0552  0.1753 -0.0385 -1.3348  0.6688 -0.5345
  1.4106  0.3275  0.8286 -0.3779  0.0715  2.4600  0.0148 -1.6493 -0.4123

Columns 82 to 90 -1.0934  0.3866 -0.6236 -0.8512  0.2883  1.2344  1.0735  0.1001  2.0571
  0.1448  0.3823  0.0332 -1.5796 -0.2114 -0.2247  0.8581  0.3741  1.2200

Columns 91 to 99  0.2527 -0.3850 -0.7297 -1.9156  0.7694 -1.7946 -0.7835 -1.5291 -0.7354
  0.5801  0.1498  1.3064  0.7733  1.3199 -2.2847  1.3346  0.2151  0.0333

Columns 100 to 108  1.5247  1.2973  1.0311 -1.4480  0.4122  1.1190 -1.2616  0.3723  0.9053
  0.8875  0.9628 -1.5806  0.6019 -0.6084 -2.2267 -0.3007  0.8344  1.6745

Columns 109 to 117 -1.1061 -0.2299  0.4412  0.0682  0.2675  0.1293  0.9048 -0.3546  1.7164
 -0.3416 -0.0375 -0.3924 -0.6750  1.0639  1.2497  1.7097  0.4127 -0.1836

Columns 118 to 126  0.1727 -0.0074  1.0210  0.6285 -0.5183 -0.1717  0.4591  0.3230 -0.9187
  1.0445  0.4395  1.2245 -0.5724 -0.4579  0.8942 -0.6161  1.0353 -0.7862

Columns 127 to 128 -0.8409  0.5283
 -1.1424  0.5225'''

  input_norm_output = output_format_to_numpy(input_norm_output, cols_in_line=9, end_cols_in_line=2)

  print(f"{input_norm_output.shape=}")
  print(input_norm_output)

  hls_input = '0.0269086 -0.118814 0.811038 0.294311 -0.0831297 -1.51152 0.0323419 0.68669 -0.941092 -0.741211 -0.757548 0.0115624 2.0151 1.01299 1.71621 -0.598496 1.19619 0.667438 0.681356 -0.0429487 -0.795151 1.21724 -1.5202 0.67852 0.335759 -1.57406 2.38676 -1.40752 -3.477 1.31437 1.51783 -0.867604 -0.0323599 -0.168306 0.031863 0.67087 -1.36191 2.0143 1.02314 -0.828293 0.520689 -0.382319 0.749343 -1.11421 0.134622 -0.120575 0.129044 1.07912 -0.979151 -0.0362937 -0.715236 -1.31286 -0.168044 -1.21587 1.05322 0.996247 -1.26183 -0.438499 -0.45586 -1.62081 -0.31454 -1.43233 -1.17661 0.384195 0.391431 0.0150427 0.0240982 -1.1491 1.05301 0.0496797 -0.347022 -2.13216 -0.993042 -0.617535 -0.813817 -0.601399 0.561986 -2.43682 2.10302 -0.260688 0.958246 0.844396 -0.482265 -0.156033 -1.17003 0.397064 0.756928 -1.0113 -0.712592 0.530414 0.076093 0.551064 0.965819 0.736508 0.654377 0.726731 -1.97731 -0.387319 -0.0623615 0.384214 -0.814551 0.28912 -0.333022 0.489469 -1.88423 -0.0451786 -0.436484 0.77173 1.15011 0.991698 -0.485388 -0.887677 -0.838797 -1.66231 -0.342433 -0.377171 0.148756 -2.24024 0.151342 -0.487263 2.3128 -0.986615 -0.012348 1.10122 0.6876 1.20261 1.55237 -0.965043 0.033364 -0.192759 -0.120115 0.199762 0.000779615 -0.158341 -0.039034 0.122834 -0.0775555 0.131805 -0.0573609 -0.294633 -0.0870022 0.13104 -0.035301 0.0214841 -0.0305592 -0.227438 0.109561 -0.0474663 -0.075392 -0.13453 0.0930227 0.0519626 0.0158479 -0.0187631 -0.0032622 -0.395971 -0.116724 -0.0166748 0.0590363 0.260917 -0.0465029 0.0711151 -0.0956452 -0.0109163 0.0342386 -0.0614958 -0.0543914 -0.0150816 -0.0744552 0.244669 0.0254268 0.0457671 0.108041 0.0431511 0.094012 -0.134045 0.00849369 -0.0548889 0.180314 -0.185107 0.0226559 -0.0918293 -0.0338965 -0.00779573 -0.0634669 -0.209088 -0.167985 -0.129041 0.0674409 -0.195141 -0.157114 0.387549 -0.0685798 -0.217299 -0.133527 -0.021343 -0.0642605 -0.00911182 0.13389 -0.131588 0.114145 -0.156813 0.0904209 0.261517 -0.126631 -0.0986003 0.03649 0.0934614 0.0983884 0.362082 -0.110539 0.0321699 0.0325461 -0.129954 0.0793815 -0.278186 -0.0966719 0.0520174 -0.0201377 -0.00506127 0.0385967 0.0686211 0.00625272 0.104839 0.0236516 -0.162944 0.0114029 -0.201089 0.0796175 -0.293633 -0.0312517 -0.0688784 0.151052 0.0126527 0.0155205 -0.160365 -0.000574701 -0.0641685 0.0896675 -0.190804 0.0550329 0.0970125 -0.0451128 0.0606544 -0.0151648 -0.125356 0.0401825 0.101634 0.0281035 -0.173165 -0.0806829 0.12045 -0.073967 0.173474 0.0464663 -0.0679771'
  hls_input = torch.unsqueeze(torch.from_numpy(np.array([float(el) for el in hls_input.split(' ')]).reshape(2, 128, order='F')), dim=0)

  print(f'{hls_input.shape=}')
  print(f'{hls_input=}')

  # input_norm_raw = '''0.0269086 -0.118814 0.811038 0.294311 -0.0831297 -1.51152 0.0323419 0.68669 -0.941092 -0.741211 -0.757548 0.0115624 2.0151 1.01299 1.71621 -0.598496 1.19619 0.667438 0.681356 -0.0429487 -0.795151 1.21724 -1.5202 0.67852 0.335759 -1.57406 2.38676 -1.40752 -3.477 1.31437 1.51783 -0.867604 -0.0323599 -0.168306 0.031863 0.67087 -1.36191 2.0143 1.02314 -0.828293 0.520689 -0.382319 0.749343 -1.11421 0.134622 -0.120575 0.129044 1.07912 -0.979151 -0.0362937 -0.715236 -1.31286 -0.168044 -1.21587 1.05322 0.996247 -1.26183 -0.438499 -0.45586 -1.62081 -0.31454 -1.43233 -1.17661 0.384195 0.391431 0.0150427 0.0240982 -1.1491 1.05301 0.0496797 -0.347022 -2.13216 -0.993042 -0.617535 -0.813817 -0.601399 0.561986 -2.43682 2.10302 -0.260688 0.958246 0.844396 -0.482265 -0.156033 -1.17003 0.397064 0.756928 -1.0113 -0.712592 0.530414 0.076093 0.551064 0.965819 0.736508 0.654377 0.726731 -1.97731 -0.387319 -0.0623615 0.384214 -0.814551 0.28912 -0.333022 0.489469 -1.88423 -0.0451786 -0.436484 0.77173 1.15011 0.991698 -0.485388 -0.887677 -0.838797 -1.66231 -0.342433 -0.377171 0.148756 -2.24024 0.151342 -0.487263 2.3128 -0.986615 -0.012348 1.10122 0.6876 1.20261 1.55237 -0.965043 0.033364 -0.192759 -0.120115 0.199762 0.000779615 -0.158341 -0.039034 0.122834 -0.0775555 0.131805 -0.0573609 -0.294633 -0.0870022 0.13104 -0.035301 0.0214841 -0.0305592 -0.227438 0.109561 -0.0474663 -0.075392 -0.13453 0.0930227 0.0519626 0.0158479 -0.0187631 -0.0032622 -0.395971 -0.116724 -0.0166748 0.0590363 0.260917 -0.0465029 0.0711151 -0.0956452 -0.0109163 0.0342386 -0.0614958 -0.0543914 -0.0150816 -0.0744552 0.244669 0.0254268 0.0457671 0.108041 0.0431511 0.094012 -0.134045 0.00849369 -0.0548889 0.180314 -0.185107 0.0226559 -0.0918293 -0.0338965 -0.00779573 -0.0634669 -0.209088 -0.167985 -0.129041 0.0674409 -0.195141 -0.157114 0.387549 -0.0685798 -0.217299 -0.133527 -0.021343 -0.0642605 -0.00911182 0.13389 -0.131588 0.114145 -0.156813 0.0904209 0.261517 -0.126631 -0.0986003 0.03649 0.0934614 0.0983884 0.362082 -0.110539 0.0321699 0.0325461 -0.129954 0.0793815 -0.278186 -0.0966719 0.0520174 -0.0201377 -0.00506127 0.0385967 0.0686211 0.00625272 0.104839 0.0236516 -0.162944 0.0114029 -0.201089 0.0796175 -0.293633 -0.0312517 -0.0688784 0.151052 0.0126527 0.0155205 -0.160365 -0.000574701 -0.0641685 0.0896675 -0.190804 0.0550329 0.0970125 -0.0451128 0.0606544 -0.0151648 -0.125356 0.0401825 0.101634 0.0281035 -0.173165 -0.0806829 0.12045 -0.073967 0.173474 0.0464663 -0.0679771'''

  # input_norm = np.array([float(el) for el in input_norm_raw.split(' ')]).reshape(2, 128, order='F')
  # # print(input_norm)
  # print(f"{input_norm.shape=}")
  # print(input_norm)

  weights_raw = np.genfromtxt('extracted_weights_biases/transformers_0_self_attention_qkv_weight.txt', delimiter=',')
  print(f"{weights_raw=}")
  weights = weights_raw.reshape(128, 384, order='C')
  weights = weights.flatten(order='F').reshape(384, 128, order='C')
  # weights *= 100

  print(f"{weights.shape=}")
  print(f"{weights=}")

#   result = np.matmul(input_norm, weights.T)
#   print(f"{result.shape=}")
#   print(f"{result[0][0]=}")
#   print(f"{result[1][0]=}")
#   print(f"{result[0][1]=}")
#   # print(f"{result[0]=}")
#   # print(f"{result[1]=}")

#   print("-"*10 + '\n'*3)

#   my_linear = MyLinear(in_dim=128)
#   my_linear = my_linear.double()

#   with torch.no_grad():
#     my_linear.linear.weight = nn.Parameter(torch.from_numpy(weights).double())
#     print(my_linear.linear.weight.size())
#     print(my_linear.linear.weight[0][0])
#     print(my_linear.linear.weight[0][1])
#     print(my_linear.linear.weight[1][0])
#     torch_input = torch.from_numpy(input_norm).double()
#     torch_result = my_linear(torch_input)
#     print(f"{torch_result.size()=}")
#     print(f"{torch_result[0][0]=}")
#     print(f"{torch_result[0][1]=}")
#     print(f"{torch_result[1][0]=}")

# # ------------------------------
  # print("-"*10 + '\n'*3)

  print("-"*10 + '\n')

  print('from Pytorch')
  result = np.matmul(input_norm_output, weights.T)
  print(f"{result.shape=}")
  print(f"{result[0][0][0]=}")
  print(f"{result[0][0][1]=}")
  print(f"{result[0][1][0]=}")
  # print(f"{result[0]=}")
  # print(f"{result[1]=}")

  print("-"*10 + '\n')

  print('from HLS')
  result = np.matmul(hls_input, weights.T)
  print(f"{result.shape=}")
  print(f"{result[0][0][0]=}")
  print(f"{result[0][0][1]=}")
  print(f"{result[0][1][0]=}")
  # print(f"{result[0]=}")
  # print(f"{result[1]=}")

  print("-"*10 + '\n')

  my_linear = MyLinear(in_dim=128)
  my_linear = my_linear.double()

  with torch.no_grad():
    my_linear.linear.weight = nn.Parameter(torch.from_numpy(weights).double())
    print(my_linear.linear.weight.size())
    print(my_linear.linear.weight[0][0])
    print(my_linear.linear.weight[0][1])
    print(my_linear.linear.weight[1][0])
    torch_input = input_norm_output.double()
    torch_result = my_linear(torch_input)
    print(f"{torch_result.size()=}")
    print(f"{torch_result[0][0][0]=}")
    print(f"{torch_result[0][0][1]=}")
    print(f"{torch_result[0][1][0]=}")

  
# ----------transformers.0.self_attention.qkv.weight----------
# (384, 128)
# [[-0.00898834  0.00902199 -0.01533934 ... -0.005541   -0.05873524
#   -0.00996126]
#  [-0.05303931  0.08799385  0.08227132 ... -0.04167942 -0.04918507
#   -0.00189221]
#  [ 0.01941064  0.05720583 -0.00805616 ... -0.07202554  0.00615531
#   -0.05741082]
#  ...
#  [ 0.06110692  0.00345555 -0.05413802 ... -0.03739041 -0.04096637
#    0.03696138]
#  [ 0.04641997  0.05074244  0.0003428  ...  0.08522545 -0.08433212
#    0.01248936]
#  [-0.08507212  0.06609162  0.02735423 ... -0.05896661  0.04827594
#    0.05005991]]
# -0.008988336
# 0.009021994
# -0.053039312

  
  # for o1 in ['C', 'F']:
  #   for o2 in ['C', 'F']:
  #     for dim0, dim1 in [(128, 384)]:
  #       print(f"weights.flatten(order={o1}).reshape({dim0}, {dim1}, order={o2})")
  #       weights = weights.flatten(order=o1).reshape(dim0, dim1, order=o2)

  #       print(f"{weights.shape=}")

  #       result = np.matmul(input_norm, weights)
  #       print(f"{result[0][0]=}")
  #       print(f"{result[1][0]=}")
  #       print(f"{result[0][1]=}")
  #       print(f"{result.shape=}")

  #       print('-'*20)

def case_2():
  Q0 = ''' Columns 1 to 9  0.7816  0.2182 -0.5967 -0.6641 -0.5740  0.5754  0.9927  0.1897 -0.0010
  0.1142  0.1890  0.4796  0.6132 -0.1758 -0.5132 -0.4686  0.5684 -0.7580

Columns 10 to 18  0.3508 -0.6623 -0.4952 -0.5038 -0.2951  0.7907  0.3334  0.6005  0.3095
  0.1059 -0.9421 -0.0358  0.5901 -0.7009  1.0662 -0.7423  0.5748 -1.3157

Columns 19 to 27  0.4978 -0.1412 -0.3135 -0.0893  0.0319  0.8357 -0.0375  0.7775 -0.9866
 -0.0193  0.2563 -0.3428 -0.5987  0.4950  0.3273  0.7174  0.1965  0.0034

Columns 28 to 36 -0.3525 -0.6089  1.1613  0.3135 -0.3337  0.3960  0.1398  0.5729  0.8596
 -0.0084  0.1167  0.8329 -1.5989  0.4726 -0.2916  0.5019  0.1763 -0.1552

Columns 37 to 45 -0.4243  0.7490  0.1169  0.4393  0.6917  0.5287  0.4618  0.7318 -0.9401
 -0.2427  0.8754  0.2789 -0.6365 -0.5680 -0.0184 -0.8926 -0.1592 -0.2299

Columns 46 to 54 -0.8897  0.1149  1.6177 -0.8365  1.3816 -0.3123 -0.7084 -0.5757  0.7002
 -0.3911 -0.3718  0.0437  0.1062 -1.1578 -0.6145 -0.7539 -0.6955 -1.0695

Columns 55 to 63  0.5533  0.0223  0.5999 -0.2611 -1.0698  0.1765  0.6165  0.3568  0.2168
 -0.8161 -1.0975 -0.8156  0.0807  1.4137 -0.2564 -0.2317  0.9167 -0.7836

Columns 64 to 64 -1.4957
 -0.0880'''

  Q0 = output_format_to_numpy(Q0, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  Q1 = '''Columns 1 to 9 -0.1059 -0.6212 -0.1832 -0.3533  0.8036  0.4080 -0.2310  0.1815 -0.1116
 -0.1508  0.5860 -0.6561  0.5026  0.0625 -0.4479 -1.1988  0.9187  0.3439

Columns 10 to 18  0.8945  0.1846  0.0126  0.5418  0.1616  0.0073  0.6619  0.1191  0.5974
 -0.6358  0.2502  0.2566 -0.5851 -0.7367 -0.1551 -0.8047  0.1899  0.3374

Columns 19 to 27 -0.0953  0.4197 -0.3458  0.0953 -0.2849  0.9333  0.0961  0.5790 -0.4502
 -0.2615 -0.4720  0.1783 -0.3847 -0.1889  0.8073  0.2382  0.4819 -0.2146

Columns 28 to 36  0.2253 -1.0316  0.1609 -0.2694 -0.5618  0.7192  1.1204 -0.8624  0.4389
 -0.4205  0.4281 -0.3806 -0.5208 -0.5082  0.5803 -0.1014 -0.1436 -0.4402

Columns 37 to 45  0.3857  0.0403  0.2308 -0.1865  1.3191 -0.4918 -0.4346 -0.0051 -0.4336
 -0.4383 -0.1340  0.1147  0.5723 -0.1317  0.4509  1.1373 -0.4683 -0.3365

Columns 46 to 54  0.0671 -0.4638  0.4019  1.2216  0.7999 -0.0292  0.8957  0.0788 -0.0755
  0.0597  0.4118 -0.3185  0.3395 -0.2471 -0.0383 -0.7783  0.6862  0.5101

Columns 55 to 63  0.6125  0.8265  0.4655 -0.8866 -1.2173  1.0072 -0.3879 -0.3823 -1.0458
 -0.2952 -0.3399 -0.4470 -0.5657  0.1882  0.0244  0.1526 -0.4349  0.3572

Columns 64 to 64 -0.0963
 -1.1778'''

  Q1 = output_format_to_numpy(Q1, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  Q = torch.cat((Q0.unsqueeze(1), Q1.unsqueeze(1)), dim=1)

  print(f"{Q.shape=}")
  print(Q)

  K0 = ''' Columns 1 to 9 -0.2070 -0.0802 -0.3264 -0.3735 -0.2153  0.7609 -0.4315 -0.0513  0.1610
  0.2679  0.4093  0.7580 -0.6820  0.0685  0.6804  0.4106  0.7682  0.9975

Columns 10 to 18 -0.1361  0.4660 -0.1081  0.0976  0.0715 -0.5853  0.2375  0.6171  0.7801
 -0.2893  0.6736 -0.8219 -0.1779 -0.7313 -0.6961 -0.1527 -0.0408  0.4760

Columns 19 to 27 -1.0615  1.0182  0.6093  0.5407 -0.4834  0.3660  0.5515 -1.1986 -0.3335
  0.2274 -0.2816  1.3787  0.7663 -0.6612  0.3471 -0.0510  0.4430  0.6930

Columns 28 to 36  0.2092  0.2017 -0.1859 -0.9623 -0.0461 -0.0748  0.2908  0.7627 -0.0957
 -0.6927  0.1138 -0.1637  0.1040 -0.8021  0.9052 -0.6556 -0.0745  1.1103

Columns 37 to 45  0.8808 -0.0712 -0.2723 -0.2144 -0.0443 -0.2620  0.1154  0.8655  0.0455
  0.0539 -0.9029 -0.0944 -0.5852 -1.0303  0.3013 -0.6300  0.9261 -0.3297

Columns 46 to 54 -0.7644 -0.1720 -0.1010  0.1692 -0.6255  0.8575 -0.0902 -0.7931 -0.6005
  0.2609 -0.3254  0.4619  1.1797  0.5314  0.2027 -0.5998 -0.5394 -0.1401

Columns 55 to 63  0.0394 -0.4366 -0.2055 -0.6681 -0.9517  0.4048  0.8629 -0.4931  0.9441
 -0.9623 -0.2975  0.0857  0.4390  0.2773  0.1420 -0.5309 -1.2483  0.7230

Columns 64 to 64 -1.1136
 -0.4965'''

  K0 = output_format_to_numpy(K0, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  K1 = '''Columns 1 to 9 -0.2287 -0.4777 -0.7038 -0.1130 -0.6067 -0.4735 -0.2960 -0.3712 -0.5108
 -0.0660 -0.6325  0.9033 -0.6616 -0.1606  0.7991  0.2767 -0.1298  0.4075

Columns 10 to 18 -0.3133  0.2550  0.0498 -0.3837 -0.3095 -0.5382 -0.3890  0.1138 -0.5153
 -0.2420  0.2164  1.1631  0.4055  0.1439  1.0441  1.1151 -0.8698  0.3938

Columns 19 to 27  0.2207  0.0994  0.1544  1.3248 -1.1930 -0.4838  1.0247  0.2549 -0.9181
 -0.5574  0.3987 -0.6866  0.2494 -0.1082  0.0444  0.4146 -0.4557  0.2621

Columns 28 to 36  0.7460  0.8365  0.4663 -0.0951 -0.2285 -0.5172  0.4428 -0.1643  0.8138
  0.3171 -0.1332  0.4793 -0.1823 -0.3557  0.7987  0.2203 -0.5441 -0.6431

Columns 37 to 45  1.1462  0.4526 -0.1573  0.0828 -0.5600  0.3344 -0.5365 -0.5617  0.2033
  0.1431  0.4914  0.4806 -0.4866  0.6111  0.4437 -0.2562 -0.6519 -0.1506

Columns 46 to 54 -0.1362  0.1817 -0.0833  0.7547  0.1360 -0.4704 -0.1767  0.0668  0.0495
 -0.7539 -0.6415  0.4411  0.2198  0.3992  0.1330  0.2499  0.5968  0.3790

Columns 55 to 63 -0.2015  0.0215  0.1744  0.4138  0.0813  0.4185 -0.2212  0.3575 -0.0956
  0.4375 -0.7087  0.3642  0.2765 -0.2911 -0.1942  0.2377  0.1791 -0.3763

Columns 64 to 64 -0.5006
 -0.6957'''

  K1 = output_format_to_numpy(K1, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  K = torch.cat((K0.unsqueeze(1), K1.unsqueeze(1)), dim=1)

  print(f"{K.shape=}")
  print(K)

  Qt0 = torch.transpose(Q, dim0=1, dim1=2)
  print(f"{Qt0.shape=}")
  print(Qt0)

  Kt0 = torch.transpose(K, dim0=1, dim1=2)
  print(f"{Kt0.shape=}")
  print(Kt0)

  Kt = torch.transpose(Kt0, dim0=2, dim1=3)
  print(f"{Kt.shape=}")
  print(Kt)

  energy = torch.matmul(Qt0, Kt)
  print(f"{energy.shape=}")
  print(energy)

  Qt0_0 = Qt0[:,0:1,:,:]
  print(f'{Qt0_0.shape=}')
  print(f'{Qt0_0}')

  Kt_0 = Kt[:,0:1,:,:]
  print(f'{Kt_0.shape=}')
  print(f'{Kt_0}')

  energy0 = torch.matmul(Qt0_0, Kt_0)
  print(f"{energy0.shape=}")
  print(energy0)

  energy1 = torch.matmul(Qt0[:,1:2,:,:], Kt[:,1:2,:,:])
  print(f"{energy1.shape=}")
  print(energy1)

  energy0_scaled = energy0 * 0.0883883476483
  print(f"{energy0_scaled.shape=}")
  print(energy0_scaled)

  energy1_scaled = energy1 * 0.0883883476483
  print(f"{energy1_scaled.shape=}")
  print(energy1_scaled)

  # def my_softmax(a):
  #   import math
  #   denom = 0.
  #   for el in a:
  #     denom += math.exp(el)
    
  #   res = [0] * len(a)
  #   for i, el in enumerate(a):
  #     res[i] = (1/denom) * math.exp(el)

  #   return res

  # a = [0.1139, -0.0598]
  # print(my_softmax(a))


  def my_matmul(a, b, r1, c1, r2, c2):
    res = [''] * (r1*c2)
    for i in range(c1):
      for j in range(c2):
        for k in range(r1):
          res[j*r1 + k] += a[i*r1 + k] + b[j*r2 + i] # + for str, * for others

    return res

  a = ['a', 'c', 'e', 'b', 'd', 'f']
  b = ['g', 'j', 'h', 'k', 'i', 'l']

  print(my_matmul(a, b, 3, 2, 2, 3))

def case_3():
  attention = torch.Tensor(
    [[
      [
        [0.5433, 0.4567],
        [0.5383, 0.4617]
      ],
      [
        [0.4330, 0.5670],
        [0.6094, 0.3906]
      ]
    ]]
  )
  print(f'{attention.shape=}')
  print(f'{attention=}')

  V0_raw = '''Columns 1 to 9 -0.0650 -0.5841 -0.1509 -0.2460 -0.2642 -0.8658  1.0201  0.3724  0.4119
  0.0941 -0.4614  0.3039  0.3784 -0.4162  0.0336  0.1146  0.3917  0.1663

Columns 10 to 18  0.2832 -0.7095  0.0459 -0.3490  0.3871  0.1638  0.3574  0.8309  0.0659
 -0.1820 -0.3054  0.2883  0.1130  0.4493 -0.1468  0.0985  0.0086 -0.3111

Columns 19 to 27 -0.0561 -0.0403  0.0645  0.7857 -0.2670  0.0510  0.2601 -0.2996 -0.5562
  0.0065 -0.6758 -0.5414 -0.2815  0.6808  0.5117 -0.2878 -0.6120 -0.2988

Columns 28 to 36  0.4390  0.0089 -0.2570 -0.0238  1.1973  0.1499 -0.1992  0.5500 -1.9580
  0.8792  0.3696 -0.1427  0.3740 -1.2102 -0.1181  0.5225 -0.5571  0.1194

Columns 37 to 45 -0.3174  0.3788 -0.4581  0.2740 -0.3381  0.5322 -0.6621 -0.6024  0.5271
  0.1596  0.1505  1.4997 -0.7408 -0.0636  0.0254  0.2522  0.5241 -0.7936

Columns 46 to 54  0.3351  0.5222  0.7184  0.3974  1.2780 -0.1299  0.3111 -0.2581 -0.3389
  1.0380 -0.6128  0.3465 -0.1650 -0.5822 -0.3650 -0.6806  0.4508  0.5543

Columns 55 to 63 -0.2982 -0.6822  0.6322 -0.5404 -0.3652  0.2605 -2.0046  0.7174 -0.2708
  0.1613 -0.2951  1.2724 -1.1358  0.4088 -0.2603  0.3140  0.6327 -0.7492

Columns 64 to 64 -0.2524
  0.4847
  '''

  V0 = output_format_to_numpy(V0_raw, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  # print(f'{V0=}')

  V1_raw = '''Columns 1 to 9  0.1560  0.0693  0.0125 -0.3305  0.7646  0.0466  0.8342 -0.7864  0.0428
  0.2903 -0.2793 -0.3975 -0.5091 -0.2505 -0.2594  0.6958 -0.2727  0.3977

Columns 10 to 18 -1.0113 -0.3754 -0.5327 -0.0994  0.9969 -0.4325 -0.6694 -0.1338  0.0131
  0.1172 -0.0116  0.0399  0.3364 -0.2216  0.4053  0.2306 -0.1834 -0.7994

Columns 19 to 27 -0.5812 -0.0225 -1.1963  0.1932  0.4850  0.1237 -0.9797 -0.6129  0.5215
  0.2654  0.3180  0.6228  0.5943 -0.3610  0.2584  0.4601 -0.4857 -0.3587

Columns 28 to 36 -1.9314 -0.8045  0.2324  1.1373  1.1950 -0.7049  0.1844 -0.2838 -0.9160
 -0.9528 -0.1678  0.6803 -0.7199 -0.9337  0.5682 -0.1224  0.7069  0.2910

Columns 37 to 45  0.8066 -0.1878 -0.1893 -0.3751  0.3851 -0.7158  0.4574  0.1460  0.6049
 -0.7566 -0.0869  0.1320 -0.1763  0.1449  0.2444 -0.2579 -0.8452  0.1696

Columns 46 to 54  0.7020 -0.2302 -0.2844  0.5544  0.7920 -0.8926  0.4333  0.8628 -0.4675
  0.2153 -0.3004  0.2897  1.1046  0.0004  0.3160  1.4463 -0.8350 -0.1920

Columns 55 to 63  0.4495  0.2607  0.2537 -0.1025  1.2429 -0.6866  0.3932 -0.5698 -0.6173
 -0.3577  0.2079  0.2383 -0.2284  0.3849 -0.7646 -0.4749 -0.1506 -0.3787

Columns 64 to 64  0.8253
 -0.4429
  '''

  V1 = output_format_to_numpy(V1_raw, cols_in_line=9, end_cols_in_line=1, total_elements=128)

  # print(f'{V1=}')

  V = torch.cat((V0.unsqueeze(1), V1.unsqueeze(1)), dim=1)

  print(f'{V.shape=}')
  print(f'{V=}')

  # ABCD
  # ABDC ACBD ACDB ADBC ADCB
  attention_T = attention # torch.transpose(attention, dim0=1, dim1=2)
  # attention_T = torch.transpose(attention_T, dim0=2, dim1=3)
  V_T = torch.transpose(V, dim0=1, dim1=2)
  # V_T = torch.transpose(V_T, dim0=2, dim1=3)

  out_T = torch.matmul(attention_T.double(), V_T.double())
  # out_T = torch.matmul(V_T.double(), attention_T.double())
  out = torch.transpose(out_T, dim0=1, dim1=2)

  print(f'{out.shape=}')
  print(f'{out=}')

  def my_reshape(ar, particles=2, sc_att=3):
    size = particles * sc_att
    out = [[''] * size] * 2
    for jj in range(2):
      for kk in range(particles):
        for ii in range(sc_att):
          print(f'adding {ar[jj][ii * particles + kk]}')
          out[jj][ii + kk * sc_att] = ar[jj][ii * particles + kk]
          print(f'{out[jj][ii + kk * sc_att]=}')

    return out

  ar = [
    ['a', 'd', 'b', 'e', 'c', 'f'],
    ['g', 'j', 'h', 'k', 'i', 'l']
  ]

  print(my_reshape(ar))

import numpy as np
from math import log

def isPower2(x):
  return (x & (x-1) == 0) and x != 0

def generateTable(size: int, func, max_val: int) -> np.array:
  assert isPower2(size)

  table = np.empty(shape=(size))
  for i, x in enumerate(np.linspace(0, max_val, size, endpoint=False)):
    try:
      table[i] = func(x)
    except:
      table[i] = func(x + 0.001)
    print(f'{i=}, {x=}')
  return table

def case_4():
<<<<<<< HEAD
  table_size = 1024
=======
  table_size = 1024 * 4
>>>>>>> 9c0d86c28c83f71f1cb2ea0cb2e3aa899ae4e20c
  table = generateTable(size=table_size, func=log, max_val=32)

  name = 'log_table'
  guard = name.upper() + '_H_'
<<<<<<< HEAD
  var_type = 'general_table_t'
=======
  var_type = 'top_log_softmax_log_t'
>>>>>>> 9c0d86c28c83f71f1cb2ea0cb2e3aa899ae4e20c
  
  table_values = ', '.join(['\n' * (n % 7 == 6) + f'{el:.16f}' for n, el in enumerate(table)])
  
  content = []
  content.append(f'#ifndef {guard}')
  content.append(f'#define {guard}')
  content.append('')
  # content.append('#ifndef __SYNTHESIS__')
  # content.append(f'{var_type} {name}[{table_size}];')
  # content.append('#else')
  content.append(f'{var_type} {name}[{table_size}] = {{{table_values}}};')
  # content.append('')
  # content.append('#endif')
  content.append('')
  content.append('#endif')

  content = '\n'.join(content)
  
  with open('log_table.h', 'w') as f:
    f.write(content)

def case_5():
  
  n_particles = 2
  n_size = 12


  for i in range(n_particles):
    for l in range(n_size//(2*3)):
      for j in range(2):

        in_index_Q = j * (n_size/2) + 0 * (n_size/(2*3)) + l
        in_index_K = j * (n_size/2) + 1 * (n_size/(2*3)) + l
        in_index_V = j * (n_size/2) + 2 * (n_size/(2*3)) + l


        out_index = l * 2 + j

        print(f'{i=}   ||   {j=}, {l=}   ||   {in_index_Q=}, {in_index_K=}, {in_index_V=}   |   {out_index=}')

    print('-'*50)

def case_6():
  q_raw = '-0.573581 -0.0126638 -2.6687 -0.0104666 0.237875 -0.0807695 0.138438 -1.82827 1.81419 -3.1979 0.363598 -0.0369701 -1.48174 -0.029624 -0.824718 2.07316 -0.364979 0.508903 -0.165102 -0.0899839 0.571781 0.467929 0.012764 0.834812 0.499103 0.306692 -0.671947 -1.48111 -0.379766 -1.01598 -0.297065 -0.410746'
  Q = torch.tensor([float(el) for el in q_raw.split(' ')]).resize(2, 16)

  k_raw = '0.874372 -0.309067 1.07256 -0.767412 -0.637673 -0.242593 0.0283365 2.7894 -1.61147 2.46017 -0.23806 -1.22862 0.38568 0.0877934 0.744831 -2.13324 -1.12905 -0.183349 -0.513114 -0.100763 0.0185909 -0.0231266 0.172382 -0.521946 1.79796 -0.661231 0.163136 0.00135326 -1.20765 -0.205399 -0.569398 0.481296'
  K = torch.tensor([float(el) for el in k_raw.split(' ')]).resize(2, 16)

  QQ = torch.Tensor([
    [
      [-0.57357, -2.66869,  0.23788,  0.13845,  1.81420,  0.36361, -1.48174, -0.82471],
      [-0.01266, -0.01046, -0.08076, -1.82825, -3.19789, -0.03696, -0.02962,  2.07316]
    ],
    [
      [-0.36497, -0.16509,  0.57179,  0.01277,  0.49912, -0.67194, -0.37975, -0.29706],
      [0.50890, -0.08997,  0.46794,  0.83482,  0.30670, -1.48110, -1.01597, -0.41075]
    ],
  ])

  KK = torch.Tensor([
    [
      [0.87438,  1.07257, -0.63767, 0.02834, -1.61146, -0.23805, 0.38569, 0.74484],
      [-0.30906, -0.76740, -0.24259, 2.78941, 2.46018, -1.22861, 0.08780, -2.13324]
    ],
    [
      [-1.12905, -0.51312,  0.01861, 0.17239, 1.79798, 0.16316, -1.20765, -0.56939],
      [-0.18334, -0.10076, -0.02312, -0.52194, -0.66121, 0.00136, -0.20540, 0.48130]
    ],
  ])

  print(f'{Q=}')
  print(f'{Q.shape=}\n')

  print(f'{K=}')
  print(f'{K.shape=}\n')

<<<<<<< HEAD
  n_particles = 2

=======
>>>>>>> 9c0d86c28c83f71f1cb2ea0cb2e3aa899ae4e20c
  q = 2
  h = 2
  c = 8

  k = 2
  h = 2
  c = 8

  res = torch.zeros((2, 2, 2))
  print(f'{res.shape=}\n')

<<<<<<< HEAD
  # K * Q^T
=======
  # K * Q^T qhc,khc->hqk
>>>>>>> 9c0d86c28c83f71f1cb2ea0cb2e3aa899ae4e20c
  for cc in range(c):
    for hh in range(h):
      for qq in range(q):
        for kk in range(k):

          # res[hh][kk][qq] += K[qq][hh][cc] * Q[kk][hh][cc]
          res[hh][kk][qq] += K[qq][hh + cc * h] * Q[kk][hh + cc * h]

<<<<<<< HEAD
          # for hhh in range(h):
          #   for ccc in range(c):
          #       res[hh][qq][kk] += K[qq][hhh][ccc] * Q[kk][hhh][ccc]
                # res[hh][qq][kk] += K[qq][hhh * c + ccc] * Q[kk][hhh * c + ccc]
                # res[hh][qq][kk] += K[qq][hhh + ccc * h] * Q[kk][hhh + ccc * h]

          # print(f'{res[hh][qq][kk]=}')

=======
>>>>>>> 9c0d86c28c83f71f1cb2ea0cb2e3aa899ae4e20c
  print(res)
  print('\n')
  print(torch.einsum("qhc,khc->hqk", [QQ, KK]))

<<<<<<< HEAD
  quit()
  -7.70746
  -1.87252
  7.62541
  1.92514 
  -17.3153
  5.48813
  4.07788
  -0.7246 
  



if __name__ == '__main__':
  case_6()
=======
def case_7():
  a = torch.Tensor([[1.56459, 1.51986, -1.29900, 0.24279, 0.24415]])
  print(f'{a=}')
  
  # res = nn.functional.softmax(a, dim=1)
  # print(f'{res=}')

  # res2 = nn.functional.softmax(-a, dim=1)
  # print(f'{res2=}')

  es = a.exp()
  print(f'{es=}')

  s = es.sum()
  print(f'{s=}')

  l = s.log()
  print(f'{l=}')

  res = a - l
  print(f'{res=}')

  # log_res = res.log()
  # print(f'{log_res=}')

def case_8():
  att = torch.Tensor([[[0.03222, 0.03405, 0.04132, 0.04240, 0.03299, 0.03216, 0.03272, 0.03203, 0.03161, 0.03238, 0.03158, 0.02755, 0.03134, 0.03289, 0.03418, 0.03394, 0.03117, 0.03231, 0.02710, 0.02984, 0.03294, 0.03463, 0.03009, 0.03637, 0.03261, 0.02890, 0.03171, 0.02902, 0.03162, 0.03329, 0.02302],
          [0.03299, 0.02277, 0.01770, 0.01722, 0.02789, 0.02894, 0.02814, 0.02984, 0.03073, 0.03078, 0.03224, 0.03597, 0.03331, 0.03171, 0.03151, 0.03155, 0.03388, 0.03355, 0.03799, 0.03591, 0.03377, 0.03271, 0.03619, 0.03156, 0.03434, 0.03769, 0.03519, 0.03802, 0.03496, 0.03350, 0.04741],
          [0.02674, 0.02013, 0.01790, 0.01675, 0.02874, 0.02997, 0.02908, 0.03078, 0.03268, 0.03153, 0.03318, 0.03827, 0.03298, 0.03057, 0.02980, 0.02957, 0.03314, 0.03230, 0.04120, 0.03625, 0.03223, 0.03042, 0.03631, 0.02810, 0.03272, 0.03905, 0.03429, 0.03955, 0.03382, 0.03098, 0.06099],
          [0.02639, 0.02089, 0.01813, 0.01939, 0.03201, 0.03313, 0.03127, 0.03289, 0.03384, 0.03267, 0.03382, 0.03814, 0.03291, 0.03037, 0.02863, 0.02913, 0.03269, 0.03169, 0.04083, 0.03617, 0.03124, 0.02919, 0.03578, 0.02657, 0.03151, 0.03885, 0.03334, 0.03839, 0.03261, 0.02980, 0.05770],
          [0.02973, 0.03323, 0.03547, 0.03723, 0.03811, 0.03976, 0.03851, 0.03824, 0.03740, 0.03630, 0.03486, 0.03453, 0.03393, 0.03250, 0.03141, 0.03150, 0.03050, 0.03045, 0.03051, 0.02977, 0.02930, 0.02883, 0.02891, 0.02829, 0.02850, 0.02891, 0.02853, 0.02861, 0.02842, 0.02829, 0.02945],
          [0.03040, 0.03391, 0.03632, 0.03798, 0.03920, 0.03733, 0.03917, 0.03838, 0.03717, 0.03677, 0.03556, 0.03503, 0.03394, 0.03274, 0.03173, 0.03184, 0.03071, 0.03038, 0.02998, 0.02962, 0.02899, 0.02876, 0.02860, 0.02823, 0.02825, 0.02841, 0.02798, 0.02815, 0.02807, 0.02778, 0.02862],
          [0.03125, 0.03283, 0.03513, 0.03646, 0.03706, 0.03813, 0.03564, 0.03885, 0.03751, 0.03693, 0.03594, 0.03462, 0.03445, 0.03329, 0.03238, 0.03261, 0.03118, 0.03079, 0.02995, 0.02965, 0.02956, 0.02936, 0.02857, 0.02903, 0.02880, 0.02845, 0.02849, 0.02830, 0.02853, 0.02845, 0.02780],
          [0.03159, 0.03465, 0.03812, 0.03893, 0.03764, 0.03846, 0.04016, 0.03557, 0.03889, 0.03907, 0.03635, 0.03376, 0.03429, 0.03406, 0.03318, 0.03276, 0.03061, 0.03049, 0.02820, 0.02858, 0.02895, 0.02943, 0.02747, 0.02905, 0.02805, 0.02679, 0.02764, 0.02692, 0.02770, 0.02814, 0.02449],
          [0.03220, 0.03485, 0.03790, 0.03883, 0.03683, 0.03785, 0.03962, 0.03939, 0.03492, 0.03955, 0.03651, 0.03390, 0.03484, 0.03408, 0.03343, 0.03277, 0.03064, 0.03057, 0.02794, 0.02834, 0.02903, 0.02931, 0.02741, 0.02918, 0.02808, 0.02693, 0.02764, 0.02723, 0.02758, 0.02818, 0.02445],
          [0.03358, 0.03369, 0.03730, 0.03783, 0.03571, 0.03681, 0.03936, 0.04020, 0.03994, 0.03534, 0.03698, 0.03291, 0.03496, 0.03557, 0.03497, 0.03353, 0.03059, 0.03077, 0.02663, 0.02790, 0.02911, 0.02988, 0.02713, 0.02986, 0.02821, 0.02630, 0.02777, 0.02711, 0.02783, 0.02859, 0.02362],
          [0.03358, 0.03300, 0.03790, 0.03838, 0.03548, 0.03677, 0.03945, 0.03977, 0.03946, 0.04036, 0.03346, 0.03238, 0.03525, 0.03605, 0.03523, 0.03386, 0.03053, 0.03089, 0.02637, 0.02756, 0.02888, 0.02988, 0.02673, 0.02972, 0.02815, 0.02644, 0.02748, 0.02697, 0.02787, 0.02857, 0.02359],
          [0.03278, 0.03575, 0.04184, 0.04349, 0.03579, 0.03675, 0.03947, 0.03948, 0.03914, 0.04004, 0.03659, 0.03030, 0.03442, 0.03502, 0.03448, 0.03300, 0.02936, 0.02974, 0.02541, 0.02685, 0.02840, 0.02909, 0.02640, 0.02947, 0.02763, 0.02571, 0.02731, 0.02680, 0.02757, 0.02834, 0.02361],
          [0.03391, 0.03261, 0.03753, 0.03828, 0.03407, 0.03489, 0.03852, 0.03897, 0.03895, 0.04103, 0.03703, 0.03098, 0.03260, 0.03675, 0.03622, 0.03485, 0.03053, 0.03094, 0.02602, 0.02749, 0.02951, 0.03018, 0.02693, 0.03026, 0.02850, 0.02641, 0.02796, 0.02698, 0.02820, 0.02914, 0.02378],
          [0.03366, 0.03253, 0.03610, 0.03670, 0.03340, 0.03393, 0.03742, 0.03872, 0.03893, 0.04153, 0.03783, 0.03091, 0.03573, 0.03374, 0.03668, 0.03507, 0.03091, 0.03126, 0.02608, 0.02772, 0.02986, 0.03075, 0.02739, 0.03110, 0.02888, 0.02667, 0.02812, 0.02729, 0.02837, 0.02897, 0.02374],
          [0.03468, 0.03137, 0.03462, 0.03572, 0.03270, 0.03342, 0.03646, 0.03736, 0.03766, 0.04077, 0.03677, 0.03110, 0.03558, 0.03697, 0.03446, 0.03544, 0.03109, 0.03172, 0.02696, 0.02864, 0.03030, 0.03076, 0.02785, 0.03136, 0.02922, 0.02731, 0.02874, 0.02796, 0.02886, 0.02967, 0.02448],
          [0.03355, 0.03221, 0.03521, 0.03619, 0.03310, 0.03367, 0.03679, 0.03751, 0.03672, 0.03907, 0.03627, 0.03078, 0.03528, 0.03662, 0.03692, 0.03297, 0.03110, 0.03186, 0.02712, 0.02861, 0.03034, 0.03110, 0.02814, 0.03168, 0.02949, 0.02744, 0.02896, 0.02813, 0.02907, 0.02968, 0.02444],
          [0.03395, 0.03305, 0.03502, 0.03636, 0.03231, 0.03315, 0.03545, 0.03627, 0.03597, 0.03825, 0.03595, 0.03048, 0.03496, 0.03606, 0.03618, 0.03472, 0.03049, 0.03169, 0.02718, 0.02874, 0.03100, 0.03138, 0.02852, 0.03177, 0.02991, 0.02803, 0.02938, 0.02846, 0.02981, 0.03036, 0.02515],
          [0.03377, 0.03279, 0.03528, 0.03639, 0.03210, 0.03292, 0.03574, 0.03634, 0.03602, 0.03788, 0.03570, 0.03036, 0.03444, 0.03588, 0.03574, 0.03504, 0.03128, 0.03156, 0.02725, 0.02897, 0.03086, 0.03141, 0.02865, 0.03208, 0.03001, 0.02820, 0.02952, 0.02867, 0.02996, 0.03042, 0.02478],
          [0.03375, 0.03541, 0.03823, 0.04063, 0.03257, 0.03323, 0.03558, 0.03574, 0.03563, 0.03723, 0.03431, 0.03003, 0.03392, 0.03490, 0.03490, 0.03388, 0.03079, 0.03137, 0.02699, 0.02853, 0.03047, 0.03111, 0.02845, 0.03180, 0.02983, 0.02819, 0.02965, 0.02849, 0.02972, 0.03040, 0.02426],
          [0.03371, 0.03443, 0.03697, 0.03732, 0.03161, 0.03243, 0.03467, 0.03510, 0.03522, 0.03704, 0.03476, 0.03031, 0.03398, 0.03518, 0.03541, 0.03401, 0.03097, 0.03174, 0.02770, 0.02897, 0.03108, 0.03135, 0.02914, 0.03222, 0.03031, 0.02881, 0.03005, 0.02903, 0.03042, 0.03102, 0.02504],
          [0.03441, 0.03218, 0.03433, 0.03621, 0.03093, 0.03181, 0.03407, 0.03451, 0.03442, 0.03612, 0.03418, 0.03057, 0.03427, 0.03617, 0.03566, 0.03451, 0.03140, 0.03231, 0.02809, 0.02971, 0.03125, 0.03162, 0.02949, 0.03306, 0.03095, 0.02918, 0.03066, 0.02969, 0.03102, 0.03164, 0.02556],
          [0.03473, 0.03308, 0.03491, 0.03652, 0.03127, 0.03181, 0.03422, 0.03463, 0.03466, 0.03652, 0.03422, 0.02972, 0.03429, 0.03680, 0.03539, 0.03505, 0.03126, 0.03204, 0.02731, 0.02920, 0.03116, 0.03160, 0.02930, 0.03280, 0.03113, 0.02893, 0.03043, 0.02939, 0.03084, 0.03175, 0.02503],
          [0.03433, 0.03371, 0.03605, 0.03802, 0.03144, 0.03210, 0.03453, 0.03456, 0.03474, 0.03591, 0.03349, 0.02985, 0.03325, 0.03521, 0.03460, 0.03425, 0.03079, 0.03186, 0.02765, 0.02940, 0.03119, 0.03176, 0.02966, 0.03242, 0.03097, 0.02926, 0.03071, 0.02966, 0.03106, 0.03165, 0.02592],
          [0.03518, 0.03154, 0.03419, 0.03559, 0.03093, 0.03161, 0.03394, 0.03427, 0.03460, 0.03622, 0.03417, 0.03009, 0.03404, 0.03587, 0.03579, 0.03455, 0.03123, 0.03206, 0.02774, 0.02966, 0.03156, 0.03186, 0.02962, 0.03270, 0.03131, 0.02946, 0.03071, 0.02998, 0.03154, 0.03193, 0.02606],
          [0.03415, 0.03208, 0.03511, 0.03759, 0.03183, 0.03254, 0.03495, 0.03504, 0.03501, 0.03622, 0.03412, 0.03014, 0.03384, 0.03511, 0.03493, 0.03395, 0.03106, 0.03151, 0.02764, 0.02909, 0.03104, 0.03138, 0.02931, 0.03238, 0.03093, 0.02908, 0.03058, 0.02987, 0.03109, 0.03159, 0.02687],
          [0.03459, 0.03582, 0.03898, 0.04143, 0.03294, 0.03346, 0.03567, 0.03531, 0.03492, 0.03667, 0.03400, 0.02853, 0.03331, 0.03495, 0.03507, 0.03359, 0.02984, 0.03116, 0.02576, 0.02799, 0.03026, 0.03110, 0.02838, 0.03216, 0.03014, 0.02896, 0.02992, 0.02889, 0.03038, 0.03105, 0.02475],
          [0.03380, 0.03326, 0.03564, 0.03669, 0.03197, 0.03240, 0.03450, 0.03463, 0.03448, 0.03561, 0.03414, 0.03018, 0.03363, 0.03476, 0.03460, 0.03362, 0.03083, 0.03168, 0.02778, 0.02933, 0.03111, 0.03151, 0.02970, 0.03211, 0.03108, 0.02943, 0.03094, 0.03014, 0.03133, 0.03157, 0.02756],
          [0.03424, 0.03596, 0.03848, 0.03928, 0.03298, 0.03311, 0.03510, 0.03521, 0.03494, 0.03590, 0.03375, 0.02908, 0.03306, 0.03451, 0.03445, 0.03341, 0.03019, 0.03090, 0.02648, 0.02820, 0.03053, 0.03109, 0.02875, 0.03212, 0.03058, 0.02884, 0.03031, 0.03021, 0.03078, 0.03128, 0.02625],
          [0.03401, 0.03483, 0.03682, 0.03794, 0.03309, 0.03340, 0.03487, 0.03488, 0.03488, 0.03603, 0.03390, 0.02993, 0.03319, 0.03437, 0.03412, 0.03311, 0.03026, 0.03084, 0.02738, 0.02881, 0.03070, 0.03099, 0.02910, 0.03217, 0.03075, 0.02916, 0.03040, 0.03002, 0.03128, 0.03128, 0.02749],
          [0.03477, 0.03534, 0.03691, 0.03844, 0.03361, 0.03391, 0.03577, 0.03526, 0.03539, 0.03675, 0.03394, 0.02917, 0.03337, 0.03505, 0.03471, 0.03355, 0.03003, 0.03088, 0.02637, 0.02821, 0.03036, 0.03060, 0.02869, 0.03186, 0.03041, 0.02859, 0.03007, 0.02955, 0.03066, 0.03162, 0.02618],
          [0.03334, 0.04058, 0.04174, 0.04274, 0.03527, 0.03521, 0.03645, 0.03543, 0.03513, 0.03579, 0.03315, 0.02859, 0.03174, 0.03306, 0.03269, 0.03181, 0.02902, 0.02958, 0.02608, 0.02746, 0.02947, 0.02992, 0.02845, 0.03083, 0.02969, 0.02860, 0.03003, 0.02968, 0.03050, 0.03090, 0.02705]],

         [[0.02977, 0.02640, 0.02349, 0.02316, 0.02918, 0.02979, 0.02870, 0.02917, 0.02906, 0.02852, 0.02973, 0.03304, 0.03051, 0.02920, 0.02934, 0.02953, 0.03256, 0.03228, 0.03867, 0.03660, 0.03403, 0.03333, 0.03722, 0.03189, 0.03488, 0.03937, 0.03623, 0.03811, 0.03499, 0.03444, 0.04683],
          [0.03457, 0.03487, 0.04022, 0.04145, 0.03448, 0.03312, 0.03372, 0.03266, 0.03283, 0.03380, 0.03261, 0.03008, 0.03275, 0.03406, 0.03418, 0.03479, 0.03143, 0.03196, 0.02831, 0.02967, 0.03150, 0.03225, 0.02930, 0.03336, 0.03127, 0.02754, 0.03036, 0.02820, 0.03091, 0.03140, 0.02236],
          [0.03829, 0.03590, 0.04090, 0.04492, 0.03780, 0.03605, 0.03820, 0.03735, 0.03655, 0.03753, 0.03431, 0.03022, 0.03345, 0.03492, 0.03488, 0.03566, 0.03076, 0.03142, 0.02434, 0.02732, 0.03029, 0.03111, 0.02617, 0.03316, 0.02940, 0.02354, 0.02776, 0.02400, 0.02871, 0.02941, 0.01567],
          [0.03750, 0.03769, 0.04606, 0.04651, 0.04272, 0.04124, 0.04242, 0.03987, 0.03811, 0.03766, 0.03407, 0.02950, 0.03253, 0.03381, 0.03412, 0.03447, 0.02973, 0.03032, 0.02328, 0.02585, 0.02884, 0.02956, 0.02458, 0.03174, 0.02765, 0.02157, 0.02619, 0.02239, 0.02693, 0.02757, 0.01553],
          [0.02914, 0.03467, 0.03735, 0.03906, 0.03837, 0.03995, 0.03855, 0.03814, 0.03732, 0.03619, 0.03481, 0.03457, 0.03387, 0.03233, 0.03120, 0.03123, 0.03026, 0.03021, 0.03036, 0.02953, 0.02904, 0.02851, 0.02864, 0.02790, 0.02814, 0.02872, 0.02825, 0.02834, 0.02806, 0.02791, 0.02937],
          [0.02965, 0.03369, 0.03575, 0.03719, 0.03895, 0.03736, 0.03869, 0.03796, 0.03687, 0.03634, 0.03547, 0.03561, 0.03391, 0.03237, 0.03128, 0.03143, 0.03072, 0.03032, 0.03090, 0.03006, 0.02894, 0.02854, 0.02905, 0.02782, 0.02828, 0.02922, 0.02820, 0.02877, 0.02820, 0.02766, 0.03081],
          [0.02932, 0.03311, 0.03461, 0.03565, 0.03673, 0.03797, 0.03543, 0.03738, 0.03615, 0.03504, 0.03520, 0.03638, 0.03426, 0.03174, 0.03045, 0.03080, 0.03109, 0.03039, 0.03263, 0.03075, 0.02917, 0.02844, 0.02970, 0.02753, 0.02859, 0.03110, 0.02895, 0.03017, 0.02872, 0.02799, 0.03455],
          [0.02875, 0.03162, 0.03256, 0.03384, 0.03627, 0.03721, 0.03644, 0.03544, 0.03594, 0.03412, 0.03447, 0.03773, 0.03351, 0.03088, 0.02959, 0.02980, 0.03091, 0.03009, 0.03462, 0.03192, 0.02884, 0.02849, 0.03089, 0.02696, 0.02878, 0.03305, 0.02960, 0.03146, 0.02908, 0.02845, 0.03871],
          [0.02847, 0.03107, 0.03163, 0.03281, 0.03530, 0.03633, 0.03527, 0.03567, 0.03464, 0.03323, 0.03387, 0.03867, 0.03374, 0.03012, 0.02858, 0.02951, 0.03119, 0.03025, 0.03605, 0.03252, 0.02919, 0.02855, 0.03153, 0.02745, 0.02919, 0.03384, 0.03015, 0.03240, 0.02945, 0.02902, 0.04031],
          [0.02734, 0.03143, 0.03155, 0.03277, 0.03398, 0.03492, 0.03275, 0.03343, 0.03341, 0.03310, 0.03248, 0.04003, 0.03232, 0.02808, 0.02655, 0.02847, 0.03093, 0.02981, 0.03855, 0.03360, 0.02912, 0.02812, 0.03287, 0.02728, 0.02980, 0.03660, 0.03138, 0.03435, 0.03056, 0.02991, 0.04448],
          [0.02779, 0.03079, 0.03028, 0.03162, 0.03325, 0.03427, 0.03200, 0.03214, 0.03213, 0.02933, 0.03324, 0.04081, 0.03222, 0.02770, 0.02595, 0.02777, 0.03151, 0.03026, 0.03985, 0.03456, 0.03010, 0.02884, 0.03347, 0.02833, 0.03075, 0.03708, 0.03190, 0.03523, 0.03152, 0.03078, 0.04453],
          [0.02844, 0.02725, 0.02574, 0.02675, 0.03177, 0.03295, 0.03127, 0.03241, 0.03284, 0.03114, 0.03371, 0.03762, 0.03372, 0.03029, 0.02914, 0.03036, 0.03334, 0.03220, 0.04024, 0.03553, 0.03192, 0.03045, 0.03419, 0.02959, 0.03173, 0.03535, 0.03278, 0.03393, 0.03191, 0.03115, 0.04028],
          [0.02898, 0.02921, 0.02808, 0.02886, 0.03171, 0.03260, 0.02991, 0.03039, 0.03080, 0.02818, 0.03149, 0.04116, 0.03177, 0.02778, 0.02696, 0.02858, 0.03258, 0.03155, 0.04136, 0.03564, 0.03159, 0.03020, 0.03454, 0.02927, 0.03201, 0.03796, 0.03315, 0.03601, 0.03259, 0.03210, 0.04298],
          [0.02920, 0.03038, 0.03003, 0.03059, 0.03212, 0.03261, 0.02927, 0.02875, 0.02856, 0.02632, 0.02935, 0.04167, 0.03110, 0.02747, 0.02549, 0.02749, 0.03190, 0.03091, 0.04279, 0.03591, 0.03105, 0.03024, 0.03529, 0.02859, 0.03200, 0.03925, 0.03355, 0.03679, 0.03298, 0.03244, 0.04593],
          [0.02874, 0.03105, 0.03056, 0.03055, 0.03169, 0.03208, 0.02875, 0.02910, 0.02884, 0.02558, 0.02906, 0.04105, 0.03096, 0.02632, 0.02616, 0.02702, 0.03214, 0.03121, 0.04223, 0.03644, 0.03128, 0.03029, 0.03543, 0.02896, 0.03222, 0.03941, 0.03399, 0.03750, 0.03335, 0.03276, 0.04532],
          [0.02966, 0.03045, 0.03002, 0.03010, 0.03122, 0.03197, 0.02883, 0.02877, 0.02960, 0.02745, 0.02958, 0.04003, 0.03103, 0.02679, 0.02571, 0.02808, 0.03215, 0.03129, 0.04129, 0.03598, 0.03166, 0.03062, 0.03532, 0.02931, 0.03239, 0.03848, 0.03386, 0.03687, 0.03323, 0.03244, 0.04582],
          [0.03066, 0.02832, 0.02801, 0.02768, 0.02996, 0.03070, 0.02906, 0.02918, 0.02975, 0.02764, 0.03003, 0.03977, 0.03174, 0.02786, 0.02753, 0.02908, 0.03255, 0.03227, 0.04118, 0.03641, 0.03275, 0.03134, 0.03529, 0.03031, 0.03309, 0.03738, 0.03393, 0.03635, 0.03389, 0.03294, 0.04334],
          [0.03106, 0.02825, 0.02768, 0.02744, 0.02984, 0.03021, 0.02855, 0.02837, 0.02921, 0.02805, 0.03018, 0.03880, 0.03170, 0.02831, 0.02780, 0.02884, 0.03304, 0.03200, 0.04118, 0.03629, 0.03257, 0.03139, 0.03534, 0.03011, 0.03310, 0.03814, 0.03408, 0.03692, 0.03394, 0.03292, 0.04471],
          [0.03092, 0.02534, 0.02447, 0.02379, 0.02851, 0.02920, 0.02839, 0.02935, 0.03034, 0.02954, 0.03174, 0.03830, 0.03329, 0.03062, 0.03033, 0.03133, 0.03420, 0.03356, 0.03856, 0.03607, 0.03352, 0.03229, 0.03487, 0.03147, 0.03365, 0.03625, 0.03421, 0.03608, 0.03383, 0.03292, 0.04306],
          [0.03158, 0.02690, 0.02581, 0.02579, 0.02904, 0.02926, 0.02801, 0.02858, 0.02941, 0.02828, 0.03083, 0.03806, 0.03226, 0.02926, 0.02917, 0.03032, 0.03346, 0.03300, 0.03893, 0.03543, 0.03343, 0.03214, 0.03540, 0.03140, 0.03364, 0.03713, 0.03442, 0.03684, 0.03454, 0.03341, 0.04428],
          [0.03226, 0.02823, 0.02707, 0.02624, 0.02854, 0.02911, 0.02730, 0.02752, 0.02885, 0.02768, 0.02983, 0.03797, 0.03144, 0.02810, 0.02810, 0.02916, 0.03304, 0.03237, 0.03994, 0.03643, 0.03308, 0.03188, 0.03552, 0.03086, 0.03371, 0.03805, 0.03497, 0.03789, 0.03467, 0.03378, 0.04643],
          [0.03202, 0.02764, 0.02675, 0.02579, 0.02866, 0.02902, 0.02739, 0.02796, 0.02871, 0.02742, 0.02988, 0.03755, 0.03148, 0.02809, 0.02823, 0.02911, 0.03299, 0.03244, 0.03980, 0.03635, 0.03312, 0.03212, 0.03570, 0.03151, 0.03414, 0.03793, 0.03491, 0.03833, 0.03475, 0.03405, 0.04617],
          [0.03245, 0.02692, 0.02577, 0.02507, 0.02848, 0.02889, 0.02760, 0.02835, 0.02965, 0.02793, 0.03027, 0.03722, 0.03181, 0.02942, 0.02928, 0.03027, 0.03314, 0.03303, 0.03912, 0.03646, 0.03344, 0.03239, 0.03488, 0.03168, 0.03415, 0.03732, 0.03469, 0.03735, 0.03488, 0.03398, 0.04411],
          [0.03215, 0.02901, 0.02763, 0.02693, 0.02894, 0.02933, 0.02740, 0.02763, 0.02854, 0.02722, 0.02931, 0.03738, 0.03112, 0.02815, 0.02771, 0.02893, 0.03280, 0.03202, 0.03934, 0.03605, 0.03301, 0.03211, 0.03552, 0.03173, 0.03401, 0.03776, 0.03497, 0.03806, 0.03514, 0.03418, 0.04593],
          [0.03186, 0.02875, 0.02717, 0.02631, 0.02904, 0.02985, 0.02808, 0.02911, 0.03020, 0.02809, 0.03053, 0.03739, 0.03168, 0.02872, 0.02869, 0.02951, 0.03287, 0.03215, 0.03926, 0.03575, 0.03289, 0.03200, 0.03479, 0.03138, 0.03396, 0.03693, 0.03464, 0.03705, 0.03478, 0.03399, 0.04259],
          [0.03201, 0.02543, 0.02394, 0.02349, 0.02901, 0.02967, 0.02913, 0.03026, 0.03126, 0.03013, 0.03187, 0.03716, 0.03298, 0.03132, 0.03090, 0.03147, 0.03341, 0.03373, 0.03804, 0.03585, 0.03344, 0.03248, 0.03454, 0.03220, 0.03400, 0.03436, 0.03429, 0.03577, 0.03465, 0.03354, 0.03967],
          [0.03223, 0.02810, 0.02688, 0.02664, 0.02956, 0.03023, 0.02909, 0.03004, 0.03078, 0.02894, 0.03056, 0.03698, 0.03168, 0.02947, 0.02910, 0.02965, 0.03264, 0.03231, 0.03872, 0.03557, 0.03289, 0.03194, 0.03457, 0.03160, 0.03374, 0.03651, 0.03408, 0.03621, 0.03454, 0.03371, 0.04105],
          [0.03185, 0.02688, 0.02557, 0.02575, 0.02996, 0.03045, 0.02986, 0.03083, 0.03132, 0.02964, 0.03114, 0.03670, 0.03209, 0.03024, 0.02985, 0.03053, 0.03302, 0.03260, 0.03807, 0.03554, 0.03313, 0.03206, 0.03436, 0.03185, 0.03399, 0.03545, 0.03389, 0.03551, 0.03415, 0.03355, 0.04018],
          [0.03132, 0.02941, 0.02846, 0.02813, 0.03104, 0.03174, 0.03059, 0.03108, 0.03130, 0.02868, 0.03034, 0.03671, 0.03125, 0.02887, 0.02840, 0.02934, 0.03204, 0.03161, 0.03759, 0.03462, 0.03245, 0.03153, 0.03400, 0.03119, 0.03350, 0.03620, 0.03394, 0.03624, 0.03451, 0.03357, 0.04036],
          [0.03120, 0.02907, 0.02825, 0.02787, 0.03129, 0.03193, 0.03073, 0.03134, 0.03162, 0.02911, 0.03045, 0.03674, 0.03139, 0.02909, 0.02864, 0.02938, 0.03207, 0.03168, 0.03727, 0.03487, 0.03239, 0.03139, 0.03412, 0.03123, 0.03341, 0.03548, 0.03375, 0.03548, 0.03419, 0.03412, 0.04046],
          [0.03158, 0.02547, 0.02509, 0.02550, 0.03115, 0.03157, 0.03158, 0.03239, 0.03289, 0.03204, 0.03287, 0.03719, 0.03321, 0.03254, 0.03175, 0.03230, 0.03308, 0.03316, 0.03487, 0.03437, 0.03283, 0.03153, 0.03277, 0.03166, 0.03359, 0.03272, 0.03277, 0.03380, 0.03339, 0.03275, 0.03760]]])

  val = torch.Tensor([[[     1.95468,     -1.42115,      1.12285,      0.90912,     -0.65178,     -0.65429,      0.84439,     -0.62072,      1.20268,     -1.00766,      0.90130,     -0.12892,      0.66431,      0.58879,      1.52981,     -0.62564,     -0.18528,     -1.98438,     -0.23588,      0.32179,     -0.80556,     -0.08875,     -0.59964,      1.05083,      0.02545,     -0.23813,     -1.55175,      1.51661,      0.01353,      0.77854,      1.79397,      0.40769],
          [     0.68412,      1.14571,      0.49391,     -1.38580,     -0.19754,      0.42672,      0.81738,      1.89740,      0.49337,     -0.11100,     -0.37519,     -0.76736,     -0.94078,     -0.56643,     -0.94885,     -0.33263,      0.97746,     -0.51412,     -1.60809,      0.54492,     -0.15513,     -0.50873,     -0.46978,     -1.06346,     -0.44293,      0.14029,      0.60202,      0.20840,      0.33800,     -0.07569,     -0.67389,      1.06104]],

         [[   -25.48104,     14.63728,    -19.82492,    -12.03147,     13.72647,     10.99798,    -10.20726,      9.14671,    -16.29673,      5.81981,    -14.05105,      3.35893,     -9.88140,     -7.75727,    -18.65454,     11.65749,      3.70105,     25.41901,     -4.05584,     -3.22665,      6.82577,      2.39421,      6.85947,    -10.92322,     -2.81246,      1.52946,     14.51780,    -20.92754,     -0.26074,     -7.85903,    -21.96344,     -0.60539],
          [   -22.30858,    -21.19254,    -17.70695,     19.60217,      3.39381,     -0.78414,     -9.01045,    -27.64128,     -9.23708,      8.92418,     -0.11068,     10.10802,     21.48689,     -2.64038,      5.80840,     -1.78248,    -12.53534,     14.89740,     26.06103,     -4.70056,      2.74177,     15.05915,     -0.87563,     23.79338,     13.56439,    -10.22571,    -14.82178,     -0.35493,     -2.09404,      0.75223,     11.34956,    -16.01933]],

         [[   -14.75201,      8.60140,    -11.33826,     -7.05498,      7.80833,      6.24426,     -5.73103,      5.31467,     -9.40508,      3.52236,     -7.85955,      2.17009,     -5.39201,     -4.79941,    -10.62060,      6.65456,      1.71582,     14.78562,     -2.39611,     -1.78282,      4.10525,      1.22492,      4.11883,     -6.33076,     -1.83010,      1.03352,      8.16361,    -12.24429,     -0.22960,     -4.77478,    -12.76966,     -0.41954],
          [   -12.82052,    -12.34174,     -9.99101,     10.97217,      1.81978,     -0.31296,     -5.57466,    -15.64792,     -4.84547,      5.15898,      0.15209,      5.99097,     12.27767,     -1.48001,      3.49706,     -1.20688,     -7.31846,      8.28557,     14.62595,     -3.04784,      1.23346,      8.40943,     -0.16937,     13.67209,      7.94373,     -5.62583,     -8.08940,     -0.45099,     -0.92599,      0.43877,      6.13264,     -9.43164]],

         [[    -9.17098,      5.45635,     -6.92614,     -4.46235,      4.73443,      3.77449,     -3.40614,      3.32128,     -5.81988,      2.31763,     -4.64701,      1.54827,     -3.06520,     -3.24940,     -6.44556,      4.05290,      0.69456,      9.25125,     -1.53297,     -1.03466,      2.68435,      0.62044,      2.68663,     -3.94170,     -1.31369,      0.77118,      4.86372,     -7.72539,     -0.21007,     -3.16242,     -7.98195,     -0.31888],
          [    -7.89203,     -7.73747,     -5.98658,      6.49404,      1.00507,     -0.07149,     -3.77417,     -9.41776,     -2.57587,      3.20487,      0.28089,      3.84645,      7.49490,     -0.88036,      2.28887,     -0.90459,     -4.60197,      4.85737,      8.69178,     -2.17796,      0.46088,      4.96018,      0.18600,      8.41262,      5.01987,     -3.24541,     -4.60318,     -0.49175,     -0.32637,      0.27491,      3.43202,     -5.99698]],

         [[    -0.55465,      0.58568,     -0.14166,     -0.45020,      0.02607,     -0.01555,      0.14216,      0.23685,     -0.28161,      0.42259,      0.25846,      0.54274,      0.47349,     -0.81386,     -0.02302,      0.05340,     -0.81087,      0.70485,     -0.19382,      0.09147,      0.46241,     -0.28518,      0.45027,     -0.25487,     -0.46988,      0.34635,     -0.19637,     -0.73094,     -0.17668,     -0.63655,     -0.58631,     -0.15621],
          [    -0.29638,     -0.61254,      0.15291,     -0.35029,     -0.22497,      0.26534,     -0.94413,      0.14116,      0.84666,      0.18591,      0.44665,      0.51485,      0.13637,      0.04864,      0.40226,     -0.40309,     -0.39452,     -0.38373,     -0.37742,     -0.77828,     -0.67198,     -0.31636,      0.67559,      0.30742,      0.48782,      0.37380,      0.68249,     -0.50558,      0.55382,      0.03764,     -0.65375,     -0.67532]],

         [[    -0.18906,      0.33215,      0.07545,     -0.25241,     -0.07839,     -0.11686,      0.19397,      0.09059,     -0.04045,      0.24474,      0.32909,      0.38956,      0.48652,     -0.59723,      0.18994,     -0.07248,     -0.69175,      0.33209,     -0.12656,      0.06988,      0.29234,     -0.25391,      0.28948,     -0.10023,     -0.32172,      0.27462,     -0.32703,     -0.39031,     -0.16258,     -0.43126,     -0.25553,     -0.12361],
          [    -0.02279,     -0.27797,      0.29402,     -0.46251,     -0.20543,      0.19523,     -0.68742,      0.39359,      0.78138,      0.06280,      0.36219,      0.32132,     -0.09982,      0.08308,      0.26031,     -0.30217,     -0.18251,     -0.46621,     -0.52778,     -0.57509,     -0.56479,     -0.40878,      0.53888,      0.01059,      0.25960,      0.37696,      0.65760,     -0.38065,      0.47663,      0.06141,     -0.61344,     -0.39551]],

         [[    -0.07331,      0.34149,      0.27748,     -0.24003,     -0.28907,     -0.26026,      0.39117,      0.07242,      0.02618,      0.37178,      0.60499,      0.54525,      0.74495,     -0.73809,      0.36743,     -0.19647,     -0.99411,      0.23309,     -0.12105,      0.19076,      0.38083,     -0.37340,      0.35901,     -0.04896,     -0.48057,      0.35259,     -0.52242,     -0.36445,     -0.18202,     -0.54948,     -0.18152,     -0.16404],
          [     0.16124,     -0.22686,      0.56552,     -0.83080,     -0.33273,      0.32683,     -0.86102,      0.75786,      1.15223,      0.01301,      0.50931,      0.35727,     -0.32041,      0.10948,      0.33419,     -0.41420,     -0.17676,     -0.75638,     -1.01142,     -0.77746,     -0.81756,     -0.68595,      0.79127,     -0.17625,      0.24885,      0.66062,      1.11421,     -0.57451,      0.66483,      0.00780,     -0.99715,     -0.40784]],

         [[     0.45414,     -0.05160,      0.58211,      0.04004,     -0.45281,     -0.41154,      0.52420,     -0.12289,      0.35059,      0.09463,      0.74464,      0.37469,      0.79474,     -0.43051,      0.68694,     -0.36620,     -0.85075,     -0.32206,     -0.07515,      0.19612,      0.12959,     -0.34555,      0.14308,      0.19232,     -0.32649,      0.24462,     -0.75019,      0.12039,     -0.14244,     -0.25366,      0.30824,     -0.07948],
          [     0.46156,      0.18958,      0.71651,     -1.00685,     -0.31805,      0.28834,     -0.48082,      1.14564,      1.07599,     -0.11750,      0.35476,      0.08597,     -0.60652,      0.06843,      0.08446,     -0.34367,      0.12161,     -0.83906,     -1.26234,     -0.50582,     -0.69444,     -0.78370,      0.57755,     -0.53722,      0.00059,      0.64096,      1.09951,     -0.40932,      0.57966,      0.00241,     -0.98128,     -0.00044]],

         [[     0.44413,     -0.01276,      0.59839,      0.03485,     -0.45649,     -0.41496,      0.48912,     -0.13067,      0.36451,      0.13422,      0.73589,      0.35363,      0.79155,     -0.45352,      0.68164,     -0.38120,     -0.86208,     -0.29382,     -0.03274,      0.17618,      0.15261,     -0.34380,      0.14621,      0.17153,     -0.29835,      0.25985,     -0.72714,      0.10905,     -0.16303,     -0.27610,      0.28459,     -0.11760],
          [     0.54738,      0.23203,      0.78232,     -1.02331,     -0.31931,      0.24908,     -0.52151,      1.14689,      1.10124,     -0.15838,      0.40192,      0.09424,     -0.66243,      0.14955,      0.14000,     -0.29334,      0.11525,     -0.89485,     -1.27160,     -0.51926,     -0.69815,     -0.83819,      0.62415,     -0.59756,     -0.05932,      0.68971,      1.12720,     -0.42198,      0.57797,      0.03083,     -0.98363,     -0.01892]],

         [[     0.52004,     -0.03658,      0.69556,      0.05118,     -0.55871,     -0.48573,      0.60001,     -0.14246,      0.40040,      0.16954,      0.87308,      0.42922,      0.91583,     -0.50072,      0.77758,     -0.43821,     -0.99462,     -0.36991,     -0.04622,      0.24118,      0.17675,     -0.39944,      0.17075,      0.21228,     -0.37891,      0.28703,     -0.83575,      0.14217,     -0.16274,     -0.31323,      0.34423,     -0.11881],
          [     0.60894,      0.25314,      0.88754,     -1.19515,     -0.37809,      0.32561,     -0.57456,      1.32976,      1.26228,     -0.16984,      0.44695,      0.09824,     -0.75307,      0.12764,      0.14371,     -0.36413,      0.13075,     -1.01106,     -1.50250,     -0.59953,     -0.81164,     -0.94886,      0.71686,     -0.67273,     -0.04808,      0.80263,      1.32937,     -0.50250,      0.66465,     -0.00557,     -1.16229,     -0.00517]],

         [[     0.66837,     -0.14693,      0.76385,      0.14509,     -0.56253,     -0.50339,      0.56243,     -0.21404,      0.51101,      0.06847,      0.83832,      0.30711,      0.85892,     -0.36948,      0.83798,     -0.47921,     -0.86874,     -0.51793,      0.00615,      0.19245,      0.08198,     -0.35495,      0.07524,      0.26643,     -0.25497,      0.23957,     -0.84289,      0.29838,     -0.16015,     -0.19312,      0.48081,     -0.11023],
          [     0.74039,      0.42558,      0.92398,     -1.17078,     -0.33851,      0.24110,     -0.42400,      1.36846,      1.15350,     -0.23693,      0.39203,     -0.00130,     -0.84283,      0.17643,      0.08218,     -0.26381,      0.23082,     -1.00886,     -1.46666,     -0.45748,     -0.69898,     -0.95462,      0.61054,     -0.80330,     -0.18674,      0.75899,      1.22162,     -0.40032,      0.58034,      0.03149,     -1.05164,      0.12953]],

         [[     1.10867,     -0.57386,      0.85682,      0.45237,     -0.46131,     -0.48194,      0.39577,     -0.42572,      0.81471,     -0.38952,      0.59900,     -0.13694,      0.54657,      0.16586,      0.95300,     -0.52426,     -0.28599,     -0.99435,      0.10547,      0.00369,     -0.30929,     -0.15158,     -0.27530,      0.45003,      0.18456,      0.02056,     -0.80364,      0.81286,     -0.10940,      0.29466,      0.92475,      0.00001],
          [     0.93633,      0.89231,      0.79790,     -0.87624,     -0.14232,     -0.03853,      0.21998,      1.30691,      0.56933,     -0.36552,      0.06135,     -0.35760,     -0.93250,      0.19109,     -0.24561,      0.03936,      0.56373,     -0.76334,     -1.08775,      0.13023,     -0.19780,     -0.74411,      0.05983,     -1.03569,     -0.53639,      0.39333,      0.59249,      0.05215,      0.21173,      0.13421,     -0.49148,      0.60449]],

         [[     0.90995,     -0.37530,      0.84137,      0.29280,     -0.56686,     -0.52804,      0.57040,     -0.30768,      0.65367,     -0.14144,      0.80770,      0.16488,      0.78803,     -0.14100,      0.94192,     -0.51829,     -0.67177,     -0.78939,      0.01206,      0.15806,     -0.09392,     -0.29503,     -0.07015,      0.38465,     -0.12226,      0.14668,     -0.89937,      0.55047,     -0.12231,      0.01834,      0.72544,     -0.03642],
          [     0.79847,      0.61438,      0.87529,     -1.11940,     -0.28132,      0.18657,     -0.13882,      1.43326,      0.96090,     -0.26881,      0.23775,     -0.16289,     -0.89040,      0.11398,     -0.10032,     -0.20359,      0.39148,     -0.92198,     -1.41269,     -0.23049,     -0.53338,     -0.87945,      0.38352,     -0.90351,     -0.29251,      0.62265,      1.02921,     -0.23493,      0.46680,      0.03821,     -0.89529,      0.35596]],

         [[     0.84708,     -0.23793,      0.93095,      0.21108,     -0.70964,     -0.61190,      0.70972,     -0.26306,      0.61041,      0.06605,      1.02113,      0.36152,      1.01104,     -0.37463,      1.00445,     -0.57525,     -0.99856,     -0.69846,      0.00346,      0.26562,      0.06871,     -0.41402,      0.06766,      0.35308,     -0.31773,      0.25416,     -1.00543,      0.42080,     -0.15631,     -0.18757,      0.63057,     -0.10148],
          [     0.87436,      0.52755,      1.07981,     -1.40156,     -0.40414,      0.31775,     -0.42488,      1.64842,      1.32968,     -0.27965,      0.42219,     -0.04359,     -1.00464,      0.15363,      0.04935,     -0.32700,      0.29990,     -1.16413,     -1.77807,     -0.50427,     -0.81050,     -1.10616,      0.68590,     -0.96006,     -0.22416,      0.89200,      1.45577,     -0.46577,      0.66486,     -0.00751,     -1.25598,      0.21387]],

         [[     0.77504,     -0.14972,      0.93281,      0.15880,     -0.73941,     -0.62477,      0.73360,     -0.23120,      0.56895,      0.16680,      1.07153,      0.43650,      1.07634,     -0.48084,      0.99207,     -0.58181,     -1.11761,     -0.61290,      0.00468,      0.29639,      0.14927,     -0.45418,      0.13047,      0.31660,     -0.38876,      0.30159,     -1.01339,      0.32987,     -0.17358,     -0.28527,      0.55166,     -0.13839],
          [     0.88713,      0.46986,      1.14310,     -1.47886,     -0.44449,      0.35634,     -0.55916,      1.67797,      1.45858,     -0.27576,      0.50653,      0.02232,     -1.02457,      0.18647,      0.13191,     -0.36376,      0.24173,     -1.24492,     -1.87254,     -0.62032,     -0.90898,     -1.17923,      0.81222,     -0.95685,     -0.19078,      0.98962,      1.59853,     -0.55856,      0.73778,     -0.01630,     -1.37195,      0.12497]],

         [[     0.89740,     -0.23498,      1.00628,      0.22532,     -0.77525,     -0.65909,      0.75209,     -0.27993,      0.64973,      0.10730,      1.09643,      0.38645,      1.08176,     -0.40882,      1.06384,     -0.62453,     -1.07906,     -0.73796,      0.02683,      0.29001,      0.09264,     -0.44439,      0.07699,      0.36846,     -0.34060,      0.27768,     -1.05817,      0.44324,     -0.16861,     -0.21685,      0.66297,     -0.12709],
          [     0.97819,      0.58073,      1.19083,     -1.51785,     -0.43838,      0.33400,     -0.47371,      1.76360,      1.43843,     -0.31621,      0.47857,     -0.04172,     -1.10341,      0.19728,      0.08451,     -0.33117,      0.31216,     -1.27323,     -1.92509,     -0.55378,     -0.87404,     -1.21097,      0.76724,     -1.05605,     -0.26634,      0.99140,      1.59222,     -0.51784,      0.71292,     -0.00913,     -1.36007,      0.21888]],

         [[     1.17560,     -0.49182,      1.08371,      0.41578,     -0.73151,     -0.65825,      0.66128,     -0.41244,      0.84422,     -0.16005,      0.97087,      0.12421,      0.91003,     -0.09282,      1.14794,     -0.66556,     -0.74689,     -1.03515,      0.09478,      0.18099,     -0.13781,     -0.32921,     -0.13329,      0.48202,     -0.07977,      0.15097,     -1.04610,      0.75932,     -0.14115,      0.07110,      0.94012,     -0.06707],
          [     1.12487,      0.87678,      1.14488,     -1.36885,     -0.32837,      0.16711,     -0.09607,      1.75585,      1.11269,     -0.40397,      0.29326,     -0.25583,     -1.18333,      0.21952,     -0.10310,     -0.14671,      0.51600,     -1.15322,     -1.73304,     -0.21070,     -0.58587,     -1.11199,      0.45508,     -1.22187,     -0.49056,      0.79559,      1.24654,     -0.25656,      0.50032,      0.05266,     -1.04689,      0.50968]],

         [[     1.17587,     -0.50391,      1.07466,      0.41227,     -0.73374,     -0.65909,      0.68499,     -0.40498,      0.83255,     -0.16757,      0.98321,      0.14459,      0.92231,     -0.09451,      1.15199,     -0.65842,     -0.75674,     -1.04377,      0.07024,      0.19750,     -0.14057,     -0.33552,     -0.12710,      0.49161,     -0.10467,      0.14905,     -1.06375,      0.75680,     -0.13211,      0.07098,      0.94506,     -0.05141],
          [     1.08131,      0.84891,      1.11735,     -1.37322,     -0.33323,      0.19584,     -0.09143,      1.76147,      1.11727,     -0.38132,      0.27900,     -0.25386,     -1.15790,      0.17805,     -0.12483,     -0.18107,      0.51409,     -1.13507,     -1.74405,     -0.21874,     -0.59825,     -1.09205,      0.44660,     -1.19138,     -0.45338,      0.78361,      1.25193,     -0.26339,      0.51401,      0.03621,     -1.06287,      0.50653]],

         [[     1.65391,     -0.96879,      1.17288,      0.75517,     -0.60940,     -0.62690,      0.47525,     -0.64139,      1.17182,     -0.67675,      0.69497,     -0.36512,      0.55507,      0.50762,      1.26531,     -0.70809,     -0.09041,     -1.55923,      0.19547,     -0.02715,     -0.57540,     -0.10112,     -0.52369,      0.68471,      0.40396,     -0.09641,     -0.99810,      1.32210,     -0.07904,      0.61686,      1.43073,      0.06337],
          [     1.31346,      1.37714,      0.98046,     -1.02547,     -0.10641,     -0.13748,      0.62778,      1.66830,      0.44895,     -0.53118,     -0.08453,     -0.64825,     -1.25870,      0.21869,     -0.47628,      0.18072,      0.88149,     -0.85932,     -1.29292,      0.44504,     -0.02096,     -0.85704,     -0.16984,     -1.45611,     -0.86169,      0.37124,      0.52958,      0.25048,      0.08857,      0.16134,     -0.41266,      1.03503]],

         [[     1.45205,     -0.74679,      1.16460,      0.60061,     -0.69970,     -0.66453,      0.59217,     -0.53899,      1.03184,     -0.41743,      0.86650,     -0.11496,      0.76028,      0.20772,      1.23980,     -0.70865,     -0.44188,     -1.33318,      0.15067,      0.08788,     -0.35910,     -0.22545,     -0.33245,      0.59924,      0.15595,      0.03044,     -1.05136,      1.06685,     -0.11121,      0.34588,      1.21607,     -0.00394],
          [     1.25775,      1.15559,      1.10115,     -1.24336,     -0.22926,      0.02341,      0.26546,      1.76746,      0.81507,     -0.48143,      0.11408,     -0.46154,     -1.26058,      0.22473,     -0.29148,      0.01266,      0.71392,     -1.04252,     -1.57386,      0.11003,     -0.32220,     -1.02015,      0.15964,     -1.37935,     -0.69344,      0.61339,      0.93405,     -0.01463,      0.30861,      0.10316,     -0.76685,      0.79056]],

         [[     1.21922,     -0.50529,      1.13448,      0.42989,     -0.77690,     -0.69092,      0.70079,     -0.42488,      0.87123,     -0.14752,      1.02641,      0.14339,      0.95843,     -0.10583,      1.19376,     -0.69589,     -0.79545,     -1.07575,      0.10014,      0.20163,     -0.13190,     -0.34929,     -0.12999,      0.50070,     -0.09982,      0.16101,     -1.08979,      0.78517,     -0.14418,      0.06174,      0.97345,     -0.07256],
          [     1.17581,      0.90671,      1.20388,     -1.44336,     -0.35073,      0.18665,     -0.11256,      1.83884,      1.17710,     -0.42141,      0.31504,     -0.26115,     -1.23870,      0.22679,     -0.09801,     -0.16076,      0.52997,     -1.21178,     -1.83119,     -0.23536,     -0.62620,     -1.16897,      0.49324,     -1.27444,     -0.50677,      0.84808,      1.32910,     -0.28433,      0.52973,      0.04442,     -1.11519,      0.52478]],

         [[     1.34001,     -0.67286,      1.11997,      0.50198,     -0.74871,     -0.68177,      0.73627,     -0.45694,      0.90974,     -0.31513,      0.99670,      0.08726,      0.90424,      0.04540,      1.23498,     -0.67377,     -0.65374,     -1.23706,      0.03537,      0.20311,     -0.25915,     -0.30982,     -0.20672,      0.58575,     -0.06225,      0.08733,     -1.13500,      0.92323,     -0.09614,      0.20598,      1.11215,      0.02216],
          [     1.05622,      0.92922,      1.05123,     -1.35651,     -0.30807,      0.20803,      0.09680,      1.82997,      1.01078,     -0.37412,      0.15595,     -0.35765,     -1.15692,      0.07103,     -0.27815,     -0.19818,      0.61654,     -1.05723,     -1.74144,     -0.08707,     -0.52107,     -1.02082,      0.29016,     -1.21317,     -0.46425,      0.68307,      1.14816,     -0.16904,      0.46286,      0.01352,     -0.99925,      0.65331]],

         [[     1.51162,     -0.78308,      1.21029,      0.62983,     -0.72858,     -0.68796,      0.61247,     -0.56181,      1.07069,     -0.43843,      0.89522,     -0.12773,      0.77689,      0.23148,      1.28160,     -0.73354,     -0.44227,     -1.39191,      0.16137,      0.09054,     -0.37967,     -0.22790,     -0.35090,      0.62371,      0.16828,      0.02400,     -1.08014,      1.11919,     -0.11083,      0.36957,      1.26752,     -0.00003],
          [     1.30716,      1.20594,      1.13773,     -1.27927,     -0.23422,      0.01930,      0.29405,      1.82788,      0.82631,     -0.50437,      0.10763,     -0.48796,     -1.30710,      0.23056,     -0.30839,      0.02207,      0.74480,     -1.06962,     -1.62270,      0.12971,     -0.32174,     -1.04982,      0.15249,     -1.43135,     -0.72730,      0.62834,      0.95584,     -0.00501,      0.30441,      0.10304,     -0.78320,      0.83459]],

         [[     1.20219,     -0.50604,      1.12840,      0.39811,     -0.81429,     -0.71045,      0.79842,     -0.39395,      0.82565,     -0.12523,      1.11024,      0.24403,      1.04008,     -0.16410,      1.21731,     -0.68376,     -0.89470,     -1.07329,      0.02925,      0.26860,     -0.10445,     -0.39242,     -0.07838,      0.51765,     -0.21265,      0.17903,     -1.15788,      0.74989,     -0.12824,      0.01535,      0.95928,     -0.04095],
          [     1.06536,      0.80465,      1.16886,     -1.51099,     -0.39160,      0.29315,     -0.16284,      1.90229,      1.26816,     -0.36444,      0.31061,     -0.22967,     -1.18888,      0.11738,     -0.12663,     -0.28228,      0.50249,     -1.20861,     -1.93558,     -0.31508,     -0.72367,     -1.16024,      0.53254,     -1.19418,     -0.38523,      0.86897,      1.43317,     -0.34948,      0.60472,     -0.01150,     -1.23768,      0.48752]],

         [[     1.33746,     -0.59175,      1.20148,      0.49617,     -0.80609,     -0.72046,      0.71410,     -0.47268,      0.94922,     -0.21300,      1.04294,      0.08935,      0.95529,     -0.02825,      1.25962,     -0.73414,     -0.74678,     -1.19774,      0.12112,      0.19159,     -0.19216,     -0.33574,     -0.18573,      0.55093,     -0.04742,      0.13397,     -1.12811,      0.89774,     -0.13793,      0.13478,      1.08279,     -0.05841],
          [     1.25776,      1.01393,      1.24083,     -1.46923,     -0.34054,      0.16077,     -0.01988,      1.91278,      1.14382,     -0.45913,      0.28034,     -0.32629,     -1.30814,      0.23424,     -0.14930,     -0.12568,      0.60006,     -1.22842,     -1.86719,     -0.16151,     -0.58275,     -1.19005,      0.43827,     -1.36492,     -0.58009,      0.83853,      1.30680,     -0.23626,      0.49773,      0.05221,     -1.09058,      0.62072]],

         [[     2.18828,     -1.55889,      1.25555,      1.07834,     -0.56437,     -0.64300,      0.53328,     -0.82808,      1.43671,     -1.24318,      0.59702,     -0.66795,      0.35935,      1.07260,      1.47549,     -0.72349,      0.42497,     -2.19669,      0.10461,     -0.08100,     -1.03073,      0.05176,     -0.85175,      0.98574,      0.66135,     -0.34733,     -1.14399,      1.90290,      0.04727,      1.15003,      1.99287,      0.31721],
          [     1.21045,      1.68477,      0.66656,     -0.80317,      0.04771,     -0.19040,      1.36717,      1.73754,     -0.09826,     -0.50901,     -0.55950,     -1.03441,     -1.19666,     -0.11076,     -1.02096,      0.21660,      1.24713,     -0.48613,     -1.05946,      1.00970,      0.38721,     -0.51168,     -0.82121,     -1.50261,     -0.95133,     -0.08927,     -0.04516,      0.67497,     -0.18704,      0.12810,      0.00668,      1.56113]],

         [[     1.46236,     -0.72210,      1.22208,      0.58280,     -0.77565,     -0.71243,      0.67672,     -0.52985,      1.02926,     -0.35484,      0.97773,     -0.03001,      0.86650,      0.12917,      1.29216,     -0.74043,     -0.57807,     -1.33727,      0.13680,      0.14260,     -0.30954,     -0.27827,     -0.28438,      0.60728,      0.06891,      0.06747,     -1.12105,      1.04586,     -0.11822,      0.27886,      1.21187,     -0.01544],
          [     1.28555,      1.13118,      1.18421,     -1.37758,     -0.28455,      0.09173,      0.17399,      1.89375,      0.96934,     -0.48472,      0.17116,     -0.42922,     -1.31500,      0.21418,     -0.25817,     -0.05363,      0.69496,     -1.13994,     -1.75328,      0.00992,     -0.44003,     -1.11291,      0.26744,     -1.40951,     -0.66043,      0.71850,      1.11794,     -0.10255,      0.39101,      0.07316,     -0.92994,      0.76104]],

         [[     1.91575,     -1.21091,      1.27980,      0.88648,     -0.67780,     -0.69179,      0.59613,     -0.71819,      1.29610,     -0.85711,      0.77694,     -0.40891,      0.59087,      0.67425,      1.42490,     -0.75813,     -0.01378,     -1.86143,      0.14204,      0.01286,     -0.72417,     -0.09290,     -0.62148,      0.83390,      0.42378,     -0.16669,     -1.15017,      1.56393,     -0.03000,      0.78226,      1.68895,      0.16028],
          [     1.31393,      1.49995,      0.94339,     -1.08910,     -0.09973,     -0.08307,      0.85625,      1.84764,      0.38404,     -0.52592,     -0.22589,     -0.78732,     -1.30431,      0.06572,     -0.68149,      0.12318,      1.02885,     -0.81395,     -1.40402,      0.58251,      0.02981,     -0.81567,     -0.33476,     -1.52769,     -0.87349,      0.29313,      0.48838,      0.33605,      0.06583,      0.11437,     -0.41233,      1.23687]],

         [[     1.44583,     -0.68448,      1.24506,      0.56090,     -0.81548,     -0.73664,      0.71671,     -0.51601,      1.01742,     -0.29204,      1.03661,      0.02742,      0.93252,      0.06195,      1.31037,     -0.75889,     -0.67285,     -1.31521,      0.13228,      0.17709,     -0.26154,     -0.31322,     -0.24661,      0.60082,      0.01080,      0.09895,     -1.15619,      1.00643,     -0.12615,      0.21907,      1.18853,     -0.03550],
          [     1.30758,      1.10716,      1.24224,     -1.46361,     -0.31965,      0.13413,      0.09040,      1.95306,      1.07808,     -0.48240,      0.22827,     -0.39390,     -1.34813,      0.22452,     -0.21535,     -0.09345,      0.66917,     -1.21257,     -1.86319,     -0.07302,     -0.51949,     -1.17802,      0.35745,     -1.42865,     -0.63912,      0.79933,      1.24443,     -0.17440,      0.45580,      0.05925,     -1.03560,      0.71461]],

         [[     1.73066,     -1.04340,      1.24575,      0.73247,     -0.75993,     -0.72255,      0.76193,     -0.60859,      1.13366,     -0.66065,      0.96254,     -0.13715,      0.79475,      0.40707,      1.40825,     -0.72615,     -0.34142,     -1.67134,      0.03617,      0.14824,     -0.54692,     -0.21460,     -0.43015,      0.77779,      0.14351,     -0.06339,     -1.22916,      1.33778,     -0.03786,      0.54649,      1.49745,      0.15031],
          [     1.13242,      1.22061,      0.96313,     -1.26848,     -0.21927,      0.12498,      0.55699,      1.94747,      0.70120,     -0.43200,     -0.10783,     -0.62228,     -1.22568,     -0.04716,     -0.58344,     -0.10912,      0.87450,     -0.90536,     -1.65391,      0.28249,     -0.26664,     -0.89256,     -0.08445,     -1.35746,     -0.62106,      0.45438,      0.83550,      0.10373,      0.27233,      0.02033,     -0.75468,      1.02517]],

         [[     2.53296,     -1.77264,      1.46203,      1.32647,     -0.55761,     -0.67491,      0.32884,     -1.03215,      1.75284,     -1.47679,      0.44465,     -1.05574,      0.14687,      1.41959,      1.58926,     -0.85261,      0.79346,     -2.50729,      0.34576,     -0.28344,     -1.25637,      0.19338,     -1.12020,      1.06414,      1.08216,     -0.46238,     -1.05926,      2.28805,      0.01848,      1.45412,      2.30280,      0.27803],
          [     1.70407,      2.21692,      0.86190,     -0.69483,      0.17138,     -0.53749,      1.73502,      1.78826,     -0.41254,     -0.76647,     -0.64669,     -1.28247,     -1.50591,      0.18865,     -1.07728,      0.62492,      1.49585,     -0.53532,     -0.88763,      1.41028,      0.73925,     -0.59708,     -1.06464,     -1.93332,     -1.45295,     -0.17048,     -0.36312,      0.97702,     -0.48061,      0.28804,      0.37360,      1.92431]]])

  print(torch.einsum('hql,lhc->qhc', [att, val]))

  
  h = 2
  q = 31
  l = 31

  l = 31
  h = 2
  c = 32

  res = torch.zeros((q, h, c))
  print(f'{res.shape=}\n')

  # hql, lhc -> qhc
  for hh in range(h):
    for ll in range(l):
      for qq in range(q):
        for cc in range(c):

          res[qq][hh][cc] += att[hh][qq][ll] * val[ll][hh][cc]

  print(f'\n\n{res=}')

if __name__ == '__main__':
  case_4()
>>>>>>> 9c0d86c28c83f71f1cb2ea0cb2e3aa899ae4e20c
