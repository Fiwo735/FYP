myproject_mul_mul_16s_16s_32_1_1
softmax_latency_ap_fixed_ap_fixed_sa_softmax_config0_s_exbkb
softmax_latency_ap_fixed_ap_fixed_sa_softmax_config0_s_incud
self_attention_qkv_out
self_attention_energy_scaled_red_V
self_attention_attention
myproject_fpext_32ns_64_2_1
myproject_flog_32ns_32ns_32_9_full_dsp_1
myproject_mul_mul_16s_16s_26_1_1
softmax_latency_ap_fixed_ap_fixed_softmax_config0_s_exp_tdEe
myproject_log_table_i
myproject_transformer_0_out
myproject_mlp_dimensions_reduced_V
dense_latency_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_2
p_fill_n_a_ap_fixed_unsigned_long_double_s
dense_latency_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_1
join_equally_ap_fixed_35_15_5_3_0_2ul_48ul_s
transpose_2d_ap_fixed_sa_transpose_config0_s
matmul_ap_fixed_ap_fixed_2ul_8ul_8ul_2ul_s
softmax_latency_ap_fixed_ap_fixed_sa_softmax_config0_s
matmul_ap_fixed_ap_fixed_2ul_2ul_2ul_8ul_s
dense_latency_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_1
self_attention
relu_ap_fixed_ap_fixed_35_15_5_3_0_sigmoid_config0_s
dense_latency_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_s
relu_ap_fixed_ap_fixed_35_15_5_3_0_sigmoid_config1_s
dense_latency_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0
transformer
dense_latency_ap_fixed_ap_fixed_mlp_config_0_0_0_0_0_0
init_log_table_ap_fixed_16_6_5_3_0_softmax_config0_s
softmax_latency_ap_fixed_ap_fixed_softmax_config0_s
myproject
