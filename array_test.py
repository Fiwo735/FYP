import numpy as np
import csv
import torch

torch.set_printoptions(profile='full', sci_mode=False, threshold=2097152)

# x_data = '29.938385009765625 178.55862426757812 -67.51207733154297 193.2311248779297 0.17523084580898285 181.0510711669922 0.1761978417634964 -0.3647479712963104 0.008448980748653412 -0.0 1.4046745300292969 0.000455104949651286 0.0 0.008461219258606434 -0.34938931465148926 6.123234262925839e-17'
# x = np.array([float(el) for el in x_data.split(' ')])

# def read_data():
#   t = []
#   with open('extracted_weights_biases/inp_layer_weight.txt', 'r') as infile:
#     for el in infile:
#       t = np.array([float(el) for el in el.split(',')])
#   return t

# def transform_like_HLS(t):
#   return t.reshape(16, 128).flatten(order='F')

# def revert_HLS_read(t):
#   return t.reshape(16, 128, order='F').flatten(order='C')

# # W = transform_like_HLS(revert_HLS_read(read_data()))
# W = transform_like_HLS(read_data())
# # W = read_data()
# # W = revert_HLS_read(transform_like_HLS(read_data()))



# b_data = '-0.13000548725358962, 0.15321474786081246, 0.12929527674052557, 0.015206950508695275, 0.23447747558185644, 0.09874022281814714, 0.08055443812192893, 0.09708138089159975, -0.07273779311405582, -0.12322893275532082, 0.16638944719546161, -0.0380457062048275, -0.20809643746408354, 0.04636303598221752, -0.20849156361063526, -0.15630856766465812, 0.019939414314105005, -0.16307167763540578, 0.13211308506049915, 0.04850441757396684, -0.09605553168035963, 0.20517953883247553, -0.0408014574744629, -0.22652685286299823, 0.0032850375610422246, -0.22111437002391734, -0.0880627388591199, -0.11984570212060142, 0.15678703319760884, -0.14426147915908177, 0.11157324693706362, 0.1877708760096317, -0.24377023986353905, -0.10699731327490204, -0.1952664140062446, 0.1366991235539754, 0.008239823194571864, 0.0874801104650833, 0.14869336991091753, -0.18322061544820797, -0.1777989928778831, -0.016878166650835174, 0.05496910612179433, 0.20661068647095915, -0.05943587907088381, -0.24288547842567945, -0.22713314446368765, 0.2465288022714794, 0.021385598861928857, -0.08325646337390469, 0.10686191047424619, 0.10676398985397025, -0.1633541553777138, -0.14952053865999454, 0.19915420681385843, 0.19434344422812577, -0.037831224849853795, -0.2045501259373567, -0.1148028831831879, -0.0880642920004664, 0.018541760343543365, 0.07898868376789869, -0.20087723036054073, 0.08877956109138797, -0.07548296276593816, 0.08422281814314962, -0.1711215736333881, -0.016183058909840634, -0.2372349461674604, 0.13729433629661408, 0.1453569999605651, 0.12664026308996867, 0.08795013021367319, -0.06395550856608195, 0.007341727299098417, 0.14988690935009433, 0.02718889977683301, -0.0774961605130109, -0.18716313122256875, -0.2141118183176563, 0.22251814368767156, 0.0965662375998225, -0.18929985167605012, -0.24342734586290096, -0.07715175155848142, 0.002901013650594975, -0.06282506813712062, 0.1854011302119078, 0.05603784210655197, 0.16435496283300896, -0.1116496321861801, 0.23209457393288221, -0.14106338057169604, 0.24824387425689096, -0.19459800833927585, 0.1554442863288982, -0.21786285020126947, 0.08693781040991075, 0.05889466432744567, -0.15008649518130698, -0.09922898795523674, 0.11174678126515006, 0.1950289181500067, 0.14313260797022276, 0.06399799977414075, -0.21927662166126963, 0.10037644803307723, -0.022731347992944115, -0.11744471022481108, -0.14947014731247013, -0.04559797825776891, -0.1809006827788517, 0.11896195525425354, -0.1541681865731511, 0.1988955903280805, -0.07942795574391706, -0.19091740893201584, 0.18847534823639128, 0.06740288058960743, 0.0891185801504262, -0.04352037047977531, 0.008459298617431572, -0.23532159044450404, 0.11748434002225557, 0.12400449475606388, 0.11770784626843835, -0.1929820439264993, -0.0980418463339387'
# b = np.array([float(el) for el in b_data.split(',')])

# print(len(x))
# print(len(W))
# print(len(b))

# x_t = torch.from_numpy(x)
# W_t = torch.from_numpy(W.reshape(128, 16))
# b_t = torch.from_numpy(b)

# print(x_t.shape)
# print(W_t.shape)
# print(b_t.shape)

# y_t = torch.matmul(x_t, torch.transpose(W_t, 0, 1)) + b_t

# print(y_t.shape)



# print(x_t)
# # print(W_t)
# # print(b_t)
# print(y_t)

# data = '''Columns 1 to 9  0.0402  0.8117 -0.0672  0.0466 -0.9090 -0.7285  1.9935  1.6980  1.1884

# Columns 10 to 18  0.6829 -0.7660 -1.4799  0.3449  2.3595 -3.3989  1.5037 -0.0174  0.0446

# Columns 19 to 27 -1.3236  1.0209  0.5254  0.7503  0.1492  0.1413 -0.9472 -0.6878 -0.1498

# Columns 28 to 36  1.0474 -1.2252 -0.4341 -0.2947 -1.1419  0.3985  0.0380  1.0479 -0.3254

# Columns 37 to 45 -0.9603 -0.7849  0.5666  2.0794  0.9537 -0.4600 -1.1351  0.7572 -0.6853

# Columns 46 to 54  0.0904  0.9626  0.6569 -1.9285 -0.0470 -0.7854 -0.3121 -1.8357 -0.4154

# Columns 55 to 63  1.1418 -0.4621 -0.8087 -0.3228  0.1605  0.1636  2.2846  0.0018  0.6899

# Columns 64 to 72  1.5376  0.3815 -1.3340  0.0230 -0.4254 -0.8556 -0.6259 -0.9643 -0.3805

# Columns 73 to 81 -0.3294  1.2386 -0.8283  1.0510  0.1857 -0.0237 -1.2979  0.6695 -0.5101

# Columns 82 to 90 -1.0592  0.3944 -0.5982 -0.8230  0.2975  1.2248  1.0692  0.1132  2.0313

# Columns 91 to 99  0.2631 -0.3638 -0.7028 -1.8671  0.7682 -1.7487 -0.7558 -1.4865 -0.7106

# Columns 100 to 108  1.5116  1.2869  1.0267 -1.4078  0.4175  1.1119 -1.2257  0.3794  0.9028

# Columns 109 to 117 -1.0706 -0.2135  0.4457  0.0810  0.2778  0.1414  0.9020 -0.3339  1.7003

# Columns 118 to 126  0.1839  0.0069  1.0168  0.6317 -0.4956 -0.1546  0.4645  0.3311 -0.8886

# Columns 127 to 128 -0.8108  0.5335'''

# data = data.replace('\n', ' ').replace('  ', ' ').split(' ')
# data = [float(el) for el in data if el != 'Columns' and (len(el) == 6 or len(el) == 7)]
# data = np.array(data)

# def LayerNorm(x):
#   eps = 0.00001
#   mean = data.mean()
#   var = data.var()
#   denom = np.sqrt(var + eps)
#   return (x - mean) / denom

# print(data)
# print(len(data))
# data_norm = list(map(LayerNorm, data))
# print(data_norm)

# x0 = data_norm[0]
# W0 = 0.9975005800275384
# b0 = 0.0026351373616355705

# print(f'{x0} * {W0} + {b0} = {x0 * W0 + b0}')

# eps = 0.00001
# denom = np.sqrt(data.var() + eps)

# x0p = data[0]
# W0p = W0 / denom
# b0p = b0 - (W0 * data.mean()) / denom

# print(f'{x0p} * {W0p} + {b0p} = {x0p * W0p + b0p}')

two_cols_data = ''' Columns 1 to 6  3.9991e-02  8.1095e-01 -6.7665e-02  4.5703e-02 -9.1089e-01 -7.3059e-01
 -6.1523e+00  1.5260e+01 -7.8616e+01  3.5627e+01 -3.8582e+01  5.4523e-01

Columns 7 to 12  1.9954e+00  1.7009e+00  1.1902e+00  6.8370e-01 -7.6744e-01 -1.4813e+00
  5.2707e+01 -3.1169e+01  3.4800e+01 -2.2370e+00  6.3342e+01  3.5254e+01

Columns 13 to 18  3.4362e-01  2.3607e+00 -3.4036e+00  1.5055e+00 -1.8115e-02  4.5108e-02
 -8.1840e+01 -7.3203e+01  6.8378e+01 -4.5159e+01 -8.6566e+00  3.4873e+01

Columns 19 to 24 -1.3254e+00  1.0194e+00  5.2582e-01  7.5094e-01  1.4674e-01  1.4119e-01
  1.0476e+02 -4.3084e+01 -1.9942e+01 -5.7885e+01 -6.2312e+00  5.6122e+01

Columns 25 to 30 -9.4793e-01 -6.8910e-01 -1.5162e-01  1.0495e+00 -1.2273e+00 -4.3411e-01
 -1.9256e+00 -6.8234e+01 -6.3197e+01  5.1908e+01 -2.2754e+01 -8.4253e+01

Columns 31 to 36 -2.9526e-01 -1.1433e+00  3.9852e-01  3.7263e-02  1.0493e+00 -3.2723e-01
 -7.4467e+01  1.9988e+01  7.6060e-01 -5.9744e+01  2.5900e+00 -1.1099e+02

Columns 37 to 42 -9.6225e-01 -7.8604e-01  5.6631e-01  2.0819e+00  9.5608e-01 -4.6015e-01
 -3.2139e+01 -3.1245e+01 -1.2681e+02 -1.3546e+01  4.3933e+01 -8.0395e+00

Columns 43 to 48 -1.1359e+00  7.5822e-01 -6.8692e-01  8.8960e-02  9.6339e-01  6.5711e-01
  2.0699e+01 -5.2676e+01  2.7513e+01  2.8659e+01  3.8386e+01  3.7802e+01

Columns 49 to 54 -1.9308e+00 -4.7181e-02 -7.8657e-01 -3.1313e-01 -1.8381e+00 -4.1529e-01
 -2.0213e+01  2.0026e+01  1.5013e+01  2.5405e+01 -2.2660e+00  4.0137e+01

Columns 55 to 60  1.1442e+00 -4.6309e-01 -8.1051e-01 -3.2279e-01  1.6047e-01  1.6301e-01
  5.1563e+01 -4.6199e+01 -8.6484e+01 -1.9677e+01 -1.1647e+02 -2.5375e+01

Columns 61 to 66  2.2873e+00  1.9724e-03  6.9057e-01  1.5403e+00  3.8256e-01 -1.3344e+00
 -5.1251e+01  5.7303e+01  6.2597e+01 -5.0145e+01  6.0105e+01 -6.7048e+01

Columns 67 to 72  2.3210e-02 -4.2549e-01 -8.5724e-01 -6.2708e-01 -9.6605e-01 -3.8244e-01
  4.6392e+01 -3.5880e+01 -3.8093e+01  9.2044e+01 -4.1169e+01 -2.4093e+00

Columns 73 to 78 -3.2861e-01  1.2402e+00 -8.3001e-01  1.0517e+00  1.8609e-01 -2.3605e-02
  7.3557e+01  1.7257e+01  4.3155e+01 -1.9613e+01  3.9070e+00  1.2794e+02

Columns 79 to 84 -1.2990e+00  6.7165e-01 -5.1183e-01 -1.0611e+00  3.9397e-01 -5.9869e-01
  8.8469e-01 -8.5691e+01 -2.1229e+01  7.7522e+00  2.0085e+01  1.7608e+00

Columns 85 to 90 -8.2245e-01  2.9716e-01  1.2273e+00  1.0695e+00  1.1247e-01  2.0362e+00
 -8.1886e+01 -1.0984e+01 -1.1414e+01  4.4739e+01  1.9691e+01  6.3646e+01

Columns 91 to 96  2.6256e-01 -3.6467e-01 -7.0295e-01 -1.8692e+00  7.7005e-01 -1.7508e+00
  3.0171e+01  7.9858e+00  6.8097e+01  4.0402e+01  6.8650e+01 -1.1860e+02

Columns 97 to 102 -7.5655e-01 -1.4886e+00 -7.0932e-01  1.5126e+00  1.2890e+00  1.0279e+00
  6.9442e+01  1.1253e+01  1.7532e+00  4.6354e+01  5.0235e+01 -8.2173e+01

Columns 103 to 108 -1.4096e+00  4.1890e-01  1.1135e+00 -1.2266e+00  3.7971e-01  9.0455e-01
  3.1491e+01 -3.1440e+01 -1.1554e+02 -1.5440e+01  4.3577e+01  8.7132e+01

Columns 109 to 114 -1.0729e+00 -2.1255e-01  4.4764e-01  8.0864e-02  2.7734e-01  1.4090e-01
 -1.7538e+01 -1.7563e+00 -2.0202e+01 -3.5053e+01  5.5379e+01  6.5117e+01

Columns 115 to 120  9.0318e-01 -3.3479e-01  1.7014e+00  1.8363e-01  6.7760e-03  1.0173e+00
  8.9089e+01  2.1531e+01 -9.3153e+00  5.4296e+01  2.3040e+01  6.3879e+01

Columns 121 to 126  6.3204e-01 -4.9531e-01 -1.5471e-01  4.6500e-01  3.3187e-01 -8.8935e-01
 -2.9480e+01 -2.3560e+01  4.6506e+01 -3.1823e+01  5.3896e+01 -4.0775e+01

Columns 127 to 128 -8.1257e-01  5.3334e-01
 -5.9235e+01  2.7378e+01'''

two_cols_data = two_cols_data.replace('\n', ' ').replace('  ', ' ').split(' ')
print(two_cols_data)
two_cols_data = [float(el) for el in two_cols_data if 'e' in el]
two_cols_data = np.array(two_cols_data)

print(two_cols_data)

col1_data = np.array([el for i, el in enumerate(two_cols_data) if ((i < 252 and i % 12 < 6) or (i >= 252 and i % 4 < 2))])
col2_data = np.array([el for i, el in enumerate(two_cols_data) if ((i < 252 and i % 12 >= 6) or (i >= 252 and i % 4 >= 2))])

# print(col1_data)
# print(col2_data)

# print(col1_data.shape)
# print(col2_data.shape)

# col1_mean = col1_data.mean()
# col1_var = col1_data.var()
# col2_mean = col2_data.mean()
# col2_var = col2_data.var()

# print(f"{col1_mean = }")
# print(f"{col1_var = }")
# print(f"{col2_mean = }")
# print(f"{col2_var = }")

col_data = torch.unsqueeze(torch.cat((torch.unsqueeze(torch.from_numpy(col1_data), dim=0), torch.unsqueeze(torch.from_numpy(col2_data), dim=0)), dim=0), dim=0)
# col_mean = 
print("col_data (concat):")
print(col_data)
print(col_data.size())

col_mean = torch.mean(col_data, dim=2)
print("col_mean:")
print(col_mean)

col_var = torch.var(col_data, dim=2, unbiased=False)
print("col_var:")
print(col_var)

# col_var_sum = None
# col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
# col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
# col_var_sum = (col_var_sum + col_var) if col_var_sum is not None else (col_var)
# print("col_var_sum")
# print(col_var_sum)

# col_var_avg = torch.div(col_var_sum, 3)
# print("col_var_avg")
# print(col_var_avg)

eps = 1e-5
W0 = 0.99982661008 # 0.99982661008
b0 = 0.00038191396 # 0.00038191396

def layer_norm(x, mean, var, eps, weight, bias):
  numerator = x - mean
  denominator = np.sqrt(var + eps)
  return numerator * weight / denominator + bias

col1_data_normalized = np.array([layer_norm(x=x, mean=col_mean[0][0], var=col_var[0][0], eps=eps, weight=W0, bias=b0) for x in col1_data])
print(col1_data_normalized)
print(col1_data_normalized.shape)

col2_data_normalized = np.array([layer_norm(x=x, mean=col_mean[0][1], var=col_var[0][1], eps=eps, weight=W0, bias=b0) for x in col2_data])
print(col2_data_normalized)
print(col2_data_normalized.shape)

results = ''' Columns 1 to 9  0.0269  0.8112 -0.0830  0.0319 -0.9405 -0.7571  2.0154  1.7169  1.1973
 -0.1204  0.2913 -1.5156  0.6827 -0.7446  0.0081  1.0114 -0.6022  0.6674

Columns 10 to 18  0.6811 -0.7951 -1.5215  0.3356  2.3869 -3.4773  1.5171 -0.0327  0.0313
 -0.0456  1.2158  0.6755 -1.5757 -1.4110  1.3134 -0.8719 -0.1692  0.6682

Columns 19 to 27 -1.3624  1.0228  0.5204  0.7497  0.1355  0.1293 -0.9782 -0.7146 -0.1684
  2.0125 -0.8306 -0.3866 -1.1160 -0.1219  1.0768 -0.0392 -1.3145 -1.2181

Columns 28 to 36  1.0538 -1.2626 -0.4555 -0.3145 -1.1772  0.3913  0.0238  1.0535 -0.3474
  0.9965 -0.4400 -1.6229 -1.4356  0.3822  0.0124 -1.1517  0.0475 -2.1390

Columns 37 to 45 -0.9927 -0.8135  0.5622  2.1029  0.9584 -0.4817 -1.1695  0.7574 -0.7139
 -0.6206 -0.6035 -2.4428 -0.2628  0.8427 -0.1568  0.3955 -1.0165  0.5264

Columns 46 to 54  0.0763  0.9659  0.6542 -1.9788 -0.0624 -0.8145 -0.3329 -1.8825 -0.4371
  0.5488  0.7359  0.7247 -0.3920  0.3825  0.2859  0.4862 -0.0456  0.7695

Columns 55 to 63  1.1498 -0.4855 -0.8378 -0.3423  0.1488  0.1513  2.3109 -0.0120  0.6882
  0.9894 -0.8917 -1.6650 -0.3808 -2.2436 -0.4913 -0.9882  1.1002  1.2014

Columns 64 to 72  1.5514  0.3752 -1.3709  0.0099 -0.4466 -0.8856 -0.6522 -0.9969 -0.4033
 -0.9672  1.1544 -1.2918  0.8899 -0.6923 -0.7348  1.7687 -0.7946 -0.0490

Columns 73 to 81 -0.3482  1.2473 -0.8587  1.0551  0.1753 -0.0386 -1.3348  0.6690 -0.5346
  1.4133  0.3290  0.8271 -0.3804  0.0729  2.4583  0.0147 -1.6503 -0.4108

Columns 82 to 90 -1.0934  0.3866 -0.6237 -0.8511  0.2884  1.2341  1.0730  0.1002  2.0573
  0.1461  0.3839  0.0311 -1.5783 -0.2133 -0.2225  0.8575  0.3761  1.2221

Columns 91 to 99  0.2528 -0.3850 -0.7296 -1.9156  0.7693 -1.7941 -0.7833 -1.5295 -0.7355
  0.5777  0.1513  1.3076  0.7750  1.3183 -2.2829  1.3326  0.2136  0.0310

Columns 100 to 108  1.5252  1.2976  1.0307 -1.4479  0.4124  1.1194 -1.2616  0.3726  0.9053
  0.8897  0.9644 -1.5828  0.6027 -0.6073 -2.2261 -0.2994  0.8365  1.6730

Columns 109 to 117 -1.1063 -0.2299  0.4412  0.0682  0.2676  0.1295  0.9051 -0.3545  1.7165
 -0.3402 -0.0359 -0.3911 -0.6765  1.0624  1.2499  1.7122  0.4116 -0.1820

Columns 118 to 126  0.1731 -0.0076  1.0214  0.6284 -0.5181 -0.1720  0.4591  0.3228 -0.9185
  1.0429  0.4402  1.2273 -0.5705 -0.4559  0.8919 -0.6148  1.0335 -0.7867

Columns 127 to 128 -0.8407  0.5285
 -1.1420  0.5243'''

results = results.replace('\n', ' ').replace('  ', ' ').split(' ')
results = [float(el) for el in results if len(el) == 6 or (len(el) == 7 and el[0] == '-')]
results = np.array(results)

col1_res = np.array([el for i, el in enumerate(results) if ((i < 252 and i % 18 < 9) or (i >= 252 and i % 4 < 2))])
col2_res = np.array([el for i, el in enumerate(results) if ((i < 252 and i % 18 >= 9) or (i >= 252 and i % 4 >= 2))])
print("results:\n")
print(col1_res)
print(col2_res)
print(col1_res.shape)
print(col2_res.shape)

print("diff:\n")
col1_diff = np.absolute(col1_data_normalized - col1_res)
print(col1_diff)
col2_diff = np.absolute(col2_data_normalized - col2_res)
print(col2_diff)
print(f"Biggest error in col1 is {np.amax(col1_diff)}")
print(f"Biggest error in col2 is {np.amax(col2_diff)}")


A = torch.tensor([1, 2, 3])
print(A[0])
print(A[1:])

X0 = 3.9991e-02
X1 = -6.1523

b0 = -0.01386488933016245
b1 = -0.0007655795714506348

W0 = 1.0169684936528849
W1 = 0.08921465251486772

print(f"{(X0*W0+b0)=}")
print(f"{(X1*W1+b1)=}")

# transformed_b0 = b0 - W0 * col1_mean / np.sqrt(col1_var + eps)
# transformed_W0 = W0 / np.sqrt(col1_var + eps)
# print(f'{transformed_W0 = }')
# print(f'{transformed_b0 = }')
# transformed_b0 = b0 - W0 * col2_mean / np.sqrt(col2_var + eps)
# transformed_W0 = W0 / np.sqrt(col2_var + eps)
# print(f'{transformed_W0 = }')
# print(f'{transformed_b0 = }')
# # c1_norm = lambda x: (x - col1_mean) / (np.sqrt(col1_var + eps)) * W0 + b0
# # c2_norm = lambda x: (x - col2_mean) / (np.sqrt(col2_var + eps)) * W0 + b0
# W0 = 0.08868025162534571
# b0 = -0.0006761925608737499
# c1_norm = lambda x: x * W0 + b0
# c2_norm = lambda x: x * W0 + b0

# x0 = 3.9991e-02
# x1 = -6.1523e+00

# print(c1_norm(x0))
# print(c2_norm(x1))


# name = 'transformer.0.self.norm.weight'

# is_weight = name.split('.')[-1] == 'weight'
# base = '.'.join(name.split('.')[:-1])

# print(is_weight)
# print(base)



# out = '0.0291  0.8141 -0.0834  0.0323 -0.9389 -0.7561  2.0164  1.7139  1.1976 0.6806 -0.7944 -1.5261  0.3381  2.3899 -3.4767  1.5191 -0.0319  0.0322 -1.3620  1.0260  0.5201  0.7483  0.1395  0.1292 -0.9794 -0.7127 -0.1668 1.0547 -1.2606 -0.4559 -0.3145 -1.1771  0.3919  0.0242  1.0530 -0.3467 -0.9925 -0.8133  0.5623  2.1039  0.9564 -0.4818 -1.1674  0.7567 -0.7161 0.0782  0.9672  0.6551 -1.9840 -0.0609 -0.8147 -0.3324 -1.8744 -0.4407 1.1481 -0.4855 -0.8371 -0.3444  0.1488  0.1512  2.3114 -0.0107  0.6887 1.5515  0.3745 -1.3714  0.0093 -0.4467 -0.8814 -0.6521 -0.9963 -0.4015 -0.3504  1.2474 -0.8581  1.0542  0.1751 -0.0384 -1.3358  0.6673 -0.5328 -1.0917  0.3879 -0.6216 -0.8539  0.2906  1.2314  1.0723  0.1012  2.0475 0.2561 -0.3860 -0.7291 -1.9155  0.7681 -1.7925 -0.7818 -1.5299 -0.7381 1.5272  1.2956  1.0302 -1.4486  0.4100  1.1184 -1.2609  0.3731  0.9049 -1.1066 -0.2312  0.4405  0.0676  0.2684  0.1302  0.9057 -0.3540  1.7166 0.1733 -0.0075  1.0232  0.6287 -0.5198 -0.1727  0.4604  0.3220 -0.9179 -0.8400  0.5301'
# out = out.replace('  ', ' ')
# out = out.split(' ')
# out = list(map(float, out))
# print(out)
# print(len(out))
# print(sum(out))
# print(np.mean(out))

# A = torch.rand(1, 2, 8)
# print(A)
# print(A.size())

# A_mean = torch.mean(A, dim=2)
# print(A_mean)
# print(A_mean.size())

# A_var = torch.var(A, dim=2)
# print(A_var)
# print(A_var.size())